{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2b32ed-1efe-402f-963a-44a020baad17",
   "metadata": {},
   "source": [
    "# Powergrid library construction and zero gamma data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabf5e9f-06fd-4cbd-b993-9575b851952b",
   "metadata": {},
   "source": [
    "$\\Gamma = 0$,\n",
    "\n",
    "Perburbations = large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f352add-6385-4885-8a9e-6d29c23137db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import odeint\n",
    "import pandas as pd\n",
    "import warnings\n",
    "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.sparse import coo_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c323906f-ab1b-4b2d-b62c-361ca68e2d03",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b22f7-17a1-4454-a69e-a95bbea6ff12",
   "metadata": {},
   "source": [
    "#### Reading gamma values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1153bb86-8ebf-479c-ab3a-58398f3c2ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_gamma.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6bed91a-b2e6-4e90-9699-e7f77e79fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62191c6-5c17-486d-8149-c2ed8654e496",
   "metadata": {},
   "source": [
    "#### Reading time series data and skipping rows to sub sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8487dae-1a17-434e-ab45-ef303c05a65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_timeseries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0713be27-3217-4d8f-bb57-4bfd3a8042b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_n_rows_btw = 100\n",
    "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bcacf48-1eeb-410e-8a67-cd60ae98fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6588acda-56ef-4026-beb7-92a64f666f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
    "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
    "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
    "                     inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dee5015-72f4-4623-a550-3727f5fce24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
    "\n",
    "data_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "327cb89d-1ae4-4210-bbb7-7c7ed98a99f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606f395-3002-4f8b-b44c-ba99519c07da",
   "metadata": {},
   "source": [
    "#### Loading admittance matrix, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec506723-7da2-46ee-86ee-203e439e6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
    "for column in admittance_Y_df.columns:\n",
    "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc827ee4-4571-4ede-903a-87ab46bb51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "admittance_Y_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d5f012-c31e-4cf2-a518-a17bfbb34689",
   "metadata": {},
   "source": [
    "#### Loading static parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "625b67d4-3372-48d3-b16e-204372fbd547",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08ecfcb6-d166-4fa9-97d1-27c3a69fb2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_param_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c4ce9d-c681-44c6-b885-cf0a2542dd98",
   "metadata": {},
   "source": [
    "#### Loading coupling matrix, K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c4032f8-283c-4a7a-bd52-d16f452b0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be0d3b1f-6813-47f6-8355-bca337730974",
   "metadata": {},
   "outputs": [],
   "source": [
    "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
    "coupling_K_df_labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babb38d5-94cd-447b-a64a-5ef4119ee20e",
   "metadata": {},
   "source": [
    "## Forming candidate library for powersystems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70022649-a469-45e9-8f7b-ce591f99df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b59a4e39-9793-4229-9f0a-e8df90f21a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_matrix = gamma_df.to_numpy()\n",
    "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
    "\n",
    "gamma_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cb182e2-72e3-4220-bcab-088b1788f957",
   "metadata": {},
   "outputs": [],
   "source": [
    "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
    "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
    "# coupling_matrix_init[3,:] = 1\n",
    "\n",
    "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
    "coupling_matrix_init\n",
    "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
    "sparse_coupling_matrix_init.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c956671-f542-4dd5-b845-efc51d3275c0",
   "metadata": {},
   "source": [
    "### Defining the sin interaction terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7780a30a-b490-497a-9f4e-6860e798ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dae_finder import FeatureCouplingTransformer\n",
    "\n",
    "def coup_fun(x,y,i,j,gam_matrix):\n",
    "    return np.sin(x-y- gam_matrix[i,j])\n",
    "\n",
    "def coup_namer(x,y,i,j,gam_matrix):\n",
    "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
    "    \n",
    "\n",
    "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
    "                                           coupling_func= coup_fun,\n",
    "                                           coupling_namer= coup_namer,\n",
    "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
    "                                              return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51edd95d-31b9-4266-92c3-9a6c69fcef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
    "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
    "\n",
    "# cop_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bc7be7b-d7b7-44aa-9535-52cee8374187",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_diff_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8124e21d-d9c4-4ed4-8d3b-d3c3f0bb6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1), sin_diff_library], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c7e11f0-9702-4397-abac-4276c073b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e562e7-2a23-4d88-9ce2-eb417570d84a",
   "metadata": {},
   "source": [
    "## Finding Algebraic relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ff3e7-6c16-4cc0-aac2-a1c2ad8cab4e",
   "metadata": {},
   "source": [
    "### Using sympy to find factors of redundant feature and refine the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74d45159-ccac-4259-a221-a2990bbef246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "\n",
    "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
    "\n",
    "# Adding the state variables as scipy symbols\n",
    "feat_list = list(data_matrix_df.columns)\n",
    "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
    "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7f087-1d0e-469f-8721-bdd8c2855bbb",
   "metadata": {},
   "source": [
    "# Using Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f222300-aebe-4a68-8140-6a86b8453fce",
   "metadata": {},
   "source": [
    "## Finding the conservation laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f941add7-f0db-4c35-87e1-e5191e1be874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dae_finder import sequentialThLin, AlgModelFinder\n",
    "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
    "                                       alpha=0.3,\n",
    "                                       fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae4cc13f-e958-46e7-92d4-7adcef8b99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
    "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe401449-ec66-47ad-91a7-c5ab33eb4db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algebraic_model_lasso.best_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c19bff5-6e3e-41e4-9b10-3192840e16f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db0d4e37-96c7-4b97-ad7b-fdbb9e5b86cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = \"P_5\"\n",
    "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ba6d03-0601-4f04-aaa4-00b4e07d4de8",
   "metadata": {},
   "source": [
    "# Using Sequential Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ae9f8667-acc2-4c66-9b1d-3846be8388ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dae_finder import sequentialThLin, AlgModelFinder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg_model = LinearRegression\n",
    "lin_reg_model_arg = {\"fit_intercept\": False}\n",
    "seq_th_model = sequentialThLin(custom_model=True,\n",
    "                               custom_model_ob = lin_reg_model,\n",
    "                               custom_model_arg= lin_reg_model_arg,\n",
    "                              coef_threshold=0.5)\n",
    "# seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
    "\n",
    "algebraic_model_th = AlgModelFinder(custom_model=True, custom_model_ob= seq_th_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ac9d9d25-a7e4-4a93-a17f-9fca0614422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "algebraic_model_th.fit(candidate_lib, scale_columns= False,\n",
    "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5391732c-2250-4fd7-9d75-b26aa9b1dd71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Best 10 models using R2 metrix\n",
    "algebraic_model_th.best_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "404a887d-cca9-4187-844e-ef617106bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "algebraic_model_th.get_fitted_intercepts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "98b4e9fd-9285-4e1f-b4f6-81cb5ba30896",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = \"P_2\"\n",
    "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b6bdc-2018-4077-a56e-318ec8fda9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00729f1-19ff-40fe-bff4-662613b06ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae24ceb-23fe-4d2a-b49a-f93d31b0ec49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd84ab92-0b1c-489f-a743-20f5ef9b0702",
   "metadata": {},
   "source": [
    "## Finding the conservation laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c5bdefb-e6b6-4db6-8e0e-b85cd973e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dae_finder import sequentialThLin, AlgModelFinder\n",
    "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
    "                                       alpha=0.3,\n",
    "                                       fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47d78ebc-e5e6-4aef-8930-c16968a677a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
    "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
    "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b6f712c-a161-489a-88dd-f15f22af23e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "algebraic_model_lasso.best_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cad3bc4-d9e6-4d28-89a8-02ade7130fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "069fdce2-ffe4-4091-912f-6e08bf62cfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = \"P_5\"\n",
    "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439d23b0-9723-468a-a11f-e5d666d0b6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27bb129-18a4-4433-8efc-2640b6490e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa9a220-c3fe-41ca-99fd-61be3ca1774a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc110b04-3e10-464a-96d6-bbf219331aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eb0433-a834-452a-a594-947a9678fc18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8c3c4c99-2a17-4020-96e9-536188eb2344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0161b34b-a97e-4386-83c0-86858769a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = LinearRegression()\n",
    "lass_model = Lasso(alpha=0.01)\n",
    "ridge_model = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e52a5912-b96a-4eee-8e75-1f126837e3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model.fit(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
    "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
    "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "38f42e43-d5ef-4992-aed2-e502aef42053",
   "metadata": {},
   "outputs": [],
   "source": [
    "lass_model.fit(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
    "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
    "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])\n",
    "\n",
    "ridge_model.fit(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
    "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
    "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "978f8e36-d376-4ec4-ab82-070e41be57fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "448aeffb-6516-4270-b98f-e7407d93d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lass_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8d659e9b-fc7e-4ca9-80ec-1f0adb2e0ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e1a20778-4775-42e9-8d58-2404697407d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model.score(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
    "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
    "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6568b8c3-5628-4065-a169-b74a1e670cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995dad12-8a38-416f-9520-a6ffaa3ee616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "77cffac1-86af-4a39-9b2a-91b4bbd8565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lass_model.score(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
    "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
    "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf64b4bf-d06c-4111-9eb0-0d33e6ff6d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8c260-3dc6-456d-a36d-9114269061ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0076d486-1576-477b-845d-e321256c5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model.score(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
    "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
    "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3d998ce0-93f1-4b7a-98eb-06b626c7ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_lib.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "01f48f7b-ce00-4572-ae27-9cb86e5f98dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
    "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
    "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce05a8f-9e5c-4817-8b6c-21caa65619c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1296428-e7c1-4167-b1a0-d15dc5b23c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6802d-5d01-4165-acaa-e6d942ce7cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d03a1456-91f8-403e-b795-d1cdb24eef6d",
   "metadata": {},
   "source": [
    "### Finding the remaining algebraic relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3c9a5342-6c99-4096-a3d4-f765eb58069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_remove = {E}\n",
    "\n",
    "features_to_remove, refined_candid_lib = get_refined_lib(features_to_remove, data_matrix_df,\n",
    "                                                  candidate_lib_full, get_dropped_feat=True)\n",
    "\n",
    "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
    "                                       fit_intercept=False, alpha=0.2)\n",
    "algebraic_model_lasso.fit(refined_candid_lib, scale_columns= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "08095581-03b2-4a01-8116-cd6ba6f35a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "admittance_Y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "07dba226-e919-4b0d-bb3a-1384e5ae5e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "737e9f3a-0098-4896-8116-d8b4713b64dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f6789f-e9c5-4642-ad6c-d5d47150dfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff30fe-d54a-44e2-963c-d721fcbd96fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4053da-9e5c-4372-a037-46f841ccaeb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9c030-4ac3-403d-89bf-cac72153d511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ddd39-8be2-4158-9297-e6b64ebabc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0205f2d1-f662-4079-966d-2cb981fb5ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
    "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
    "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c523c2a-eead-4477-97a7-83f904f6843d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "25a0bf8d-df51-40bf-aa27-e7e3a7318cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_df.drop([\"time\", \"P_0\",\"P_1\",\"P_2\",\"P_3\",\"P_4\",\"P_5\", \"om_0\", \"om_1\"], axis=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101cef4-45ff-4278-bbc5-65371b698430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6dca5e25-9c8b-41c4-b298-9ee2a2c8a9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_df[[\"P_0\",\"P_1\",\"P_2\",\"P_3\",\"P_4\",\"P_5\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecdfb47-f121-407d-90b5-29a87c47ac2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071c86f4-33ee-4fff-8944-49a20b702d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb6e51-79a8-4a2f-8b7e-f0d60d357c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e93ed-49fb-44d4-a797-bcc65db84f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb530c1b-c6fe-42a3-b1f8-86bf3b0cf522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a36c43d-76d0-4b2c-98a8-1af1185d3b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5445d61-af77-474b-8997-3cb37107d91b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4769d0-0780-4a2e-8d54-96de1b39cee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca89addb-3128-4023-aa57-56654fa2f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37b58adc-431d-4414-9fc6-dd72f0ecbe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#smoothing parameter: when equal weightage: num_data_points * std of data\n",
    "s_param = num_time_points * (0.01*noise_perc*data_matrix_df_list[1].std()[\"x\"])**2\n",
    "\n",
    "tck = interpolate.splrep(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], s=s_param)\n",
    "t_eval_new = np.linspace(t_span[0], t_span[1], 500)\n",
    "x_new = interpolate.splev(t_eval_new, tck, der=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
    "        data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
    "plt.legend(['Linear', 'Cubic Spline', 'True'])\n",
    "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
    "plt.title('Cubic-spline interpolation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cbaa92b-49d1-4c82-845e-50ef0396753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(3/4)*np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb578091-f24d-45ca-a666-4ffbff1d84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_df[[\"x\",\"y\"]].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386adc28-f9d5-4e0c-b678-6f6cc0b7be15",
   "metadata": {},
   "source": [
    "### Smoothing data and finding derivatives"
   ]
  },
  {
   "attachments": {
    "c77aa784-201b-4c9a-b1b3-32fd7b314f37.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "4fc6e9e9-8968-48c8-94be-658040df22bf",
   "metadata": {},
   "source": [
    "![Screenshot 2024-03-19 at 6.16.04PM.png](attachment:c77aa784-201b-4c9a-b1b3-32fd7b314f37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0516a86-c43a-4951-a950-20a6241c0c60",
   "metadata": {},
   "source": [
    "from dae_finder import der_matrix_calculator\n",
    "\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a204092d-b2ad-4c30-8b6d-53679cc85656",
   "metadata": {},
   "source": [
    "def der_label(feature, der=1):\n",
    "    if der==0:\n",
    "        return feature\n",
    "    elif der==1:\n",
    "        return \"d({}) /dt\".format(feature)\n",
    "    else:\n",
    "        return \"d^{}({}) /dt^{}\".format(der, feature, der)\n",
    "\n",
    "def smooth_data(data_matrix, domain_var = \"t\", s_param=None, noise_perc=0, derr_order = 1, eval_points = []):\n",
    "    assert domain_var in data_matrix, \"domain variable not found in the data matrix\"\n",
    "\n",
    "    data_t = data_matrix[domain_var]\n",
    "    num_time_points = len(data_matrix)\n",
    "    find_s_param = !s_param and s_param !=0\n",
    "\n",
    "    if len(eval_points)==0:\n",
    "        eval_points = np.linspace(data_t.iloc[0], data_t.iloc[-1], 10*num_time_points)\n",
    "    t_eval_new = eval_points\n",
    "               \n",
    "    data_matrix_ = data_matrix.drop(domain_var, axis=1)\n",
    "    data_matrix_std = data_matrix_.std()\n",
    "\n",
    "    data_matrix_smooth = pd.DataFrame(t_eval_new, columns=[domain_var])\n",
    "    for feature in data_matrix_:\n",
    "        if find_s_param:\n",
    "            #smoothing parameter: when equal weightage: num_data_points * std of data\n",
    "            s_param = num_time_points * (0.01*noise_perc*data_matrix_std[feature])**2\n",
    "        tck = interpolate.splrep(data_t, data_matrix_[feature], s=s_param)\n",
    "        for der_ind in range(derr_order+1):\n",
    "            smoothed_data = interpolate.splev(t_eval_new, tck, der=der_ind)\n",
    "            data_matrix_smooth[der_label(feature, der_ind)] = smoothed_data\n",
    "\n",
    "    return data_matrix_smooth\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf603129-05a9-4463-8adf-42ca57c90f59",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "t_eval_new = np.linspace(data_matrix_df_list[1][\"t\"].iloc[0], data_matrix_df_list[1][\"t\"].iloc[-1], num_smoothed_points)\n",
    "\n",
    "smooth_data(data_matrix_df_list[1],derr_order=1, eval_points=t_eval_new, noise_perc=noise_perc) - data_matrix_smooth_df_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08ad0fa-115b-4b39-92c6-358187d64d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3d67a63-657f-4602-83ee-21dd2b687f5a",
   "metadata": {},
   "source": [
    "delta_t = t_eval_[1]- t_eval_[0]\n",
    "data_matrix_features = data_matrix_df_list[0].columns\n",
    "num_smoothed_points = num_time_points*10\n",
    "data_matrix_smooth_df_list = []\n",
    "\n",
    "for ind, data_matrix in enumerate(data_matrix_df_list):\n",
    "    data_t = data_matrix[\"t\"]\n",
    "    num_time_points = len(data_matrix)\n",
    "    data_matrix_ = data_matrix.drop([\"t\"], axis=1)\n",
    "    data_matrix_std = data_matrix_.std()\n",
    "    # print(data_matrix_std)\n",
    "    t_eval_new = np.linspace(data_t.iloc[0], data_t.iloc[-1], num_smoothed_points)\n",
    "    data_matrix_smooth = pd.DataFrame(t_eval_new, columns=[\"t\"])\n",
    "    for feature in data_matrix_:\n",
    "        #smoothing parameter: when equal weightage: num_data_points * std of data\n",
    "        s_param = num_time_points * (0.01*noise_perc*data_matrix_std[feature])**2\n",
    "        # print(s_param)\n",
    "        # s_param = 0\n",
    "        \n",
    "        tck = interpolate.splrep(data_t, data_matrix_[feature], s=s_param)\n",
    "        smoothed_data = interpolate.splev(t_eval_new, tck, der=0)\n",
    "        smoothed_derr = interpolate.splev(t_eval_new, tck, der=1)\n",
    "        data_matrix_smooth[feature] = smoothed_data\n",
    "        data_matrix_smooth[\"d({}) /dt\".format(feature)] = smoothed_derr\n",
    "\n",
    "    data_matrix_smooth_df_list.append(data_matrix_smooth)\n",
    "\n",
    "    \n",
    "    # derr_matrix = der_matrix_calculator(data_matrix_, delta_t)\n",
    "    # data_matrix_df_list[ind] = pd.concat([data_matrix_.iloc[:-1], derr_matrix], axis=1)\n",
    "\n",
    "data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list[:-1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "774b6c99-506f-4371-b688-ebb87dfe4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dae_finder import smooth_data\n",
    "\n",
    "num_smoothed_points = num_time_points*10\n",
    "\n",
    "t_eval_new = np.linspace(data_matrix_df_list[0][\"t\"].iloc[0], data_matrix_df_list[0][\"t\"].iloc[-1], num_smoothed_points)\n",
    "\n",
    "#Calling the smoothening function\n",
    "data_matrix_smooth_df_list = [smooth_data(data_matrix,derr_order=1, noise_perc=noise_perc, eval_points=t_eval_new) for data_matrix in data_matrix_df_list]\n",
    "\n",
    "data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list[:-1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92bb674-b5cf-478e-a787-81d0b4520f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1130b677-3079-4f26-b6ae-821866e20307",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 1\n",
    "feature_ = \"y\"\n",
    "\n",
    "plt.figure()\n",
    "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
    "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
    "\n",
    "plt.plot(data_matrix_df_list[ind][\"t\"], data_matrix_df_list[ind][feature_], \"x\", data_matrix_smooth_df_list[ind][\"t\"],\n",
    "         data_matrix_smooth_df_list[ind][feature_],data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"t\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \"o\")\n",
    "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
    "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
    "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c924d8cb-183a-4655-8c72-39fc680afea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_smooth_df_list[0][[\"d(x) /dt\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f081dd15-d33b-42f3-a6e1-b04d200b495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_smooth_df_list[0][[\"d(x) /dt\"]].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ce1ce-c6b0-44b6-b71e-f150c72de03d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "919ee083-27cc-46a9-aa63-80012571171e",
   "metadata": {},
   "source": [
    "#Taking second derivatives\n",
    "delta_t = t_eval_[1]- t_eval_[0]\n",
    "data_matrix_features = data_matrix_df_list[0].columns\n",
    "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
    "    derr_matrix = der_matrix_calculator(data_matrix_, delta_t)\n",
    "    data_matrix_df_list[ind] = pd.concat([data_matrix_.iloc[:-1], derr_matrix], axis=1)\n",
    "\n",
    "data_matrix_df_appended = pd.concat(data_matrix_df_list, ignore_index=True)\n",
    "data_matrix_df_appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acfdcd4c-ef3f-46b0-90bf-cf1d8a5ea85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_matrix_df = data_matrix_df_appended[[\"x\",\"y\"]]\n",
    "# data_matrix_df = pd.concat([data_matrix_df, data_matrix_df_appended[[\"d(u) /dt\"]]], axis=1)\n",
    "data_matrix_df_smooth = data_matrix_df_smooth_appended[[\"x\",\"y\", \"d(x) /dt\", \"d(y) /dt\"]]\n",
    "data_matrix_df_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c78e05a-016f-4a64-ae02-94d942da230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "new_df = deepcopy(data_matrix_df_smooth)\n",
    "\n",
    "new_df[\"energy\"] = 0.5*((new_df[\"d(x) /dt\"])**2 + (new_df[\"d(y) /dt\"])**2) +  9.81*new_df[\"y\"]\n",
    "\n",
    "new_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083b7d72-e44a-4fb8-8fc4-4cf8b95cbaba",
   "metadata": {},
   "source": [
    "## Forming candiate library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2363240e-706e-499d-98d4-4fb5beceab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from copy import deepcopy\n",
    "\n",
    "def sin_transformer(period = 2*np.pi):\n",
    "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
    "\n",
    "\n",
    "def sin_diff_transformer(period = 2*np.pi):\n",
    "    return FunctionTransformer(lambda x,y: np.sin((x-y) / period * 2 * np.pi))\n",
    "\n",
    "def cos_transformer(period = 2*np.pi):\n",
    "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
    "\n",
    "\n",
    "def sin_shift_transformer(period = 2*np.pi):\n",
    "    return FunctionTransformer(lambda x,y: np.sin((x-y) / period * 2 * np.pi))\n",
    "\n",
    "data_matrix_df_with_trig = deepcopy(data_matrix_df)\n",
    "# data_matrix_df_with_trig[\"sin(theta)\"] = sin_transformer(1).fit_transform(data_matrix_df_with_trig)[\"theta\"]\n",
    "# data_matrix_df_with_trig[\"cos(theta)\"] = cos_transformer(1).fit_transform(data_matrix_df_with_trig)[\"theta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeef1e1-372c-4588-a5c8-0ad721db7a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1911862f-07ac-44fa-aff2-6a9d041c5c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_array\n",
    "\n",
    "row  = np.array([0, 3, 1, 0, 0, 0])\n",
    "col  = np.array([0, 3, 1, 1, 2, 4])\n",
    "data = np.array([4, 5, 7, 9, 100, 101])\n",
    "# sp_array = coo_array((data, (row, col)), shape=(4, 4))\n",
    "sp_array = coo_array((data, (row, col)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1f84dd78-113c-4850-92ab-808c6f688cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a433cb-2a03-4e89-b4a2-1146093db659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08640e-852e-4773-82d4-e0c9b3b1340b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518f35be-a691-441a-8612-b4304d7f99b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06817ca3-eaff-4141-b7bb-752059ea00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(sp_array.row, sp_array.col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f88e8f6-7cfd-4922-a29d-6d33303bae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dae_finder import FeatureCouplingTransformer\n",
    "\n",
    "from scipy.sparse import coo_array\n",
    "\n",
    "row  = np.array([0, 0, 1, 1])\n",
    "col  = np.array([0, 2, 2, 1])\n",
    "data = np.array([4, 5, 7, 5])\n",
    "sp_array_2 = coo_array((data, (row, col)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "078f2c53-c652-4cfa-80e6-ebfdb075ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.triu(np.ones((3,3)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5914045-a626-4b6f-be27-1ef48c029e8a",
   "metadata": {},
   "source": [
    "### Defining the sin interaction terms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d868e46-d855-4a46-83c7-90a86fc4db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "\n",
    "def coup_fun(x,y,i,j,ph_matrix):\n",
    "    return np.sin(x-y- ph_matrix[i,j])\n",
    "\n",
    "def coup_namer(x,y,i,j,ph_matrix):\n",
    "    return \"sin( {}-{} -phi_{},{} )\".format(x,y,i,j)\n",
    "dummy_tr_sin_diff = FeatureCouplingTransformer(sp_array_2,\n",
    "                                           coupling_func= coup_fun,\n",
    "                                           coupling_namer= coup_namer,\n",
    "                                           coupling_func_args={\"ph_matrix\":phi_matrix},\n",
    "                                              return_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "121adddc-240c-49b8-99f4-18207d6ba932",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = dummy_tr_sin_diff.fit_transform(data_matrix_)\n",
    "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
    "\n",
    "cop_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98197188-8db1-400d-a727-cf528e765187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9315f269-2867-42ce-87f4-288032c13731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_tr._check_n_features(data_matrix_, 0)\n",
    "# dummy_tr._check_feature_names(data_matrix_, reset = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84bff3ca-e5f2-4b9b-8646-55bb3ba60728",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6a9f99f-2578-4ea8-9a4c-4f525fd275f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_diff = dummy_tr_diff.fit_transform(data_matrix_)\n",
    "cop_ind_diff = dummy_tr_diff.coupled_indices_list\n",
    "\n",
    "cop_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f02f3d90-8748-4ca4-a9f0-a290ef5bf624",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_diff_minus_k = dummy_tr_diff_minus_k.fit_transform(data_matrix_)\n",
    "cop_ind_diff_minus_k = dummy_tr_diff_minus_k.coupled_indices_list\n",
    "\n",
    "cop_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e9f6448-ff7a-4749-bac0-244364a8df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = dummy_tr.fit_transform(data_matrix_)\n",
    "cop_ind = dummy_tr.coupled_indices_list\n",
    "\n",
    "cop_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fadaf209-84c9-49b9-a564-35eabdd548a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tr_diff.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4c058ec-dd29-4dce-af2f-2c1e8d24a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tr.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e319ed2-9d63-4266-97be-501e48243712",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tr_sin_diff.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327e97e-5499-4d41-b3a0-568fcf3d2843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d056171-a7cc-4d8d-8097-e8d4f776facf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34a2e288-3c02-4df0-b839-099375d7ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def du_2(a,b=100,c=200):\n",
    "    print(a,b,c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "702b3fcc-0fbc-40ba-9f91-59e5254b036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def du(a,**kwargs):\n",
    "    # print(a)\n",
    "    du_2(a, **kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "779d1fc9-7ad9-4da1-9b75-76c4266e3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "du(2, b=33, c=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6f292514-090f-4bbb-b210-4d9510c2d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "du(2, **{\"b\": 3, \"c\":4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "82554f24-1010-4caf-ac0a-d1257b1cd47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(dummy_tr.sparsity_matrix.row, dummy_tr.sparsity_matrix.col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7782df8-20c6-4b87-a5f5-a864fd4602d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_array_2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "be9ea99a-53ac-448a-a7ba-34ca7e315248",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = np.zeros((dummy_tr.fit_transform(data_matrix_).shape[0], len(dummy_tr.coupled_indices_list)))\n",
    "x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "9acb40f2-b41c-40bf-8b51-fdf467d03845",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_diff_minus_k[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "f97a61af-834c-4334-b505-2ad146011f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_[\"t\"] - data_matrix_[\"y\"] - 2 - new_data_diff_minus_k[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29c45aa3-850d-4078-85c2-c375bfbf4257",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tr.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc4c25da-d77b-4506-8246-335d4958a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tr.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b955536-05a9-4cd0-b4f8-6ca28469aeea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "955dc44e-23a9-4e8e-9c39-85b9412aeedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "c = np.array([7,8,9])\n",
    "\n",
    "\n",
    "np.vstack((a,b,c)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e32e389-6fe0-41db-a57f-c5812016c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tr.fit(data_matrix_)\n",
    "dummy_tr.fit_transform(data_matrix_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47bc5d2d-e6d7-44e4-a383-09e5b71e043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tr.fit_transform(data_matrix_)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61750012-1c5d-47af-bda4-04cc89fc7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_tr.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0e0352f-84cb-4a44-ba09-be1bbb0ec4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_array.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0998bd4-c0ff-433c-aaf8-5c4d8dcb419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_array.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ae82e4f-43a1-46ac-b78c-35ec615fa59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_array.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46927a2f-b5d0-4d41-bf2d-a5e23d13c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el1, el2 in zip(sp_array.row, sp_array.col):\n",
    "    print(el1, el2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1c124045-cab9-4713-8503-cbea50302880",
   "metadata": {},
   "outputs": [],
   "source": [
    "coo_array(sp_array.toarray()).nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fbd4dac2-22e7-4b86-a452-ed9a73b8c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_array.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "07446528-0310-4625-ab21-8eb7be25abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_array.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4c09600-1a33-4a44-8648-6e52afb7f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.triu(np.ones((4,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08ac8022-28e6-4299-9d87-7df9a9a515c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "be5c2e38-d824-4451-986e-d2b8503097aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_array.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "be1adbcc-c8d0-45c0-abcc-dd17ebd416e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(sp_array.col.max(), sp_array.row.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0e0fe14a-e646-416c-ad80-fe5a0d6f79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_array.row.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91490b-2470-4e6c-bc05-7c920f736aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad517b9-8886-4c2f-99a8-500c714cea00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a182e5cb-5be1-47dd-9c78-cff31e26fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_array.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "037d31a4-f461-44b9-ae2c-04e87fc4c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_array.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8928c0fe-1d83-49ad-8269-09fd097653be",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sp = sp_array.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2a2b77bd-0d86-4000-87db-b6295a808ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sp.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ed823b0f-de21-4d92-b9e8-99828c8c285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_sp = new_sp[[0]]\n",
    "col_sp.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "11d2aad0-2c8c-406e-9453-fe6e2a363d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rows corresponding to 1 and 2\n",
    "new_sp[[0, 1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e7ad2847-8224-4d5a-8f2c-dcccf9ba6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rows corresponding to 0 and 1\n",
    "new_sp[:, [0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2c06183d-7ca8-4f9b-bfc9-752842059e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rows corresponding to 1 and 2\n",
    "new_sp[[0, 1,2]].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c0c22e27-c7ef-4605-b101-7ef7e7c20638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rows corresponding to 0 and 1\n",
    "new_sp[:, [0,1]].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c4269b0a-1d52-4f92-a77a-7cbd249cd687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rows corresponding to 1 and 2\n",
    "new_sp[[0, 1,2]].tocoo().row\n",
    "# new_sp[[1,2]].tocoo().col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "61772d13-86fe-42cc-b5f4-8f70102b6bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sp[[0, 1,2]].tocoo().col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c57c3301-3c41-498c-8566-4273086b2e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rows corresponding to 0 and 1\n",
    "new_sp[:, [0,1]].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b14e85-b8a5-49f7-858d-231e1b555395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4fa110-8a6c-4bc4-aab0-0bb9d5649d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0573e6d5-5e96-46a4-9b9a-fb05882c0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sp[[1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "63a1f4cc-bc76-480e-9e63-cf276d146af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sp.getrow(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "73c93b26-851b-4c9a-beeb-9bb4695c2271",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sp.getcol(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4caa5c-2cdb-48ec-903f-5e0f0ac04f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d15f3-3133-4e0a-980a-0c942e2394e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe772f-d606-4ff0-9593-741f5174dfb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71bc1732-cc38-4ffc-ad50-bcc70dad685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sin(data_matrix_df_with_trig) - sin_trans_obj.fit_transform(data_matrix_df_with_trig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5e67e-9eca-47c9-a414-79d8616c55a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b248a66-d2de-4ed9-910c-12af05f40946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e29302-8188-466c-bdf3-b4a9da5a852e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aabd9e-d071-488c-97f3-06ac49213e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8ea43a1-abb2-407a-b593-0d2bc960c799",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_trans_obj = sin_transformer()\n",
    "sin_trans_obj.fit_transform(data_matrix_df_with_trig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39ce585f-a9b4-4632-9dcd-a07293f019ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b590b-1ee9-47c0-8a62-50cd7af07e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70214cd-b4e1-41f9-8209-9dc8f8567e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c667a96b-12c0-4bab-808a-5d7b7e884fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dae_finder import PolyFeatureMatrix\n",
    "\n",
    "poly_feature_ob = PolyFeatureMatrix(2)\n",
    "\n",
    "candidate_lib_full = poly_feature_ob.fit_transform(data_matrix_df_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a11429f-1cc4-4c85-a27a-d10c6aa4e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_lib_full = candidate_lib_full.drop([\"1\"], axis=1)\n",
    "candidate_lib_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b04b81-6990-4940-b0c6-a08228c9bd5d",
   "metadata": {},
   "source": [
    "candid_lib_comb = pd.concat([candidate_lib_full, data_matrix_df_with_trig[[\"cos(theta)\", \"sin(theta)\"]]], axis=1)\n",
    "candid_lib_comb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1283b467-fa54-4a0f-adab-fbd8108b142e",
   "metadata": {},
   "source": [
    "### SVD analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa17c052-a6be-47e3-aaec-51dc2bf35833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca_1 = decomposition.PCA()\n",
    "pca_1.fit(candidate_lib_full)\n",
    "\n",
    "# pca_2 = decomposition.PCA()\n",
    "# pca_2.fit(mean_candidate_lib)\n",
    "\n",
    "# pca_3 = decomposition.PCA()\n",
    "# pca_3.fit(selected_data_matrix_df)\n",
    "\n",
    "pca_2 = decomposition.PCA()\n",
    "pca_2.fit(candidate_lib_full.drop([\"x^2\", \"x d(x) /dt\"],axis=1))\n",
    "\n",
    "pca_3 = decomposition.PCA()\n",
    "pca_3.fit(candidate_lib_full.drop([\"x^2\", \"x d(x) /dt\", \"y\"],axis=1))\n",
    "\n",
    "\n",
    "# singular_values = pca_1.singular_values_\n",
    "# mean_singular_values = pca_2.singular_values_\n",
    "\n",
    "var_expl_ratio = pca_1.explained_variance_ratio_\n",
    "theta_dot_sq_rem_expl_ratio = pca_2.explained_variance_ratio_\n",
    "theta_dot_rem_expl_ratio = pca_3.explained_variance_ratio_\n",
    "# data_var_expl_ratio_E = pca_4.explained_variance_\n",
    "\n",
    "# var_expl_ratio_E_rem = pca_5.explained_variance_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4d5314b-0ce0-4734-9143-c0a37e5467e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(np.arange(len(var_expl_ratio)),np.log(var_expl_ratio))\n",
    "plt.grid()\n",
    "# for x, y in zip(np.arange(len(candid_lib_sing_values)),np.log(candid_lib_sing_values)):\n",
    "#     plt.text(x,y,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fcc41f2-42e3-4f21-bde5-7e1d2b754630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(np.arange(len(theta_dot_sq_rem_expl_ratio)),np.log(theta_dot_sq_rem_expl_ratio))\n",
    "plt.grid()\n",
    "# for x, y in zip(np.arange(len(candid_lib_sing_values)),np.log(candid_lib_sing_values)):\n",
    "#     plt.text(x,y,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "382d4f54-9e25-4477-b043-07a39575acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(np.arange(len(theta_dot_rem_expl_ratio)),np.log(theta_dot_rem_expl_ratio))\n",
    "plt.grid()\n",
    "# for x, y in zip(np.arange(len(candid_lib_sing_values)),np.log(candid_lib_sing_values)):\n",
    "#     plt.text(x,y,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c027ef7f-5809-4802-a7da-9003b5d0b876",
   "metadata": {},
   "source": [
    "### Finding the remaining algebraic relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a62fdc5a-728c-4b84-90a0-dc47f7381842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dae_finder import AlgModelFinder\n",
    "from dae_finder import sequentialThLin, AlgModelFinder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e549109e-1221-4dd5-93bd-96b297a647e8",
   "metadata": {},
   "source": [
    "\n",
    "seq_th_model = sequentialThLin(fit_intercept=True, coef_threshold= 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be90638-6291-4098-9e2e-6bd420b01c20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f3ce34d-36c6-41e3-91d6-5617b72e2e20",
   "metadata": {},
   "source": [
    "algebraic_model_th.fit(candidate_lib_full, scale_columns= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6a8a0a4-ee69-4262-812f-bfc4cfa3d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_th_model = sequentialThLin(fit_intercept=True, coef_threshold= 0.05)\n",
    "algebraic_model_th = AlgModelFinder(custom_model=True, custom_model_ob= seq_th_model)\n",
    "\n",
    "algebraic_model_th.fit(candidate_lib_full.drop([\"x^2\", \"x d(x) /dt\"], axis=1), scale_columns= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31434cc6-0419-4b7a-8600-0c558a96ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "algebraic_model_th.best_models(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f68fdc2-a2ef-49d0-9be5-a495d3b0224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use lasso model by default\n",
    "algebraic_model_1 = AlgModelFinder(model_id='lasso', alpha=0.3, fit_intercept=True)\n",
    "algebraic_model_1.fit(candidate_lib_full, scale_columns= True)\n",
    "\n",
    "\n",
    "algebraic_model_1.best_models(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d28f33d3-b4ba-436b-8291-8322234747b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use lasso model by default\n",
    "algebraic_model_1 = AlgModelFinder(model_id='lasso', alpha=0.3, fit_intercept=True)\n",
    "algebraic_model_1.fit(candidate_lib_full.drop([\"x^2\", \"x d(x) /dt\"], axis=1), scale_columns= True)\n",
    "\n",
    "\n",
    "algebraic_model_1.best_models(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e4e1b8-cfb2-4f13-ba25-dc35b7925a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1701,
   "id": "745f2bc8-9186-4e17-93d0-a9f38d66bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1702,
   "id": "f4a2155a-f18b-43b3-91d3-0e181f8e82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lin = LinearRegression(fit_intercept= True)\n",
    "\n",
    "model_lin.fit(X=candidate_lib_full[[\"d(y) /dt^2\", \"d(x) /dt^2\"]], y=candidate_lib_full[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1648,
   "id": "46eef7f6-bb9b-463e-8eb6-9bb21a04bc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(model_lin.feature_names_in_, model_lin.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1555,
   "id": "e89b7a6d-0400-4aa7-92ed-f9228656b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lin.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1556,
   "id": "e025344a-a133-475a-b710-7b2289d9680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected coefficient of x_dot^2 and y_dot^2 \n",
    "1/(9.8*2) #1/2 * 1/g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1557,
   "id": "dcbe45d6-7709-4ab8-aa12-a5de1962e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected intercept (coming from energy)\n",
    "L #L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7088f8a-3361-4693-95f9-ca1da81b7137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1558,
   "id": "00f3e29d-ead8-4691-a396-2e76684bfc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lin = LinearRegression(fit_intercept= True)\n",
    "\n",
    "model_lin.fit(X=candidate_lib_full[[\"y^2\", \"d(y) /dt^2\"]], y=candidate_lib_full[\"x^2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1559,
   "id": "fd106e77-0483-44f2-8c21-34c6934e6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(model_lin.feature_names_in_, model_lin.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1446,
   "id": "923d0026-372b-4924-884d-677939bc745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lin.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7033a5f-870c-4fa0-9e3d-fcfa13fbbadd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "f24825bf-5b0e-4b3d-b42c-d89c92707b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.5f}'.format)\n",
      "\n",
      "import operator\n",
      " 1/2:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.5f}'.format)\n",
      "\n",
      "import operator\n",
      " 2/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      " 2/2:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "\n",
      "# Function to compute derivatives\n",
      "def pendulum_rhs(t, y, gamma, L=1):\n",
      "    \"\"\"\n",
      "    Function to compute derivatives for simple pendulum with damping\n",
      "    \n",
      "    Parameters:\n",
      "        t : float\n",
      "            Time\n",
      "        y : array_like\n",
      "            Vector containing [theta, omega], where\n",
      "            theta is the angle and omega is the angular velocity\n",
      "        gamma : float\n",
      "            Damping coefficient\n",
      "        L : float\n",
      "            Length of the pendulum\n",
      "        \n",
      "    Returns:\n",
      "        dydt : array_like\n",
      "            Vector containing [omega, alpha], where\n",
      "            omega is the angular velocity and alpha is the angular acceleration\n",
      "    \"\"\"\n",
      "    theta, omega = y\n",
      "    alpha = - (9.81 / L) * np.sin(theta) - gamma * omega\n",
      "    return [omega, alpha]\n",
      "\n",
      "# Parameters\n",
      "theta0 = np.pi / 4  # Initial angle (radians)\n",
      "omega0 = 0.0        # Initial angular velocity (radians per second)\n",
      "gamma = 0.0       # Damping coefficient\n",
      "L = 1.0             # Length of the pendulum (meters)\n",
      "t_span = (0, 10)    # Time span for the simulation\n",
      "\n",
      "# Function to integrate the system of ODEs\n",
      "def integrate_pendulum(t_span, y0, gamma, L):\n",
      "\n",
      "    sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, y0, method='RK45', t_eval=np.linspace(*t_span, 1000))\n",
      "    return sol\n",
      "\n",
      "# Integrate the pendulum system\n",
      "sol = integrate_pendulum(t_span, [theta0, omega0], gamma, L)\n",
      "\n",
      "# Plot the results\n",
      "plt.figure(figsize=(10, 5))\n",
      "plt.plot(sol.t, sol.y[0], label='Angle (radians)')\n",
      "plt.plot(sol.t, sol.y[1], label='Angular velocity (rad/s)')\n",
      "plt.title('Damped Simple Pendulum Simulation using scipy.solve_ivp')\n",
      "plt.xlabel('Time (s)')\n",
      "plt.ylabel('Value')\n",
      "plt.legend()\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      " 2/3: IC_df = pd.read_csv(\"parameters/init_cond_simp_pend.csv\")\n",
      " 2/4:\n",
      "IC_df = IC_df[0:2]\n",
      "IC_df\n",
      " 2/5:\n",
      "# Mechanical eEnergy level\n",
      "0.5*(IC_df[\"omega\"])**2 + 9.81*(1-np.cos(IC_df[\"theta\"]))\n",
      " 2/6:\n",
      "params_df = pd.read_csv(\"parameters/pend_param.csv\")\n",
      "params_df\n",
      " 2/7: g = 9.81   # Acceleration due to gravity (m/s^2)\n",
      " 2/8:\n",
      "L = 5.0\n",
      "# y_shift = 0.9 * L\n",
      "# y_shift = 0\n",
      "\n",
      "num_time_points = 20\n",
      "# Time span\n",
      "t_span = (0.0, 10)  # from 0 to 10 seconds\n",
      "#Valuation points\n",
      "t_eval_ = np.linspace(t_span[0], t_span[1], num_time_points)\n",
      "data_matrix_df_list = []\n",
      "\n",
      "\n",
      "for param_index in params_df.index:\n",
      "    params = params_df.loc[param_index]\n",
      "    # Define parameters\n",
      "    m_c = params['m_c']  # Mass of the cart (kg)\n",
      "    m_p = params['m_p']  # Mass of the pendulum (kg)\n",
      "    l = params['l']    # Length of the pendulum (m)\n",
      "    for IC_index in IC_df.index:\n",
      "        IC = IC_df.loc[IC_index]\n",
      "        y0 = IC.values\n",
      "                # Parameters\n",
      "        theta0 = IC[\"theta\"]  # Initial angle (radians)\n",
      "        omega0 = IC[\"omega\"]        # Initial angular velocity (radians per second)\n",
      "        gamma = 0.0         # Damping coefficient\n",
      "        # Solve the ODEs\n",
      "        sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, [theta0, omega0], method='RK45', t_eval=t_eval_)\n",
      "        sol_df = pd.DataFrame(sol.y.T, columns=[\"theta\", \"omega\"])\n",
      "        sol_df[\"x\"] = L*np.sin(sol_df[\"theta\"])\n",
      "        sol_df[\"y\"] = -L*np.cos(sol_df[\"theta\"])\n",
      "        sol_df[\"t\"] = t_eval_\n",
      "        data_matrix_df_list.append(sol_df[[\"t\", \"x\", \"y\"]])\n",
      "        # if IC_index == 0:\n",
      "        #     # Plot the results\n",
      "        #     plt.figure(figsize=(10, 6))\n",
      "        #     plt.plot(sol.t, sol.y[0], label='Cart Position (x)')\n",
      "        #     plt.plot(sol.t, sol.y[2], label='Pendulum Angle (theta)')\n",
      "        #     plt.xlabel('Time (s)')\n",
      "        #     plt.ylabel('Position (m) / Angle (rad)')\n",
      "        #     plt.title('Upright Pendulum on Moving Cart')\n",
      "        #     plt.legend()\n",
      "        #     plt.grid(True)\n",
      "        #     plt.show()\n",
      "\n",
      "data_matrix_df = pd.concat(data_matrix_df_list, ignore_index=True)\n",
      "data_matrix_df\n",
      " 2/9:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 10\n",
      "\n",
      "\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"t\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"t\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "2/10: from scipy import interpolate\n",
      "2/11:\n",
      "#smoothing parameter: when equal weightage: num_data_points * std of data\n",
      "s_param = num_time_points * (0.01*noise_perc*data_matrix_df_list[1].std()[\"x\"])**2\n",
      "\n",
      "tck = interpolate.splrep(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], s=s_param)\n",
      "t_eval_new = np.linspace(t_span[0], t_span[1], 500)\n",
      "x_new = interpolate.splev(t_eval_new, tck, der=0)\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "        data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "plt.legend(['Linear', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation')\n",
      "plt.show()\n",
      "2/12: (3/4)*np.pi\n",
      "2/13: data_matrix_df[[\"x\",\"y\"]].plot()\n",
      " 3/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      " 3/2:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "\n",
      "# Function to compute derivatives\n",
      "def pendulum_rhs(t, y, gamma, L=1):\n",
      "    \"\"\"\n",
      "    Function to compute derivatives for simple pendulum with damping\n",
      "    \n",
      "    Parameters:\n",
      "        t : float\n",
      "            Time\n",
      "        y : array_like\n",
      "            Vector containing [theta, omega], where\n",
      "            theta is the angle and omega is the angular velocity\n",
      "        gamma : float\n",
      "            Damping coefficient\n",
      "        L : float\n",
      "            Length of the pendulum\n",
      "        \n",
      "    Returns:\n",
      "        dydt : array_like\n",
      "            Vector containing [omega, alpha], where\n",
      "            omega is the angular velocity and alpha is the angular acceleration\n",
      "    \"\"\"\n",
      "    theta, omega = y\n",
      "    alpha = - (9.81 / L) * np.sin(theta) - gamma * omega\n",
      "    return [omega, alpha]\n",
      "\n",
      "# Parameters\n",
      "theta0 = np.pi / 4  # Initial angle (radians)\n",
      "omega0 = 0.0        # Initial angular velocity (radians per second)\n",
      "gamma = 0.0       # Damping coefficient\n",
      "L = 1.0             # Length of the pendulum (meters)\n",
      "t_span = (0, 10)    # Time span for the simulation\n",
      "\n",
      "# Function to integrate the system of ODEs\n",
      "def integrate_pendulum(t_span, y0, gamma, L):\n",
      "\n",
      "    sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, y0, method='RK45', t_eval=np.linspace(*t_span, 1000))\n",
      "    return sol\n",
      "\n",
      "# Integrate the pendulum system\n",
      "sol = integrate_pendulum(t_span, [theta0, omega0], gamma, L)\n",
      "\n",
      "# Plot the results\n",
      "plt.figure(figsize=(10, 5))\n",
      "plt.plot(sol.t, sol.y[0], label='Angle (radians)')\n",
      "plt.plot(sol.t, sol.y[1], label='Angular velocity (rad/s)')\n",
      "plt.title('Damped Simple Pendulum Simulation using scipy.solve_ivp')\n",
      "plt.xlabel('Time (s)')\n",
      "plt.ylabel('Value')\n",
      "plt.legend()\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      " 3/3: IC_df = pd.read_csv(\"parameters/init_cond_simp_pend.csv\")\n",
      " 3/4:\n",
      "IC_df = IC_df[0:2]\n",
      "IC_df\n",
      " 3/5:\n",
      "# Mechanical eEnergy level\n",
      "0.5*(IC_df[\"omega\"])**2 + 9.81*(1-np.cos(IC_df[\"theta\"]))\n",
      " 3/6:\n",
      "params_df = pd.read_csv(\"parameters/pend_param.csv\")\n",
      "params_df\n",
      " 3/7: g = 9.81   # Acceleration due to gravity (m/s^2)\n",
      " 3/8:\n",
      "L = 5.0\n",
      "# y_shift = 0.9 * L\n",
      "# y_shift = 0\n",
      "\n",
      "num_time_points = 20\n",
      "# Time span\n",
      "t_span = (0.0, 10)  # from 0 to 10 seconds\n",
      "#Valuation points\n",
      "t_eval_ = np.linspace(t_span[0], t_span[1], num_time_points)\n",
      "data_matrix_df_list = []\n",
      "\n",
      "\n",
      "for param_index in params_df.index:\n",
      "    params = params_df.loc[param_index]\n",
      "    # Define parameters\n",
      "    m_c = params['m_c']  # Mass of the cart (kg)\n",
      "    m_p = params['m_p']  # Mass of the pendulum (kg)\n",
      "    l = params['l']    # Length of the pendulum (m)\n",
      "    for IC_index in IC_df.index:\n",
      "        IC = IC_df.loc[IC_index]\n",
      "        y0 = IC.values\n",
      "                # Parameters\n",
      "        theta0 = IC[\"theta\"]  # Initial angle (radians)\n",
      "        omega0 = IC[\"omega\"]        # Initial angular velocity (radians per second)\n",
      "        gamma = 0.0         # Damping coefficient\n",
      "        # Solve the ODEs\n",
      "        sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, [theta0, omega0], method='RK45', t_eval=t_eval_)\n",
      "        sol_df = pd.DataFrame(sol.y.T, columns=[\"theta\", \"omega\"])\n",
      "        sol_df[\"x\"] = L*np.sin(sol_df[\"theta\"])\n",
      "        sol_df[\"y\"] = -L*np.cos(sol_df[\"theta\"])\n",
      "        sol_df[\"t\"] = t_eval_\n",
      "        data_matrix_df_list.append(sol_df[[\"t\", \"x\", \"y\"]])\n",
      "        # if IC_index == 0:\n",
      "        #     # Plot the results\n",
      "        #     plt.figure(figsize=(10, 6))\n",
      "        #     plt.plot(sol.t, sol.y[0], label='Cart Position (x)')\n",
      "        #     plt.plot(sol.t, sol.y[2], label='Pendulum Angle (theta)')\n",
      "        #     plt.xlabel('Time (s)')\n",
      "        #     plt.ylabel('Position (m) / Angle (rad)')\n",
      "        #     plt.title('Upright Pendulum on Moving Cart')\n",
      "        #     plt.legend()\n",
      "        #     plt.grid(True)\n",
      "        #     plt.show()\n",
      "\n",
      "data_matrix_df = pd.concat(data_matrix_df_list, ignore_index=True)\n",
      "data_matrix_df\n",
      " 3/9:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 10\n",
      "\n",
      "\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"t\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"t\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "3/10: from scipy import interpolate\n",
      "3/11:\n",
      "#smoothing parameter: when equal weightage: num_data_points * std of data\n",
      "s_param = num_time_points * (0.01*noise_perc*data_matrix_df_list[1].std()[\"x\"])**2\n",
      "\n",
      "tck = interpolate.splrep(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], s=s_param)\n",
      "t_eval_new = np.linspace(t_span[0], t_span[1], 500)\n",
      "x_new = interpolate.splev(t_eval_new, tck, der=0)\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "        data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "plt.legend(['Linear', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation')\n",
      "plt.show()\n",
      "3/12: (3/4)*np.pi\n",
      "3/13: data_matrix_df[[\"x\",\"y\"]].plot()\n",
      "3/14:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      " 4/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      " 4/2:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "\n",
      "# Function to compute derivatives\n",
      "def pendulum_rhs(t, y, gamma, L=1):\n",
      "    \"\"\"\n",
      "    Function to compute derivatives for simple pendulum with damping\n",
      "    \n",
      "    Parameters:\n",
      "        t : float\n",
      "            Time\n",
      "        y : array_like\n",
      "            Vector containing [theta, omega], where\n",
      "            theta is the angle and omega is the angular velocity\n",
      "        gamma : float\n",
      "            Damping coefficient\n",
      "        L : float\n",
      "            Length of the pendulum\n",
      "        \n",
      "    Returns:\n",
      "        dydt : array_like\n",
      "            Vector containing [omega, alpha], where\n",
      "            omega is the angular velocity and alpha is the angular acceleration\n",
      "    \"\"\"\n",
      "    theta, omega = y\n",
      "    alpha = - (9.81 / L) * np.sin(theta) - gamma * omega\n",
      "    return [omega, alpha]\n",
      "\n",
      "# Parameters\n",
      "theta0 = np.pi / 4  # Initial angle (radians)\n",
      "omega0 = 0.0        # Initial angular velocity (radians per second)\n",
      "gamma = 0.0       # Damping coefficient\n",
      "L = 1.0             # Length of the pendulum (meters)\n",
      "t_span = (0, 10)    # Time span for the simulation\n",
      "\n",
      "# Function to integrate the system of ODEs\n",
      "def integrate_pendulum(t_span, y0, gamma, L):\n",
      "\n",
      "    sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, y0, method='RK45', t_eval=np.linspace(*t_span, 1000))\n",
      "    return sol\n",
      "\n",
      "# Integrate the pendulum system\n",
      "sol = integrate_pendulum(t_span, [theta0, omega0], gamma, L)\n",
      "\n",
      "# Plot the results\n",
      "plt.figure(figsize=(10, 5))\n",
      "plt.plot(sol.t, sol.y[0], label='Angle (radians)')\n",
      "plt.plot(sol.t, sol.y[1], label='Angular velocity (rad/s)')\n",
      "plt.title('Damped Simple Pendulum Simulation using scipy.solve_ivp')\n",
      "plt.xlabel('Time (s)')\n",
      "plt.ylabel('Value')\n",
      "plt.legend()\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      " 4/3: IC_df = pd.read_csv(\"parameters/init_cond_simp_pend.csv\")\n",
      " 4/4:\n",
      "IC_df = IC_df[0:2]\n",
      "IC_df\n",
      " 4/5:\n",
      "# Mechanical eEnergy level\n",
      "0.5*(IC_df[\"omega\"])**2 + 9.81*(1-np.cos(IC_df[\"theta\"]))\n",
      " 4/6:\n",
      "params_df = pd.read_csv(\"parameters/pend_param.csv\")\n",
      "params_df\n",
      " 4/7: g = 9.81   # Acceleration due to gravity (m/s^2)\n",
      " 4/8:\n",
      "L = 5.0\n",
      "# y_shift = 0.9 * L\n",
      "# y_shift = 0\n",
      "\n",
      "num_time_points = 20\n",
      "# Time span\n",
      "t_span = (0.0, 10)  # from 0 to 10 seconds\n",
      "#Valuation points\n",
      "t_eval_ = np.linspace(t_span[0], t_span[1], num_time_points)\n",
      "data_matrix_df_list = []\n",
      "\n",
      "\n",
      "for param_index in params_df.index:\n",
      "    params = params_df.loc[param_index]\n",
      "    # Define parameters\n",
      "    m_c = params['m_c']  # Mass of the cart (kg)\n",
      "    m_p = params['m_p']  # Mass of the pendulum (kg)\n",
      "    l = params['l']    # Length of the pendulum (m)\n",
      "    for IC_index in IC_df.index:\n",
      "        IC = IC_df.loc[IC_index]\n",
      "        y0 = IC.values\n",
      "                # Parameters\n",
      "        theta0 = IC[\"theta\"]  # Initial angle (radians)\n",
      "        omega0 = IC[\"omega\"]        # Initial angular velocity (radians per second)\n",
      "        gamma = 0.0         # Damping coefficient\n",
      "        # Solve the ODEs\n",
      "        sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, [theta0, omega0], method='RK45', t_eval=t_eval_)\n",
      "        sol_df = pd.DataFrame(sol.y.T, columns=[\"theta\", \"omega\"])\n",
      "        sol_df[\"x\"] = L*np.sin(sol_df[\"theta\"])\n",
      "        sol_df[\"y\"] = -L*np.cos(sol_df[\"theta\"])\n",
      "        sol_df[\"t\"] = t_eval_\n",
      "        data_matrix_df_list.append(sol_df[[\"t\", \"x\", \"y\"]])\n",
      "        # if IC_index == 0:\n",
      "        #     # Plot the results\n",
      "        #     plt.figure(figsize=(10, 6))\n",
      "        #     plt.plot(sol.t, sol.y[0], label='Cart Position (x)')\n",
      "        #     plt.plot(sol.t, sol.y[2], label='Pendulum Angle (theta)')\n",
      "        #     plt.xlabel('Time (s)')\n",
      "        #     plt.ylabel('Position (m) / Angle (rad)')\n",
      "        #     plt.title('Upright Pendulum on Moving Cart')\n",
      "        #     plt.legend()\n",
      "        #     plt.grid(True)\n",
      "        #     plt.show()\n",
      "\n",
      "data_matrix_df = pd.concat(data_matrix_df_list, ignore_index=True)\n",
      "data_matrix_df\n",
      " 4/9:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 10\n",
      "\n",
      "\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"t\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"t\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "4/10: from scipy import interpolate\n",
      "4/11:\n",
      "#smoothing parameter: when equal weightage: num_data_points * std of data\n",
      "s_param = num_time_points * (0.01*noise_perc*data_matrix_df_list[1].std()[\"x\"])**2\n",
      "\n",
      "tck = interpolate.splrep(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], s=s_param)\n",
      "t_eval_new = np.linspace(t_span[0], t_span[1], 500)\n",
      "x_new = interpolate.splev(t_eval_new, tck, der=0)\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "        data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "plt.legend(['Linear', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation')\n",
      "plt.show()\n",
      "4/12: (3/4)*np.pi\n",
      "4/13: data_matrix_df[[\"x\",\"y\"]].plot()\n",
      "4/14:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "num_smoothed_points = num_time_points*10\n",
      "\n",
      "t_eval_new = np.linspace(data_matrix_df_list[0][\"t\"].iloc[0], data_matrix_df_list[0][\"t\"].iloc[-1], num_smoothed_points)\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,derr_order=1, noise_perc=noise_perc, eval_points=t_eval_new) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list[:-1], ignore_index=True)\n",
      "4/15:\n",
      "ind = 1\n",
      "feature_ = \"y\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"t\"], data_matrix_df_list[ind][feature_], \"x\", data_matrix_smooth_df_list[ind][\"t\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"t\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \"o\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "4/16: data_matrix_smooth_df_list[0][[\"d(x) /dt\"]].plot()\n",
      "4/17: data_matrix_smooth_df_list[0][[\"d(x) /dt\"]].plot()\n",
      "4/18:\n",
      "# data_matrix_df = data_matrix_df_appended[[\"x\",\"y\"]]\n",
      "# data_matrix_df = pd.concat([data_matrix_df, data_matrix_df_appended[[\"d(u) /dt\"]]], axis=1)\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[[\"x\",\"y\", \"d(x) /dt\", \"d(y) /dt\"]]\n",
      "data_matrix_df_smooth\n",
      "4/19:\n",
      "from copy import deepcopy\n",
      "new_df = deepcopy(data_matrix_df_smooth)\n",
      "\n",
      "new_df[\"energy\"] = 0.5*((new_df[\"d(x) /dt\"])**2 + (new_df[\"d(y) /dt\"])**2) +  9.81*new_df[\"y\"]\n",
      "\n",
      "new_df.plot()\n",
      "4/20:\n",
      "from sklearn.preprocessing import FunctionTransformer\n",
      "from copy import deepcopy\n",
      "\n",
      "def sin_transformer(period = 2*np.pi):\n",
      "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
      "\n",
      "\n",
      "def sin_diff_transformer(period = 2*np.pi):\n",
      "    return FunctionTransformer(lambda x,y: np.sin((x-y) / period * 2 * np.pi))\n",
      "\n",
      "def cos_transformer(period = 2*np.pi):\n",
      "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
      "\n",
      "\n",
      "data_matrix_df_with_trig = deepcopy(data_matrix_df)\n",
      "# data_matrix_df_with_trig[\"sin(theta)\"] = sin_transformer(1).fit_transform(data_matrix_df_with_trig)[\"theta\"]\n",
      "# data_matrix_df_with_trig[\"cos(theta)\"] = cos_transformer(1).fit_transform(data_matrix_df_with_trig)[\"theta\"]\n",
      "4/21:\n",
      "from scipy.sparse import coo_array\n",
      "\n",
      "row  = np.array([0, 3, 1, 0, 0, 0])\n",
      "col  = np.array([0, 3, 1, 1, 2, 4])\n",
      "data = np.array([4, 5, 7, 9, 100, 101])\n",
      "# sp_array = coo_array((data, (row, col)), shape=(4, 4))\n",
      "sp_array = coo_array((data, (row, col)))\n",
      "4/22: list(zip(sp_array.row, sp_array.col))\n",
      "4/23: sp_array.col\n",
      "4/24: sp_array.toarray()[0]\n",
      "4/25: sp_ar\n",
      "4/26: new_sp = sp_array.tocsr()\n",
      "4/27:\n",
      "class FeatureDiffTranformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = order\n",
      "\n",
      "     def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "4/29:\n",
      "class FeatureDiffTranformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = order\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "4/30:\n",
      "from sklearn.preprocessing import TransformerMixin, BaseEstimator\n",
      "\n",
      "class FeatureDiffTranformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = order\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "4/31:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "\n",
      "class FeatureDiffTranformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = order\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "4/32:\n",
      "from scipy.sparse import coo_array\n",
      "\n",
      "row  = np.array([0, 3, 1, 0, 0, 0])\n",
      "col  = np.array([0, 3, 1, 1, 2, 4])\n",
      "data = np.array([4, 5, 7, 9, 100, 101])\n",
      "# sp_array = coo_array((data, (row, col)), shape=(4, 4))\n",
      "sp_array = coo_array((data, (row, col)))\n",
      "4/33: FeatureDiffTranformer(sp_array, power=1)\n",
      "4/34:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "\n",
      "class FeatureDiffTranformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "4/35: FeatureDiffTranformer(sp_array, power=1)\n",
      "4/36:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "4/37: FeatureDiffTransformer(sp_array, power=1)\n",
      "4/38: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/39:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        check_is_fitted(self)\n",
      "\n",
      "        X = self._validate_data(\n",
      "            X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        )\n",
      "\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/40: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/41: data_matrix_\n",
      "4/42: dummy_tr.fit_transform(data_matrix_)\n",
      "4/43: dummy_tr.transform(data_matrix_)\n",
      "4/44:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        check_is_fitted(self)\n",
      "\n",
      "        X = self._validate_data(\n",
      "            X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        )\n",
      "\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/45: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/46: dummy_tr.transform(data_matrix_)\n",
      "4/47:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        check_is_fitted(self)\n",
      "\n",
      "        X = self._validate_data(\n",
      "            X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        )\n",
      "\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "\n",
      "        def fit(self, X, y=None):\n",
      "            return self\n",
      "4/48: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/49: dummy_tr.transform(data_matrix_)\n",
      "4/50: dummy_tr.fit_transform(data_matrix_)\n",
      "4/51:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        return self\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        check_is_fitted(self)\n",
      "\n",
      "        X = self._validate_data(\n",
      "            X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        )\n",
      "\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/52: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/53: dummy_tr.fit_transform(data_matrix_)\n",
      "4/54: dummy_tr.transform(data_matrix_)\n",
      "4/55:\n",
      "dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.transform(data_matrix_)\n",
      "4/56:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        return self\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        # check_is_fitted(self)\n",
      "\n",
      "        X = self._validate_data(\n",
      "            X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        )\n",
      "\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/57: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/58:\n",
      "dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.transform(data_matrix_)\n",
      "4/59: dummy_tr.get_feature_names_out()\n",
      "4/60: dummy_tr._check_n_features(X)\n",
      "4/61: dummy_tr._check_n_features(data_matrix_)\n",
      "4/62: dummy_tr._check_n_features(data_matrix_, 0)\n",
      "4/63:\n",
      "dummy_tr._check_n_features(data_matrix_, 0)\n",
      "dummy_tr._check_feature_names(data_matrix_, reset = 0):\n",
      "4/64:\n",
      "dummy_tr._check_n_features(data_matrix_, 0)\n",
      "dummy_tr._check_feature_names(data_matrix_, reset = 0)\n",
      "4/65:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        return self\n",
      "\n",
      "    # def get_feature_names_out(self, input_features=None):\n",
      "    #     \"\"\"Get output feature names for transformation.\n",
      "\n",
      "    #     Parameters\n",
      "    #     ----------\n",
      "    #     input_features : array-like of str or None, default=None\n",
      "    #         Input features.\n",
      "\n",
      "    #         - If `input_features is None`, then `feature_names_in_` is\n",
      "    #           used as feature names in. If `feature_names_in_` is not defined,\n",
      "    #           then the following input feature names are generated:\n",
      "    #           `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "    #         - If `input_features` is an array-like, then `input_features` must\n",
      "    #           match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "    #     Returns\n",
      "    #     -------\n",
      "    #     feature_names_out : ndarray of str objects\n",
      "    #         Transformed feature names.\n",
      "    #     \"\"\"\n",
      "    #     power = self.power\n",
      "    #     input_features = _check_feature_names_in(self, input_features)\n",
      "    #     feature_names = [feature for feature in input_features]\n",
      "    #     # for row in powers:\n",
      "    #     #     inds = np.where(row)[0]\n",
      "    #     #     if len(inds):\n",
      "    #     #         name = \" \".join(\n",
      "    #     #             (\n",
      "    #     #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "    #     #                 if exp != 1\n",
      "    #     #                 else input_features[ind]\n",
      "    #     #             )\n",
      "    #     #             for ind, exp in zip(inds, row[inds])\n",
      "    #     #         )\n",
      "    #     #     else:\n",
      "    #     #         name = \"1\"\n",
      "    #     #     feature_names.append(name)\n",
      "    #     return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        # check_is_fitted(self)\n",
      "\n",
      "        X = self._validate_data(\n",
      "            X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        )\n",
      "\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/66: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/67:\n",
      "dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.transform(data_matrix_)\n",
      "4/68: dummy_tr.get_feature_names_out()\n",
      "4/69: data_matrix_.shape\n",
      "4/70:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        return self\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        # check_is_fitted(self)\n",
      "\n",
      "        X = self._validate_data(\n",
      "            X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        )\n",
      "        n_features_in_ = X.shape[1]\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/71: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/72:\n",
      "dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.transform(data_matrix_)\n",
      "4/73: dummy_tr.get_feature_names_out()\n",
      "4/74:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        return self\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        # check_is_fitted(self)\n",
      "\n",
      "        X = self._validate_data(\n",
      "            X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        )\n",
      "        self.n_features_in_ = X.shape[1]\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/75: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/76:\n",
      "dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.transform(data_matrix_)\n",
      "4/77: dummy_tr.get_feature_names_out()\n",
      "4/78:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        return self\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        # check_is_fitted(self)\n",
      "\n",
      "        X = self._validate_data(\n",
      "            X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        )\n",
      "        self.n_features_in_ = X.shape[1]\n",
      "        if len(X.columns) >0:\n",
      "            self.feature_names_in_ = X.columns\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/79: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/80:\n",
      "# dummy_tr._check_n_features(data_matrix_, 0)\n",
      "# dummy_tr._check_feature_names(data_matrix_, reset = 0)\n",
      "4/81:\n",
      "dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.transform(data_matrix_)\n",
      "4/82:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        return self\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        # check_is_fitted(self)\n",
      "\n",
      "        # X = self._validate_data(\n",
      "        #     X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        # )\n",
      "        self.n_features_in_ = X.shape[1]\n",
      "        if len(X.columns) >0:\n",
      "            self.feature_names_in_ = X.columns\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/83: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/84:\n",
      "# dummy_tr._check_n_features(data_matrix_, 0)\n",
      "# dummy_tr._check_feature_names(data_matrix_, reset = 0)\n",
      "4/85:\n",
      "dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.transform(data_matrix_)\n",
      "4/86: dummy_tr.get_feature_names_out()\n",
      "4/87:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        self.n_features_in_ = X.shape[1]\n",
      "        if len(X.columns) >0:\n",
      "            self.feature_names_in_ = X.columns\n",
      "        return self\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        # check_is_fitted(self)\n",
      "\n",
      "        # X = self._validate_data(\n",
      "        #     X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        # )\n",
      "        # self.n_features_in_ = X.shape[1]\n",
      "        # if len(X.columns) >0:\n",
      "        #     self.feature_names_in_ = X.columns\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/88: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/89:\n",
      "dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.transform(data_matrix_)\n",
      "4/90: dummy_tr.get_feature_names_out()\n",
      "4/91: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/92:\n",
      "# dummy_tr._check_n_features(data_matrix_, 0)\n",
      "# dummy_tr._check_feature_names(data_matrix_, reset = 0)\n",
      "4/93:\n",
      "# dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.transform(data_matrix_)\n",
      "4/94:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        self.n_features_in_ = X.shape[1]\n",
      "        if len(X.columns) >0:\n",
      "            self.feature_names_in_ = X.columns\n",
      "        return self\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        check_is_fitted(self)\n",
      "\n",
      "        # X = self._validate_data(\n",
      "        #     X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        # )\n",
      "        # self.n_features_in_ = X.shape[1]\n",
      "        # if len(X.columns) >0:\n",
      "        #     self.feature_names_in_ = X.columns\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/95: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/96:\n",
      "# dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.transform(data_matrix_)\n",
      "4/97:\n",
      "# dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.fittransform(data_matrix_)\n",
      "4/98:\n",
      "dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.transform(data_matrix_)\n",
      "4/99:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        self.n_features_in_ = X.shape[1]\n",
      "        if len(X.columns) >0:\n",
      "            self.feature_names_in_ = X.columns\n",
      "        return self\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        check_is_fitted(self)\n",
      "\n",
      "        # X = self._validate_data(\n",
      "        #     X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        # )\n",
      "        # self.n_features_in_ = X.shape[1]\n",
      "        # if len(X.columns) >0:\n",
      "        #     self.feature_names_in_ = X.columns\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/100: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/101:\n",
      "# dummy_tr._check_n_features(data_matrix_, 0)\n",
      "# dummy_tr._check_feature_names(data_matrix_, reset = 0)\n",
      "4/102:\n",
      "dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.transform(data_matrix_)\n",
      "4/103: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/104:\n",
      "# dummy_tr._check_n_features(data_matrix_, 0)\n",
      "# dummy_tr._check_feature_names(data_matrix_, reset = 0)\n",
      "4/105:\n",
      "# dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.fit_transform(data_matrix_)\n",
      "4/106: dummy_tr.get_feature_names_out()\n",
      "4/107: sp_array.ndim\n",
      "4/108: sp_arra.col\n",
      "4/109: sp_array.col\n",
      "4/110: sp_array.rows\n",
      "4/111: sp_array.row\n",
      "4/112: sp_array.col.max\n",
      "4/113: sp_array.col.max()\n",
      "4/114: sp_array.row.max()\n",
      "4/115: max(sp_array.col.max(), sp_array.row.max())\n",
      "4/116:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "        self.n_features_in_ = 0\n",
      "        self.feature_names_in_ = None\n",
      "\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        self.n_features_in_ = X.shape[1]\n",
      "        assert max(self.sparsity_matrix.col.max(), self.sparsity_matrixrow.max()) <=self.n_features_in_-1,\n",
      "        \"sparsity matrix has indices out of bound of the number of features\"\n",
      "        if len(X.columns) >0:\n",
      "            self.feature_names_in_ = X.columns\n",
      "        return self\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        check_is_fitted(self)\n",
      "\n",
      "        # X = self._validate_data(\n",
      "        #     X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        # )\n",
      "        # self.n_features_in_ = X.shape[1]\n",
      "        # if len(X.columns) >0:\n",
      "        #     self.feature_names_in_ = X.columns\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/117:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "        self.n_features_in_ = 0\n",
      "        self.feature_names_in_ = None\n",
      "\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        self.n_features_in_ = X.shape[1]\n",
      "        assert max(self.sparsity_matrix.col.max(), self.sparsity_matrixrow.max()) <=self.n_features_in_-1, \\\n",
      "        \"sparsity matrix has indices out of bound of the number of features\"\n",
      "        if len(X.columns) >0:\n",
      "            self.feature_names_in_ = X.columns\n",
      "        return self\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        check_is_fitted(self)\n",
      "\n",
      "        # X = self._validate_data(\n",
      "        #     X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        # )\n",
      "        # self.n_features_in_ = X.shape[1]\n",
      "        # if len(X.columns) >0:\n",
      "        #     self.feature_names_in_ = X.columns\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/118:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "        self.n_features_in_ = 0\n",
      "        self.feature_names_in_ = None\n",
      "\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        self.n_features_in_ = X.shape[1]\n",
      "        assert max(self.sparsity_matrix.col.max(), self.sparsity_matrixrow.max()) <=self.n_features_in_-1, \\\n",
      "        \"sparsity matrix has indices out of bound of the number of features\"\n",
      "        if len(X.columns) >0:\n",
      "            self.feature_names_in_ = X.columns\n",
      "        return self\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        check_is_fitted(self)\n",
      "\n",
      "        X = self._validate_data(\n",
      "            X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        )\n",
      "        # self.n_features_in_ = X.shape[1]\n",
      "        # if len(X.columns) >0:\n",
      "        #     self.feature_names_in_ = X.columns\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/119: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/120:\n",
      "# dummy_tr._check_n_features(data_matrix_, 0)\n",
      "# dummy_tr._check_feature_names(data_matrix_, reset = 0)\n",
      "4/121:\n",
      "# dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.fit_transform(data_matrix_)\n",
      "4/122:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "        self.n_features_in_ = 0\n",
      "        self.feature_names_in_ = None\n",
      "\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        self.n_features_in_ = X.shape[1]\n",
      "        assert max(self.sparsity_matrix.col.max(), self.sparsity_matrix.row.max()) <=self.n_features_in_-1, \\\n",
      "        \"sparsity matrix has indices out of bound of the number of features\"\n",
      "        if len(X.columns) >0:\n",
      "            self.feature_names_in_ = X.columns\n",
      "        return self\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        check_is_fitted(self)\n",
      "\n",
      "        X = self._validate_data(\n",
      "            X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        )\n",
      "        # self.n_features_in_ = X.shape[1]\n",
      "        # if len(X.columns) >0:\n",
      "        #     self.feature_names_in_ = X.columns\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/123: dummy_tr = FeatureDiffTransformer(sp_array, power=1)\n",
      "4/124:\n",
      "# dummy_tr._check_n_features(data_matrix_, 0)\n",
      "# dummy_tr._check_feature_names(data_matrix_, reset = 0)\n",
      "4/125:\n",
      "# dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.fit_transform(data_matrix_)\n",
      "4/126:\n",
      "from scipy.sparse import coo_array\n",
      "\n",
      "row  = np.array([0, 0, 1])\n",
      "col  = np.array([0, 2, 2])\n",
      "data = np.array([4, 5, 7])\n",
      "# sp_array = coo_array((data, (row, col)), shape=(4, 4))\n",
      "sp_array_2 = coo_array((data, (row, col)))\n",
      "\n",
      "\n",
      "dummy_tr = FeatureDiffTransformer(sp_array_2, power=1)\n",
      "4/127:\n",
      "# dummy_tr._check_n_features(data_matrix_, 0)\n",
      "# dummy_tr._check_feature_names(data_matrix_, reset = 0)\n",
      "4/128:\n",
      "# dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.fit_transform(data_matrix_)\n",
      "4/129: dummy_tr.get_feature_names_out()\n",
      "4/130:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from scipy.sparse import coo_array\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "        self.n_features_in_ = 0\n",
      "        self.feature_names_in_ = None\n",
      "\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        self.n_features_in_ = X.shape[1]\n",
      "        assert max(self.sparsity_matrix.col.max(), self.sparsity_matrix.row.max()) <=self.n_features_in_-1, \\\n",
      "        \"sparsity matrix has indices out of bound of the number of features\"\n",
      "        if len(X.columns) >0:\n",
      "            self.feature_names_in_ = X.columns\n",
      "        return self\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        check_is_fitted(self)\n",
      "\n",
      "        X = self._validate_data(\n",
      "            X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        )\n",
      "        # self.n_features_in_ = X.shape[1]\n",
      "        # if len(X.columns) >0:\n",
      "        #     self.feature_names_in_ = X.columns\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "4/131: isinstance??\n",
      "4/132: sp_array.coords\n",
      "4/133: sp_array.toarray\n",
      "4/134: sp_array.toarray()\n",
      "4/135: sp_array.size\n",
      "4/136: sp_array.sizes\n",
      "4/137: sp_array.size\n",
      "4/138: sp_array.nnz\n",
      "4/139: coo_array(sp_array.toarray())\n",
      "4/140: coo_array(sp_array.toarray()).size\n",
      "4/141: coo_array(sp_array.toarray()).nnz\n",
      " 5/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      " 5/2:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "\n",
      "# Function to compute derivatives\n",
      "def pendulum_rhs(t, y, gamma, L=1):\n",
      "    \"\"\"\n",
      "    Function to compute derivatives for simple pendulum with damping\n",
      "    \n",
      "    Parameters:\n",
      "        t : float\n",
      "            Time\n",
      "        y : array_like\n",
      "            Vector containing [theta, omega], where\n",
      "            theta is the angle and omega is the angular velocity\n",
      "        gamma : float\n",
      "            Damping coefficient\n",
      "        L : float\n",
      "            Length of the pendulum\n",
      "        \n",
      "    Returns:\n",
      "        dydt : array_like\n",
      "            Vector containing [omega, alpha], where\n",
      "            omega is the angular velocity and alpha is the angular acceleration\n",
      "    \"\"\"\n",
      "    theta, omega = y\n",
      "    alpha = - (9.81 / L) * np.sin(theta) - gamma * omega\n",
      "    return [omega, alpha]\n",
      "\n",
      "# Parameters\n",
      "theta0 = np.pi / 4  # Initial angle (radians)\n",
      "omega0 = 0.0        # Initial angular velocity (radians per second)\n",
      "gamma = 0.0       # Damping coefficient\n",
      "L = 1.0             # Length of the pendulum (meters)\n",
      "t_span = (0, 10)    # Time span for the simulation\n",
      "\n",
      "# Function to integrate the system of ODEs\n",
      "def integrate_pendulum(t_span, y0, gamma, L):\n",
      "\n",
      "    sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, y0, method='RK45', t_eval=np.linspace(*t_span, 1000))\n",
      "    return sol\n",
      "\n",
      "# Integrate the pendulum system\n",
      "sol = integrate_pendulum(t_span, [theta0, omega0], gamma, L)\n",
      "\n",
      "# Plot the results\n",
      "plt.figure(figsize=(10, 5))\n",
      "plt.plot(sol.t, sol.y[0], label='Angle (radians)')\n",
      "plt.plot(sol.t, sol.y[1], label='Angular velocity (rad/s)')\n",
      "plt.title('Damped Simple Pendulum Simulation using scipy.solve_ivp')\n",
      "plt.xlabel('Time (s)')\n",
      "plt.ylabel('Value')\n",
      "plt.legend()\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      " 5/3: IC_df = pd.read_csv(\"parameters/init_cond_simp_pend.csv\")\n",
      " 5/4:\n",
      "IC_df = IC_df[0:2]\n",
      "IC_df\n",
      " 5/5:\n",
      "# Mechanical eEnergy level\n",
      "0.5*(IC_df[\"omega\"])**2 + 9.81*(1-np.cos(IC_df[\"theta\"]))\n",
      " 5/6:\n",
      "params_df = pd.read_csv(\"parameters/pend_param.csv\")\n",
      "params_df\n",
      " 5/7: g = 9.81   # Acceleration due to gravity (m/s^2)\n",
      " 5/8:\n",
      "L = 5.0\n",
      "# y_shift = 0.9 * L\n",
      "# y_shift = 0\n",
      "\n",
      "num_time_points = 20\n",
      "# Time span\n",
      "t_span = (0.0, 10)  # from 0 to 10 seconds\n",
      "#Valuation points\n",
      "t_eval_ = np.linspace(t_span[0], t_span[1], num_time_points)\n",
      "data_matrix_df_list = []\n",
      "\n",
      "\n",
      "for param_index in params_df.index:\n",
      "    params = params_df.loc[param_index]\n",
      "    # Define parameters\n",
      "    m_c = params['m_c']  # Mass of the cart (kg)\n",
      "    m_p = params['m_p']  # Mass of the pendulum (kg)\n",
      "    l = params['l']    # Length of the pendulum (m)\n",
      "    for IC_index in IC_df.index:\n",
      "        IC = IC_df.loc[IC_index]\n",
      "        y0 = IC.values\n",
      "                # Parameters\n",
      "        theta0 = IC[\"theta\"]  # Initial angle (radians)\n",
      "        omega0 = IC[\"omega\"]        # Initial angular velocity (radians per second)\n",
      "        gamma = 0.0         # Damping coefficient\n",
      "        # Solve the ODEs\n",
      "        sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, [theta0, omega0], method='RK45', t_eval=t_eval_)\n",
      "        sol_df = pd.DataFrame(sol.y.T, columns=[\"theta\", \"omega\"])\n",
      "        sol_df[\"x\"] = L*np.sin(sol_df[\"theta\"])\n",
      "        sol_df[\"y\"] = -L*np.cos(sol_df[\"theta\"])\n",
      "        sol_df[\"t\"] = t_eval_\n",
      "        data_matrix_df_list.append(sol_df[[\"t\", \"x\", \"y\"]])\n",
      "        # if IC_index == 0:\n",
      "        #     # Plot the results\n",
      "        #     plt.figure(figsize=(10, 6))\n",
      "        #     plt.plot(sol.t, sol.y[0], label='Cart Position (x)')\n",
      "        #     plt.plot(sol.t, sol.y[2], label='Pendulum Angle (theta)')\n",
      "        #     plt.xlabel('Time (s)')\n",
      "        #     plt.ylabel('Position (m) / Angle (rad)')\n",
      "        #     plt.title('Upright Pendulum on Moving Cart')\n",
      "        #     plt.legend()\n",
      "        #     plt.grid(True)\n",
      "        #     plt.show()\n",
      "\n",
      "data_matrix_df = pd.concat(data_matrix_df_list, ignore_index=True)\n",
      "data_matrix_df\n",
      " 5/9:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 10\n",
      "\n",
      "\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"t\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"t\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "5/10: (3/4)*np.pi\n",
      "5/11: data_matrix_df[[\"x\",\"y\"]].plot()\n",
      "5/12:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "num_smoothed_points = num_time_points*10\n",
      "\n",
      "t_eval_new = np.linspace(data_matrix_df_list[0][\"t\"].iloc[0], data_matrix_df_list[0][\"t\"].iloc[-1], num_smoothed_points)\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,derr_order=1, noise_perc=noise_perc, eval_points=t_eval_new) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list[:-1], ignore_index=True)\n",
      "5/13:\n",
      "ind = 1\n",
      "feature_ = \"y\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"t\"], data_matrix_df_list[ind][feature_], \"x\", data_matrix_smooth_df_list[ind][\"t\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"t\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \"o\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      " 6/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      " 6/2:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "\n",
      "# Function to compute derivatives\n",
      "def pendulum_rhs(t, y, gamma, L=1):\n",
      "    \"\"\"\n",
      "    Function to compute derivatives for simple pendulum with damping\n",
      "    \n",
      "    Parameters:\n",
      "        t : float\n",
      "            Time\n",
      "        y : array_like\n",
      "            Vector containing [theta, omega], where\n",
      "            theta is the angle and omega is the angular velocity\n",
      "        gamma : float\n",
      "            Damping coefficient\n",
      "        L : float\n",
      "            Length of the pendulum\n",
      "        \n",
      "    Returns:\n",
      "        dydt : array_like\n",
      "            Vector containing [omega, alpha], where\n",
      "            omega is the angular velocity and alpha is the angular acceleration\n",
      "    \"\"\"\n",
      "    theta, omega = y\n",
      "    alpha = - (9.81 / L) * np.sin(theta) - gamma * omega\n",
      "    return [omega, alpha]\n",
      "\n",
      "# Parameters\n",
      "theta0 = np.pi / 4  # Initial angle (radians)\n",
      "omega0 = 0.0        # Initial angular velocity (radians per second)\n",
      "gamma = 0.0       # Damping coefficient\n",
      "L = 1.0             # Length of the pendulum (meters)\n",
      "t_span = (0, 10)    # Time span for the simulation\n",
      "\n",
      "# Function to integrate the system of ODEs\n",
      "def integrate_pendulum(t_span, y0, gamma, L):\n",
      "\n",
      "    sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, y0, method='RK45', t_eval=np.linspace(*t_span, 1000))\n",
      "    return sol\n",
      "\n",
      "# Integrate the pendulum system\n",
      "sol = integrate_pendulum(t_span, [theta0, omega0], gamma, L)\n",
      "\n",
      "# Plot the results\n",
      "plt.figure(figsize=(10, 5))\n",
      "plt.plot(sol.t, sol.y[0], label='Angle (radians)')\n",
      "plt.plot(sol.t, sol.y[1], label='Angular velocity (rad/s)')\n",
      "plt.title('Damped Simple Pendulum Simulation using scipy.solve_ivp')\n",
      "plt.xlabel('Time (s)')\n",
      "plt.ylabel('Value')\n",
      "plt.legend()\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      " 6/3: IC_df = pd.read_csv(\"parameters/init_cond_simp_pend.csv\")\n",
      " 6/4:\n",
      "IC_df = IC_df[0:2]\n",
      "IC_df\n",
      " 6/5:\n",
      "# Mechanical eEnergy level\n",
      "0.5*(IC_df[\"omega\"])**2 + 9.81*(1-np.cos(IC_df[\"theta\"]))\n",
      " 6/6:\n",
      "params_df = pd.read_csv(\"parameters/pend_param.csv\")\n",
      "params_df\n",
      " 6/7: g = 9.81   # Acceleration due to gravity (m/s^2)\n",
      " 6/8:\n",
      "L = 5.0\n",
      "# y_shift = 0.9 * L\n",
      "# y_shift = 0\n",
      "\n",
      "num_time_points = 20\n",
      "# Time span\n",
      "t_span = (0.0, 10)  # from 0 to 10 seconds\n",
      "#Valuation points\n",
      "t_eval_ = np.linspace(t_span[0], t_span[1], num_time_points)\n",
      "data_matrix_df_list = []\n",
      "\n",
      "\n",
      "for param_index in params_df.index:\n",
      "    params = params_df.loc[param_index]\n",
      "    # Define parameters\n",
      "    m_c = params['m_c']  # Mass of the cart (kg)\n",
      "    m_p = params['m_p']  # Mass of the pendulum (kg)\n",
      "    l = params['l']    # Length of the pendulum (m)\n",
      "    for IC_index in IC_df.index:\n",
      "        IC = IC_df.loc[IC_index]\n",
      "        y0 = IC.values\n",
      "                # Parameters\n",
      "        theta0 = IC[\"theta\"]  # Initial angle (radians)\n",
      "        omega0 = IC[\"omega\"]        # Initial angular velocity (radians per second)\n",
      "        gamma = 0.0         # Damping coefficient\n",
      "        # Solve the ODEs\n",
      "        sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, [theta0, omega0], method='RK45', t_eval=t_eval_)\n",
      "        sol_df = pd.DataFrame(sol.y.T, columns=[\"theta\", \"omega\"])\n",
      "        sol_df[\"x\"] = L*np.sin(sol_df[\"theta\"])\n",
      "        sol_df[\"y\"] = -L*np.cos(sol_df[\"theta\"])\n",
      "        sol_df[\"t\"] = t_eval_\n",
      "        data_matrix_df_list.append(sol_df[[\"t\", \"x\", \"y\"]])\n",
      "        # if IC_index == 0:\n",
      "        #     # Plot the results\n",
      "        #     plt.figure(figsize=(10, 6))\n",
      "        #     plt.plot(sol.t, sol.y[0], label='Cart Position (x)')\n",
      "        #     plt.plot(sol.t, sol.y[2], label='Pendulum Angle (theta)')\n",
      "        #     plt.xlabel('Time (s)')\n",
      "        #     plt.ylabel('Position (m) / Angle (rad)')\n",
      "        #     plt.title('Upright Pendulum on Moving Cart')\n",
      "        #     plt.legend()\n",
      "        #     plt.grid(True)\n",
      "        #     plt.show()\n",
      "\n",
      "data_matrix_df = pd.concat(data_matrix_df_list, ignore_index=True)\n",
      "data_matrix_df\n",
      " 6/9:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 10\n",
      "\n",
      "\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"t\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"t\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "6/10: !conda uninstall scipy\n",
      " 8/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      " 8/2:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "\n",
      "# Function to compute derivatives\n",
      "def pendulum_rhs(t, y, gamma, L=1):\n",
      "    \"\"\"\n",
      "    Function to compute derivatives for simple pendulum with damping\n",
      "    \n",
      "    Parameters:\n",
      "        t : float\n",
      "            Time\n",
      "        y : array_like\n",
      "            Vector containing [theta, omega], where\n",
      "            theta is the angle and omega is the angular velocity\n",
      "        gamma : float\n",
      "            Damping coefficient\n",
      "        L : float\n",
      "            Length of the pendulum\n",
      "        \n",
      "    Returns:\n",
      "        dydt : array_like\n",
      "            Vector containing [omega, alpha], where\n",
      "            omega is the angular velocity and alpha is the angular acceleration\n",
      "    \"\"\"\n",
      "    theta, omega = y\n",
      "    alpha = - (9.81 / L) * np.sin(theta) - gamma * omega\n",
      "    return [omega, alpha]\n",
      "\n",
      "# Parameters\n",
      "theta0 = np.pi / 4  # Initial angle (radians)\n",
      "omega0 = 0.0        # Initial angular velocity (radians per second)\n",
      "gamma = 0.0       # Damping coefficient\n",
      "L = 1.0             # Length of the pendulum (meters)\n",
      "t_span = (0, 10)    # Time span for the simulation\n",
      "\n",
      "# Function to integrate the system of ODEs\n",
      "def integrate_pendulum(t_span, y0, gamma, L):\n",
      "\n",
      "    sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, y0, method='RK45', t_eval=np.linspace(*t_span, 1000))\n",
      "    return sol\n",
      "\n",
      "# Integrate the pendulum system\n",
      "sol = integrate_pendulum(t_span, [theta0, omega0], gamma, L)\n",
      "\n",
      "# Plot the results\n",
      "plt.figure(figsize=(10, 5))\n",
      "plt.plot(sol.t, sol.y[0], label='Angle (radians)')\n",
      "plt.plot(sol.t, sol.y[1], label='Angular velocity (rad/s)')\n",
      "plt.title('Damped Simple Pendulum Simulation using scipy.solve_ivp')\n",
      "plt.xlabel('Time (s)')\n",
      "plt.ylabel('Value')\n",
      "plt.legend()\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      " 8/3: IC_df = pd.read_csv(\"parameters/init_cond_simp_pend.csv\")\n",
      " 8/4:\n",
      "IC_df = IC_df[0:2]\n",
      "IC_df\n",
      " 8/5:\n",
      "# Mechanical eEnergy level\n",
      "0.5*(IC_df[\"omega\"])**2 + 9.81*(1-np.cos(IC_df[\"theta\"]))\n",
      " 8/6:\n",
      "params_df = pd.read_csv(\"parameters/pend_param.csv\")\n",
      "params_df\n",
      " 8/7: g = 9.81   # Acceleration due to gravity (m/s^2)\n",
      " 8/8:\n",
      "L = 5.0\n",
      "# y_shift = 0.9 * L\n",
      "# y_shift = 0\n",
      "\n",
      "num_time_points = 20\n",
      "# Time span\n",
      "t_span = (0.0, 10)  # from 0 to 10 seconds\n",
      "#Valuation points\n",
      "t_eval_ = np.linspace(t_span[0], t_span[1], num_time_points)\n",
      "data_matrix_df_list = []\n",
      "\n",
      "\n",
      "for param_index in params_df.index:\n",
      "    params = params_df.loc[param_index]\n",
      "    # Define parameters\n",
      "    m_c = params['m_c']  # Mass of the cart (kg)\n",
      "    m_p = params['m_p']  # Mass of the pendulum (kg)\n",
      "    l = params['l']    # Length of the pendulum (m)\n",
      "    for IC_index in IC_df.index:\n",
      "        IC = IC_df.loc[IC_index]\n",
      "        y0 = IC.values\n",
      "                # Parameters\n",
      "        theta0 = IC[\"theta\"]  # Initial angle (radians)\n",
      "        omega0 = IC[\"omega\"]        # Initial angular velocity (radians per second)\n",
      "        gamma = 0.0         # Damping coefficient\n",
      "        # Solve the ODEs\n",
      "        sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, [theta0, omega0], method='RK45', t_eval=t_eval_)\n",
      "        sol_df = pd.DataFrame(sol.y.T, columns=[\"theta\", \"omega\"])\n",
      "        sol_df[\"x\"] = L*np.sin(sol_df[\"theta\"])\n",
      "        sol_df[\"y\"] = -L*np.cos(sol_df[\"theta\"])\n",
      "        sol_df[\"t\"] = t_eval_\n",
      "        data_matrix_df_list.append(sol_df[[\"t\", \"x\", \"y\"]])\n",
      "        # if IC_index == 0:\n",
      "        #     # Plot the results\n",
      "        #     plt.figure(figsize=(10, 6))\n",
      "        #     plt.plot(sol.t, sol.y[0], label='Cart Position (x)')\n",
      "        #     plt.plot(sol.t, sol.y[2], label='Pendulum Angle (theta)')\n",
      "        #     plt.xlabel('Time (s)')\n",
      "        #     plt.ylabel('Position (m) / Angle (rad)')\n",
      "        #     plt.title('Upright Pendulum on Moving Cart')\n",
      "        #     plt.legend()\n",
      "        #     plt.grid(True)\n",
      "        #     plt.show()\n",
      "\n",
      "data_matrix_df = pd.concat(data_matrix_df_list, ignore_index=True)\n",
      "data_matrix_df\n",
      " 8/9:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 10\n",
      "\n",
      "\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"t\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"t\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "8/10: from scipy import interpolate\n",
      "8/11:\n",
      "#smoothing parameter: when equal weightage: num_data_points * std of data\n",
      "s_param = num_time_points * (0.01*noise_perc*data_matrix_df_list[1].std()[\"x\"])**2\n",
      "\n",
      "tck = interpolate.splrep(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], s=s_param)\n",
      "t_eval_new = np.linspace(t_span[0], t_span[1], 500)\n",
      "x_new = interpolate.splev(t_eval_new, tck, der=0)\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "        data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "plt.legend(['Linear', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation')\n",
      "plt.show()\n",
      "8/12: (3/4)*np.pi\n",
      "8/13: data_matrix_df[[\"x\",\"y\"]].plot()\n",
      "8/14:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "num_smoothed_points = num_time_points*10\n",
      "\n",
      "t_eval_new = np.linspace(data_matrix_df_list[0][\"t\"].iloc[0], data_matrix_df_list[0][\"t\"].iloc[-1], num_smoothed_points)\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,derr_order=1, noise_perc=noise_perc, eval_points=t_eval_new) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list[:-1], ignore_index=True)\n",
      "8/15:\n",
      "ind = 1\n",
      "feature_ = \"y\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"t\"], data_matrix_df_list[ind][feature_], \"x\", data_matrix_smooth_df_list[ind][\"t\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"t\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \"o\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "8/16: data_matrix_smooth_df_list[0][[\"d(x) /dt\"]].plot()\n",
      "8/17: data_matrix_smooth_df_list[0][[\"d(x) /dt\"]].plot()\n",
      "8/18:\n",
      "# data_matrix_df = data_matrix_df_appended[[\"x\",\"y\"]]\n",
      "# data_matrix_df = pd.concat([data_matrix_df, data_matrix_df_appended[[\"d(u) /dt\"]]], axis=1)\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[[\"x\",\"y\", \"d(x) /dt\", \"d(y) /dt\"]]\n",
      "data_matrix_df_smooth\n",
      "8/19:\n",
      "from copy import deepcopy\n",
      "new_df = deepcopy(data_matrix_df_smooth)\n",
      "\n",
      "new_df[\"energy\"] = 0.5*((new_df[\"d(x) /dt\"])**2 + (new_df[\"d(y) /dt\"])**2) +  9.81*new_df[\"y\"]\n",
      "\n",
      "new_df.plot()\n",
      "8/20:\n",
      "from sklearn.preprocessing import FunctionTransformer\n",
      "from copy import deepcopy\n",
      "\n",
      "def sin_transformer(period = 2*np.pi):\n",
      "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
      "\n",
      "\n",
      "def sin_diff_transformer(period = 2*np.pi):\n",
      "    return FunctionTransformer(lambda x,y: np.sin((x-y) / period * 2 * np.pi))\n",
      "\n",
      "def cos_transformer(period = 2*np.pi):\n",
      "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
      "\n",
      "\n",
      "data_matrix_df_with_trig = deepcopy(data_matrix_df)\n",
      "# data_matrix_df_with_trig[\"sin(theta)\"] = sin_transformer(1).fit_transform(data_matrix_df_with_trig)[\"theta\"]\n",
      "# data_matrix_df_with_trig[\"cos(theta)\"] = cos_transformer(1).fit_transform(data_matrix_df_with_trig)[\"theta\"]\n",
      "8/21:\n",
      "from scipy.sparse import coo_array\n",
      "\n",
      "row  = np.array([0, 3, 1, 0, 0, 0])\n",
      "col  = np.array([0, 3, 1, 1, 2, 4])\n",
      "data = np.array([4, 5, 7, 9, 100, 101])\n",
      "# sp_array = coo_array((data, (row, col)), shape=(4, 4))\n",
      "sp_array = coo_array((data, (row, col)))\n",
      "8/22: list(zip(sp_array.row, sp_array.col))\n",
      "8/23:\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\n",
      "from scipy.sparse import coo_array\n",
      "from sklearn.utils.validation import (\n",
      "    FLOAT_DTYPES,\n",
      "    _check_feature_names_in,\n",
      "    _check_sample_weight,\n",
      "    check_is_fitted,\n",
      ")\n",
      "\n",
      "class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "    def __init__(self, sparsity_matrix, power=1, comp_function=None):\n",
      "\n",
      "        assert isinstance(sparsity_matrix, coo_array), \"FeatureDiffTransformer only support sparsity matrix\\\n",
      "        in the scipy.sparse.coo_array format\"\n",
      "        self.sparsity_matrix = sparsity_matrix\n",
      "        if not comp_function:\n",
      "            self.comp_function = comp_function\n",
      "        else:\n",
      "            self.comp_function = lambda x: x\n",
      "        assert power>=1, \"difference power needs to be atleast 1\"\n",
      "        self.power = power\n",
      "\n",
      "        self.n_features_in_ = 0\n",
      "        self.feature_names_in_ = None\n",
      "\n",
      "\n",
      "    def get_feature_names_out(self, input_features=None):\n",
      "        \"\"\"Get output feature names for transformation.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        input_features : array-like of str or None, default=None\n",
      "            Input features.\n",
      "\n",
      "            - If `input_features is None`, then `feature_names_in_` is\n",
      "              used as feature names in. If `feature_names_in_` is not defined,\n",
      "              then the following input feature names are generated:\n",
      "              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "            - If `input_features` is an array-like, then `input_features` must\n",
      "              match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        feature_names_out : ndarray of str objects\n",
      "            Transformed feature names.\n",
      "        \"\"\"\n",
      "        power = self.power\n",
      "        input_features = _check_feature_names_in(self, input_features)\n",
      "        feature_names = [feature for feature in input_features]\n",
      "        # for row in powers:\n",
      "        #     inds = np.where(row)[0]\n",
      "        #     if len(inds):\n",
      "        #         name = \" \".join(\n",
      "        #             (\n",
      "        #                 \"%s^%d\" % (input_features[ind], exp)\n",
      "        #                 if exp != 1\n",
      "        #                 else input_features[ind]\n",
      "        #             )\n",
      "        #             for ind, exp in zip(inds, row[inds])\n",
      "        #         )\n",
      "        #     else:\n",
      "        #         name = \"1\"\n",
      "        #     feature_names.append(name)\n",
      "        return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "    def fit(self, X, y=None):\n",
      "        self.n_features_in_ = X.shape[1]\n",
      "        assert max(self.sparsity_matrix.col.max(), self.sparsity_matrix.row.max()) <=self.n_features_in_-1, \\\n",
      "        \"sparsity matrix has indices out of bound of the number of features\"\n",
      "        if len(X.columns) >0:\n",
      "            self.feature_names_in_ = X.columns\n",
      "        return self\n",
      "\n",
      "    def transform(self, X):\n",
      "        \"\"\"Transform data to specified difference feature\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The data to transform, row by row.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "            The matrix of features, where `NS` is the number of non-zero\n",
      "            connections implied from the sparsity matrix.\n",
      "        \"\"\"\n",
      "        check_is_fitted(self)\n",
      "\n",
      "        X = self._validate_data(\n",
      "            X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "        )\n",
      "        # self.n_features_in_ = X.shape[1]\n",
      "        # if len(X.columns) >0:\n",
      "        #     self.feature_names_in_ = X.columns\n",
      "        # n_samples, n_features = X.shape\n",
      "        # max_int32 = np.iinfo(np.int32).max\n",
      "        # if sparse.issparse(X) and X.format == \"csr\":\n",
      "        #     if self._max_degree > 3:\n",
      "        #         return self.transform(X.tocsc()).tocsr()\n",
      "        #     to_stack = []\n",
      "        #     if self.include_bias:\n",
      "        #         to_stack.append(\n",
      "        #             sparse.csr_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
      "        #         )\n",
      "        #     if self._min_degree <= 1 and self._max_degree > 0:\n",
      "        #         to_stack.append(X)\n",
      "\n",
      "        #     cumulative_size = sum(mat.shape[1] for mat in to_stack)\n",
      "        #     for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
      "        #         expanded = _create_expansion(\n",
      "        #             X=X,\n",
      "        #             interaction_only=self.interaction_only,\n",
      "        #             deg=deg,\n",
      "        #             n_features=n_features,\n",
      "        #             cumulative_size=cumulative_size,\n",
      "        #         )\n",
      "        #         if expanded is not None:\n",
      "        #             to_stack.append(expanded)\n",
      "        #             cumulative_size += expanded.shape[1]\n",
      "        #     if len(to_stack) == 0:\n",
      "        #         # edge case: deal with empty matrix\n",
      "        #         XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
      "        #     else:\n",
      "        #         # `scipy.sparse.hstack` breaks in scipy<1.9.2\n",
      "        #         # when `n_output_features_ > max_int32`\n",
      "        #         all_int32 = all(mat.indices.dtype == np.int32 for mat in to_stack)\n",
      "        #         if (\n",
      "        #             sp_version < parse_version(\"1.9.2\")\n",
      "        #             and self.n_output_features_ > max_int32\n",
      "        #             and all_int32\n",
      "        #         ):\n",
      "        #             raise ValueError(  # pragma: no cover\n",
      "        #                 \"In scipy versions `<1.9.2`, the function `scipy.sparse.hstack`\"\n",
      "        #                 \" produces negative columns when:\\n1. The output shape contains\"\n",
      "        #                 \" `n_cols` too large to be represented by a 32bit signed\"\n",
      "        #                 \" integer.\\n2. All sub-matrices to be stacked have indices of\"\n",
      "        #                 \" dtype `np.int32`.\\nTo avoid this error, either use a version\"\n",
      "        #                 \" of scipy `>=1.9.2` or alter the `PolynomialFeatures`\"\n",
      "        #                 \" transformer to produce fewer than 2^31 output features\"\n",
      "        #             )\n",
      "        #         XP = sparse.hstack(to_stack, dtype=X.dtype, format=\"csr\")\n",
      "        # elif sparse.issparse(X) and X.format == \"csc\" and self._max_degree < 4:\n",
      "        #     return self.transform(X.tocsr()).tocsc()\n",
      "        # elif sparse.issparse(X):\n",
      "        #     combinations = self._combinations(\n",
      "        #         n_features=n_features,\n",
      "        #         min_degree=self._min_degree,\n",
      "        #         max_degree=self._max_degree,\n",
      "        #         interaction_only=self.interaction_only,\n",
      "        #         include_bias=self.include_bias,\n",
      "        #     )\n",
      "        #     columns = []\n",
      "        #     for combi in combinations:\n",
      "        #         if combi:\n",
      "        #             out_col = 1\n",
      "        #             for col_idx in combi:\n",
      "        #                 out_col = X[:, [col_idx]].multiply(out_col)\n",
      "        #             columns.append(out_col)\n",
      "        #         else:\n",
      "        #             bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
      "        #             columns.append(bias)\n",
      "        #     XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
      "        # else:\n",
      "        #     # Do as if _min_degree = 0 and cut down array after the\n",
      "        #     # computation, i.e. use _n_out_full instead of n_output_features_.\n",
      "        #     XP = np.empty(\n",
      "        #         shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
      "        #     )\n",
      "\n",
      "        #     # What follows is a faster implementation of:\n",
      "        #     # for i, comb in enumerate(combinations):\n",
      "        #     #     XP[:, i] = X[:, comb].prod(1)\n",
      "        #     # This implementation uses two optimisations.\n",
      "        #     # First one is broadcasting,\n",
      "        #     # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
      "        #     # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
      "        #     # ...\n",
      "        #     # multiply ([X[:, start:end], X[:, start]) -> ...\n",
      "        #     # Second optimisation happens for degrees >= 3.\n",
      "        #     # Xi^3 is computed reusing previous computation:\n",
      "        #     # Xi^3 = Xi^2 * Xi.\n",
      "\n",
      "        #     # degree 0 term\n",
      "        #     if self.include_bias:\n",
      "        #         XP[:, 0] = 1\n",
      "        #         current_col = 1\n",
      "        #     else:\n",
      "        #         current_col = 0\n",
      "\n",
      "        #     if self._max_degree == 0:\n",
      "        #         return XP\n",
      "\n",
      "        #     # degree 1 term\n",
      "        #     XP[:, current_col : current_col + n_features] = X\n",
      "        #     index = list(range(current_col, current_col + n_features))\n",
      "        #     current_col += n_features\n",
      "        #     index.append(current_col)\n",
      "\n",
      "        #     # loop over degree >= 2 terms\n",
      "        #     for _ in range(2, self._max_degree + 1):\n",
      "        #         new_index = []\n",
      "        #         end = index[-1]\n",
      "        #         for feature_idx in range(n_features):\n",
      "        #             start = index[feature_idx]\n",
      "        #             new_index.append(current_col)\n",
      "        #             if self.interaction_only:\n",
      "        #                 start += index[feature_idx + 1] - index[feature_idx]\n",
      "        #             next_col = current_col + end - start\n",
      "        #             if next_col <= current_col:\n",
      "        #                 break\n",
      "        #             # XP[:, start:end] are terms of degree d - 1\n",
      "        #             # that exclude feature #feature_idx.\n",
      "        #             np.multiply(\n",
      "        #                 XP[:, start:end],\n",
      "        #                 X[:, feature_idx : feature_idx + 1],\n",
      "        #                 out=XP[:, current_col:next_col],\n",
      "        #                 casting=\"no\",\n",
      "        #             )\n",
      "        #             current_col = next_col\n",
      "\n",
      "        #         new_index.append(current_col)\n",
      "        #         index = new_index\n",
      "\n",
      "        #     if self._min_degree > 1:\n",
      "        #         n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
      "        #         if self.include_bias:\n",
      "        #             Xout = np.empty(\n",
      "        #                 shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
      "        #             )\n",
      "        #             Xout[:, 0] = 1\n",
      "        #             Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
      "        #         else:\n",
      "        #             Xout = XP[:, n_XP - n_Xout :].copy()\n",
      "        #         XP = Xout\n",
      "        return X\n",
      "8/24:\n",
      "from scipy.sparse import coo_array\n",
      "\n",
      "row  = np.array([0, 0, 1])\n",
      "col  = np.array([0, 2, 2])\n",
      "data = np.array([4, 5, 7])\n",
      "# sp_array = coo_array((data, (row, col)), shape=(4, 4))\n",
      "sp_array_2 = coo_array((data, (row, col)))\n",
      "\n",
      "\n",
      "dummy_tr = FeatureDiffTransformer(sp_array_2, power=1)\n",
      "8/25:\n",
      "# dummy_tr._check_n_features(data_matrix_, 0)\n",
      "# dummy_tr._check_feature_names(data_matrix_, reset = 0)\n",
      "8/26:\n",
      "# dummy_tr.fit(data_matrix_)\n",
      "dummy_tr.fit_transform(data_matrix_)\n",
      "8/27: dummy_tr.get_feature_names_out()\n",
      "8/28:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "8/29:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "\n",
      "# Function to compute derivatives\n",
      "def pendulum_rhs(t, y, gamma, L=1):\n",
      "    \"\"\"\n",
      "    Function to compute derivatives for simple pendulum with damping\n",
      "    \n",
      "    Parameters:\n",
      "        t : float\n",
      "            Time\n",
      "        y : array_like\n",
      "            Vector containing [theta, omega], where\n",
      "            theta is the angle and omega is the angular velocity\n",
      "        gamma : float\n",
      "            Damping coefficient\n",
      "        L : float\n",
      "            Length of the pendulum\n",
      "        \n",
      "    Returns:\n",
      "        dydt : array_like\n",
      "            Vector containing [omega, alpha], where\n",
      "            omega is the angular velocity and alpha is the angular acceleration\n",
      "    \"\"\"\n",
      "    theta, omega = y\n",
      "    alpha = - (9.81 / L) * np.sin(theta) - gamma * omega\n",
      "    return [omega, alpha]\n",
      "\n",
      "# Parameters\n",
      "theta0 = np.pi / 4  # Initial angle (radians)\n",
      "omega0 = 0.0        # Initial angular velocity (radians per second)\n",
      "gamma = 0.0       # Damping coefficient\n",
      "L = 1.0             # Length of the pendulum (meters)\n",
      "t_span = (0, 10)    # Time span for the simulation\n",
      "\n",
      "# Function to integrate the system of ODEs\n",
      "def integrate_pendulum(t_span, y0, gamma, L):\n",
      "\n",
      "    sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, y0, method='RK45', t_eval=np.linspace(*t_span, 1000))\n",
      "    return sol\n",
      "\n",
      "# Integrate the pendulum system\n",
      "sol = integrate_pendulum(t_span, [theta0, omega0], gamma, L)\n",
      "\n",
      "# Plot the results\n",
      "plt.figure(figsize=(10, 5))\n",
      "plt.plot(sol.t, sol.y[0], label='Angle (radians)')\n",
      "plt.plot(sol.t, sol.y[1], label='Angular velocity (rad/s)')\n",
      "plt.title('Damped Simple Pendulum Simulation using scipy.solve_ivp')\n",
      "plt.xlabel('Time (s)')\n",
      "plt.ylabel('Value')\n",
      "plt.legend()\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "8/30: IC_df = pd.read_csv(\"parameters/init_cond_simp_pend.csv\")\n",
      "11/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "11/2:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "\n",
      "# Function to compute derivatives\n",
      "def pendulum_rhs(t, y, gamma, L=1):\n",
      "    \"\"\"\n",
      "    Function to compute derivatives for simple pendulum with damping\n",
      "    \n",
      "    Parameters:\n",
      "        t : float\n",
      "            Time\n",
      "        y : array_like\n",
      "            Vector containing [theta, omega], where\n",
      "            theta is the angle and omega is the angular velocity\n",
      "        gamma : float\n",
      "            Damping coefficient\n",
      "        L : float\n",
      "            Length of the pendulum\n",
      "        \n",
      "    Returns:\n",
      "        dydt : array_like\n",
      "            Vector containing [omega, alpha], where\n",
      "            omega is the angular velocity and alpha is the angular acceleration\n",
      "    \"\"\"\n",
      "    theta, omega = y\n",
      "    alpha = - (9.81 / L) * np.sin(theta) - gamma * omega\n",
      "    return [omega, alpha]\n",
      "\n",
      "# Parameters\n",
      "theta0 = np.pi / 4  # Initial angle (radians)\n",
      "omega0 = 0.0        # Initial angular velocity (radians per second)\n",
      "gamma = 0.0       # Damping coefficient\n",
      "L = 1.0             # Length of the pendulum (meters)\n",
      "t_span = (0, 10)    # Time span for the simulation\n",
      "\n",
      "# Function to integrate the system of ODEs\n",
      "def integrate_pendulum(t_span, y0, gamma, L):\n",
      "\n",
      "    sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, y0, method='RK45', t_eval=np.linspace(*t_span, 1000))\n",
      "    return sol\n",
      "\n",
      "# Integrate the pendulum system\n",
      "sol = integrate_pendulum(t_span, [theta0, omega0], gamma, L)\n",
      "\n",
      "# Plot the results\n",
      "plt.figure(figsize=(10, 5))\n",
      "plt.plot(sol.t, sol.y[0], label='Angle (radians)')\n",
      "plt.plot(sol.t, sol.y[1], label='Angular velocity (rad/s)')\n",
      "plt.title('Damped Simple Pendulum Simulation using scipy.solve_ivp')\n",
      "plt.xlabel('Time (s)')\n",
      "plt.ylabel('Value')\n",
      "plt.legend()\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "11/3: IC_df = pd.read_csv(\"parameters/init_cond_simp_pend.csv\")\n",
      "11/4:\n",
      "IC_df = IC_df[0:2]\n",
      "IC_df\n",
      "11/5:\n",
      "# Mechanical eEnergy level\n",
      "0.5*(IC_df[\"omega\"])**2 + 9.81*(1-np.cos(IC_df[\"theta\"]))\n",
      "11/6:\n",
      "params_df = pd.read_csv(\"parameters/pend_param.csv\")\n",
      "params_df\n",
      "11/7: g = 9.81   # Acceleration due to gravity (m/s^2)\n",
      "11/8:\n",
      "L = 5.0\n",
      "# y_shift = 0.9 * L\n",
      "# y_shift = 0\n",
      "\n",
      "num_time_points = 20\n",
      "# Time span\n",
      "t_span = (0.0, 10)  # from 0 to 10 seconds\n",
      "#Valuation points\n",
      "t_eval_ = np.linspace(t_span[0], t_span[1], num_time_points)\n",
      "data_matrix_df_list = []\n",
      "\n",
      "\n",
      "for param_index in params_df.index:\n",
      "    params = params_df.loc[param_index]\n",
      "    # Define parameters\n",
      "    m_c = params['m_c']  # Mass of the cart (kg)\n",
      "    m_p = params['m_p']  # Mass of the pendulum (kg)\n",
      "    l = params['l']    # Length of the pendulum (m)\n",
      "    for IC_index in IC_df.index:\n",
      "        IC = IC_df.loc[IC_index]\n",
      "        y0 = IC.values\n",
      "                # Parameters\n",
      "        theta0 = IC[\"theta\"]  # Initial angle (radians)\n",
      "        omega0 = IC[\"omega\"]        # Initial angular velocity (radians per second)\n",
      "        gamma = 0.0         # Damping coefficient\n",
      "        # Solve the ODEs\n",
      "        sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, [theta0, omega0], method='RK45', t_eval=t_eval_)\n",
      "        sol_df = pd.DataFrame(sol.y.T, columns=[\"theta\", \"omega\"])\n",
      "        sol_df[\"x\"] = L*np.sin(sol_df[\"theta\"])\n",
      "        sol_df[\"y\"] = -L*np.cos(sol_df[\"theta\"])\n",
      "        sol_df[\"t\"] = t_eval_\n",
      "        data_matrix_df_list.append(sol_df[[\"t\", \"x\", \"y\"]])\n",
      "        # if IC_index == 0:\n",
      "        #     # Plot the results\n",
      "        #     plt.figure(figsize=(10, 6))\n",
      "        #     plt.plot(sol.t, sol.y[0], label='Cart Position (x)')\n",
      "        #     plt.plot(sol.t, sol.y[2], label='Pendulum Angle (theta)')\n",
      "        #     plt.xlabel('Time (s)')\n",
      "        #     plt.ylabel('Position (m) / Angle (rad)')\n",
      "        #     plt.title('Upright Pendulum on Moving Cart')\n",
      "        #     plt.legend()\n",
      "        #     plt.grid(True)\n",
      "        #     plt.show()\n",
      "\n",
      "data_matrix_df = pd.concat(data_matrix_df_list, ignore_index=True)\n",
      "data_matrix_df\n",
      "11/9:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 10\n",
      "\n",
      "\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"t\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"t\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "11/10: from scipy import interpolate\n",
      "11/11:\n",
      "#smoothing parameter: when equal weightage: num_data_points * std of data\n",
      "s_param = num_time_points * (0.01*noise_perc*data_matrix_df_list[1].std()[\"x\"])**2\n",
      "\n",
      "tck = interpolate.splrep(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], s=s_param)\n",
      "t_eval_new = np.linspace(t_span[0], t_span[1], 500)\n",
      "x_new = interpolate.splev(t_eval_new, tck, der=0)\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "        data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "plt.legend(['Linear', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation')\n",
      "plt.show()\n",
      "11/12: (3/4)*np.pi\n",
      "11/13: data_matrix_df[[\"x\",\"y\"]].plot()\n",
      "11/14:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "num_smoothed_points = num_time_points*10\n",
      "\n",
      "t_eval_new = np.linspace(data_matrix_df_list[0][\"t\"].iloc[0], data_matrix_df_list[0][\"t\"].iloc[-1], num_smoothed_points)\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,derr_order=1, noise_perc=noise_perc, eval_points=t_eval_new) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list[:-1], ignore_index=True)\n",
      "11/15:\n",
      "ind = 1\n",
      "feature_ = \"y\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"t\"], data_matrix_df_list[ind][feature_], \"x\", data_matrix_smooth_df_list[ind][\"t\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"t\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \"o\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "11/16: data_matrix_smooth_df_list[0][[\"d(x) /dt\"]].plot()\n",
      "11/17: data_matrix_smooth_df_list[0][[\"d(x) /dt\"]].plot()\n",
      "11/18:\n",
      "# data_matrix_df = data_matrix_df_appended[[\"x\",\"y\"]]\n",
      "# data_matrix_df = pd.concat([data_matrix_df, data_matrix_df_appended[[\"d(u) /dt\"]]], axis=1)\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[[\"x\",\"y\", \"d(x) /dt\", \"d(y) /dt\"]]\n",
      "data_matrix_df_smooth\n",
      "11/19:\n",
      "from copy import deepcopy\n",
      "new_df = deepcopy(data_matrix_df_smooth)\n",
      "\n",
      "new_df[\"energy\"] = 0.5*((new_df[\"d(x) /dt\"])**2 + (new_df[\"d(y) /dt\"])**2) +  9.81*new_df[\"y\"]\n",
      "\n",
      "new_df.plot()\n",
      "11/20:\n",
      "from sklearn.preprocessing import FunctionTransformer\n",
      "from copy import deepcopy\n",
      "\n",
      "def sin_transformer(period = 2*np.pi):\n",
      "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
      "\n",
      "\n",
      "def sin_diff_transformer(period = 2*np.pi):\n",
      "    return FunctionTransformer(lambda x,y: np.sin((x-y) / period * 2 * np.pi))\n",
      "\n",
      "def cos_transformer(period = 2*np.pi):\n",
      "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
      "\n",
      "\n",
      "def sin_shift_transformer(period = 2*np.pi):\n",
      "    return FunctionTransformer(lambda x,y: np.sin((x-y) / period * 2 * np.pi))\n",
      "\n",
      "data_matrix_df_with_trig = deepcopy(data_matrix_df)\n",
      "# data_matrix_df_with_trig[\"sin(theta)\"] = sin_transformer(1).fit_transform(data_matrix_df_with_trig)[\"theta\"]\n",
      "# data_matrix_df_with_trig[\"cos(theta)\"] = cos_transformer(1).fit_transform(data_matrix_df_with_trig)[\"theta\"]\n",
      "11/21:\n",
      "from scipy.sparse import coo_array\n",
      "\n",
      "row  = np.array([0, 3, 1, 0, 0, 0])\n",
      "col  = np.array([0, 3, 1, 1, 2, 4])\n",
      "data = np.array([4, 5, 7, 9, 100, 101])\n",
      "# sp_array = coo_array((data, (row, col)), shape=(4, 4))\n",
      "sp_array = coo_array((data, (row, col)))\n",
      "11/22: list(zip(sp_array.row, sp_array.col))\n",
      "11/23:\n",
      "# from sklearn.base import BaseEstimator, TransformerMixin\n",
      "# from scipy.sparse import coo_array\n",
      "# from sklearn.utils.validation import (\n",
      "#     FLOAT_DTYPES,\n",
      "#     _check_feature_names_in,\n",
      "#     _check_sample_weight,\n",
      "#     check_is_fitted,\n",
      "# )\n",
      "\n",
      "# class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "#     def __init__(self, sparsity_matrix = None, coupled_indices_list = None, power=1, comp_function=None):\n",
      "#         #If coupled indices list is not explicitly given to the constructor, a valid sparsity matrix\n",
      "#         # from which the coupled indices can be implied should be provided. \n",
      "#         if not coupled_indices_list:\n",
      "#             assert isinstance(sparsity_matrix, coo_array), \"FeatureDiffTransformer only support sparsity matrix\\\n",
      "#             in the scipy.sparse.coo_array format\"\n",
      "#             self.sparsity_matrix = sparsity_matrix\n",
      "#         self.coupled_indices_list = coupled_indices_list\n",
      "#         if not comp_function:\n",
      "#             self.comp_function = comp_function\n",
      "#         else:\n",
      "#             self.comp_function = lambda x: x\n",
      "#         assert power>=1, \"difference power needs to be atleast 1\"\n",
      "#         self.power = power\n",
      "\n",
      "#         self.n_features_in_ = 0\n",
      "#         self.feature_names_in_ = None\n",
      "\n",
      "\n",
      "#     def get_feature_names_out(self, input_features=None):\n",
      "#         \"\"\"Get output feature names for transformation.\n",
      "\n",
      "#         Parameters\n",
      "#         ----------\n",
      "#         input_features : array-like of str or None, default=None\n",
      "#             Input features.\n",
      "\n",
      "#             - If `input_features is None`, then `feature_names_in_` is\n",
      "#               used as feature names in. If `feature_names_in_` is not defined,\n",
      "#               then the following input feature names are generated:\n",
      "#               `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "#             - If `input_features` is an array-like, then `input_features` must\n",
      "#               match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "#             It is recommended that the coupling between features are given using a sparsity matrix\n",
      "#             instead of coupling indices. \n",
      "\n",
      "#         Returns\n",
      "#         -------\n",
      "#         feature_names_out : ndarray of str objects\n",
      "#             Transformed feature names.\n",
      "#         \"\"\"\n",
      "#         check_is_fitted(self)\n",
      "#         input_features = _check_feature_names_in(self, input_features)\n",
      "#         feature_names = [\"({}-{})\".format(input_features[i], input_features[j]) for i,j in self.coupled_indices_list]\n",
      "\n",
      "#         return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "#     def fit(self, X, y=None):\n",
      "#         \"\"\"\n",
      "        \n",
      "#         \"\"\"\n",
      "#         self.n_features_in_ = X.shape[1]\n",
      "#         if len(X.columns) >0:\n",
      "#             self.feature_names_in_ = X.columns\n",
      "#         if not self.coupled_indices_list: #sparsity matrix gives the coupling indices\n",
      "#             assert max(self.sparsity_matrix.col.max(), self.sparsity_matrix.row.max()) <=self.n_features_in_-1, \\\n",
      "#             \"sparsity matrix has indices out of bound of the number of features\"\n",
      "#             #Extracting the indices that has coupling with each other. \n",
      "#             self.coupled_indices_list = list(zip(self.sparsity_matrix.row, self.sparsity_matrix.col))\n",
      "        \n",
      "#         return self\n",
      "\n",
      "\n",
      "#     def transform(self, X):\n",
      "#         \"\"\"Transform data to specified difference feature\n",
      "\n",
      "#         Parameters\n",
      "#         ----------\n",
      "#         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "#             The data to transform, row by row.\n",
      "\n",
      "#         Returns\n",
      "#         -------\n",
      "#         XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "#             The matrix of features, where `NS` is the number of non-zero\n",
      "#             connections implied from the sparsity matrix.\n",
      "#         \"\"\"\n",
      "#         check_is_fitted(self)\n",
      "\n",
      "#         X = self._validate_data(\n",
      "#             X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "#         )\n",
      "#         X_transpose = X.T\n",
      "#         if self.comp_function == None:\n",
      "#             X_diff = np.vstack([X_transpose[i] - X_transpose[j] for i,j in self.coupled_indices_list]).T\n",
      "#         else:\n",
      "            \n",
      "\n",
      "#         return X_diff\n",
      "11/24:\n",
      "# from sklearn.base import BaseEstimator, TransformerMixin\n",
      "# from scipy.sparse import coo_array\n",
      "# from sklearn.utils.validation import (\n",
      "#     FLOAT_DTYPES,\n",
      "#     _check_feature_names_in,\n",
      "#     _check_sample_weight,\n",
      "#     check_is_fitted,\n",
      "# )\n",
      "\n",
      "# class FeatureCouplingTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "#     def __init__(self, sparsity_matrix = None, coupled_indices_list = None,\n",
      "#                  power=1, coupling_func=None, coupling_namer=None,\n",
      "#                 coupling_func_args = {}, return_df = False):\n",
      "#         \"\"\"\n",
      "#         Note that the coupling_func and coupling_namer function should have arguments\n",
      "#         (feature_1_value,feature_2_value, i, j) as the first four arguments. \n",
      "        \n",
      "#         coupling_func_args (dict): optional keyword arguments for the coupling_function and coupling_namer functions\n",
      "#         \"\"\"\n",
      "#         #If coupled indices list is not explicitly given to the constructor, a valid sparsity matrix\n",
      "#         # from which the coupled indices can be implied should be provided. \n",
      "#         if not coupled_indices_list:\n",
      "#             assert isinstance(sparsity_matrix, coo_array), \"FeatureDiffTransformer only support sparsity matrix\\\n",
      "#             in the scipy.sparse.coo_array format\"\n",
      "#             self.sparsity_matrix = sparsity_matrix\n",
      "#         self.coupled_indices_list = coupled_indices_list\n",
      "#         if not coupling_func:\n",
      "#             self.coupling_func = lambda x, y, i, j: x*y\n",
      "#         else:\n",
      "#             #If coupling function is not given, it is defined as the interaction term feature_1*feature_2\n",
      "#             self.coupling_func = coupling_func\n",
      "\n",
      "#         if not coupling_namer:\n",
      "#             self.coupling_namer = lambda feature_1, feature_2, i, j: \"{}*{}\".format(feature_1, feature_2)\n",
      "#         else:\n",
      "#             self.coupling_namer = coupling_namer\n",
      "\n",
      "#         self.coupling_func_args = coupling_func_args\n",
      "#         self.return_df = return_df\n",
      "            \n",
      "#         assert power>=1, \"difference power needs to be atleast 1\"\n",
      "#         self.power = power\n",
      "\n",
      "#         self.n_features_in_ = 0\n",
      "#         self.feature_names_in_ = None\n",
      "\n",
      "\n",
      "#     def get_feature_names_out(self, input_features=None):\n",
      "#         \"\"\"Get output feature names for transformation.\n",
      "\n",
      "#         Parameters\n",
      "#         ----------\n",
      "#         input_features : array-like of str or None, default=None\n",
      "#             Input features.\n",
      "\n",
      "#             - If `input_features is None`, then `feature_names_in_` is\n",
      "#               used as feature names in. If `feature_names_in_` is not defined,\n",
      "#               then the following input feature names are generated:\n",
      "#               `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "#             - If `input_features` is an array-like, then `input_features` must\n",
      "#               match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "#             It is recommended that the coupling between features are given using a sparsity matrix\n",
      "#             instead of coupling indices. \n",
      "\n",
      "#         Returns\n",
      "#         -------\n",
      "#         feature_names_out : ndarray of str objects\n",
      "#             Transformed feature names.\n",
      "#         \"\"\"\n",
      "#         check_is_fitted(self)\n",
      "#         input_features = _check_feature_names_in(self, input_features)\n",
      "        \n",
      "#         feature_names = [self.coupling_namer(input_features[i], input_features[j], i, j, **self.coupling_func_args) for i,j in self.coupled_indices_list]\n",
      "\n",
      "#         return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "#     def fit(self, X, y=None):\n",
      "#         \"\"\"\n",
      "        \n",
      "#         \"\"\"\n",
      "#         self.n_features_in_ = X.shape[1]\n",
      "#         if len(X.columns) >0:\n",
      "#             self.feature_names_in_ = X.columns\n",
      "#         if not self.coupled_indices_list: #sparsity matrix gives the coupling indices\n",
      "#             assert max(self.sparsity_matrix.col.max(), self.sparsity_matrix.row.max()) <=self.n_features_in_-1, \\\n",
      "#             \"sparsity matrix has indices out of bound of the number of features\"\n",
      "#             #Extracting the indices that has coupling with each other. \n",
      "#             self.coupled_indices_list = list(zip(self.sparsity_matrix.row, self.sparsity_matrix.col))\n",
      "        \n",
      "#         return self\n",
      "\n",
      "\n",
      "#     def transform(self, X):\n",
      "#         \"\"\"Transform data to specified difference feature\n",
      "\n",
      "#         Parameters\n",
      "#         ----------\n",
      "#         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "#             The data to transform, row by row.\n",
      "\n",
      "#         Returns\n",
      "#         -------\n",
      "#         XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "#             The matrix of features, where `NS` is the number of non-zero\n",
      "#             connections implied from the sparsity matrix.\n",
      "#         \"\"\"\n",
      "#         check_is_fitted(self)\n",
      "\n",
      "#         X = self._validate_data(\n",
      "#             X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "#         )\n",
      "#         X_transpose = X.T\n",
      "#         X_coupled = np.vstack([self.coupling_func(X_transpose[i], X_transpose[j], i, j, **self.coupling_func_args) \n",
      "#                                for i,j in self.coupled_indices_list]).T\n",
      "#         if self.return_df:\n",
      "#             return pd.DataFrame(X_coupled, columns= self.get_feature_names_out())\n",
      "            \n",
      "\n",
      "#         return X_coupled\n",
      "11/25:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "from scipy.sparse import coo_array\n",
      "\n",
      "row  = np.array([0, 0, 1, 1])\n",
      "col  = np.array([0, 2, 2, 1])\n",
      "data = np.array([4, 5, 7, 5])\n",
      "sp_array_2 = coo_array((data, (row, col)))\n",
      "\n",
      "\n",
      "# dummy_tr_interact = FeatureDiffTransformer(sp_array_2, power=1)\n",
      "dummy_tr = FeatureCouplingTransformer(sp_array_2, return_df=True)\n",
      "dummy_tr_diff = FeatureCouplingTransformer(sp_array_2,\n",
      "                                           coupling_func= lambda x,y, i, j: x-y,\n",
      "                                           coupling_namer= lambda x,y,i,j : \"{}-{}\".format(x,y))\n",
      "\n",
      "def coup_fun(x,y,i,j,k=0):\n",
      "    return x-y-k\n",
      "dummy_tr_diff_minus_k = FeatureCouplingTransformer(sp_array_2,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= lambda x,y,i,j,k : \"{}-{}-{}\".format(x,y,k),\n",
      "                                                  coupling_func_args={\"k\":2})\n",
      "\n",
      "dummy_tr_diff \n",
      "# dummy_tr = FeatureDiffTransformer(coupled_indices_list=[(1, 2), (1, 1), (100, 101)], power=1)\n",
      "11/26: phi_matrix = np.array([[1,2,3], [4,5,6], [7,8,9]]\n",
      "11/27: phi_matrix = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
      "11/28:\n",
      "def coup_fun(x,y,i,j,ph_matrix):\n",
      "    return x-y- ph_matrix[i,j]\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sp_array_2,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= lambda x,y,i,j,k : \"{}-{}-phi_{},{}\".format(x,y,i,j),\n",
      "                                                  coupling_func_args={\"k\":2})\n",
      "11/29:\n",
      "new_data = dummy_tr_sin_diff.fit_transform(data_matrix_)\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "cop_ind\n",
      "11/30:\n",
      "phi_matrix = np.array([1,2,3],[4,5,6],[7,8,9])\n",
      "\n",
      "def coup_fun(x,y,i,j,ph_matrix):\n",
      "    return x-y- ph_matrix[i,j]\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sp_array_2,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= lambda x,y,i,j,k : \"{}-{}-phi_{},{}\".format(x,y,i,j),\n",
      "                                                  coupling_func_args={\"ph_matrix\":phi_matrix})\n",
      "11/31:\n",
      "phi_matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
      "\n",
      "def coup_fun(x,y,i,j,ph_matrix):\n",
      "    return x-y- ph_matrix[i,j]\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sp_array_2,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= lambda x,y,i,j,k : \"{}-{}-phi_{},{}\".format(x,y,i,j),\n",
      "                                                  coupling_func_args={\"ph_matrix\":phi_matrix})\n",
      "11/32:\n",
      "new_data = dummy_tr_sin_diff.fit_transform(data_matrix_)\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "cop_ind\n",
      "11/33: dummy_tr_sin_diff.get_feature_names_out()\n",
      "11/34:\n",
      "phi_matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
      "\n",
      "def coup_fun(x,y,i,j,ph_matrix):\n",
      "    return x-y- ph_matrix[i,j]\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sp_array_2,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= lambda x,y,i,j,phi_matrix : \"{}-{}-phi_{},{}\".format(x,y,i,j),\n",
      "                                                  coupling_func_args={\"ph_matrix\":phi_matrix})\n",
      "11/35:\n",
      "new_data = dummy_tr_sin_diff.fit_transform(data_matrix_)\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "cop_ind\n",
      "11/36: dummy_tr_sin_diff.get_feature_names_out()\n",
      "11/37:\n",
      "phi_matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
      "\n",
      "def coup_fun(x,y,i,j,ph_matrix):\n",
      "    return x-y- ph_matrix[i,j]\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sp_array_2,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= lambda x,y,i,j,ph_matrix : \"{}-{}-phi_{},{}\".format(x,y,i,j),\n",
      "                                                  coupling_func_args={\"ph_matrix\":phi_matrix})\n",
      "11/38:\n",
      "new_data = dummy_tr_sin_diff.fit_transform(data_matrix_)\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "cop_ind\n",
      "11/39: dummy_tr_sin_diff.get_feature_names_out()\n",
      "11/40:\n",
      "phi_matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
      "\n",
      "def coup_fun(x,y,i,j,ph_matrix):\n",
      "    return x-y- ph_matrix[i,j]\n",
      "\n",
      "def coup_namer(x,y,i,j,ph_matrix):\n",
      "    return \"sin( {}-{}-phi_{},{} )\".format(x,y,i,j)\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sp_array_2,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= lambda x,y,i,j,ph_matrix : \"{}-{}-phi_{},{}\".format(x,y,i,j),\n",
      "                                                  coupling_func_args={\"ph_matrix\":phi_matrix})\n",
      "11/41:\n",
      "new_data = dummy_tr_sin_diff.fit_transform(data_matrix_)\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "cop_ind\n",
      "11/42: dummy_tr_sin_diff.get_feature_names_out()\n",
      "11/43:\n",
      "phi_matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
      "\n",
      "def coup_fun(x,y,i,j,ph_matrix):\n",
      "    return x-y- ph_matrix[i,j]\n",
      "\n",
      "def coup_namer(x,y,i,j,ph_matrix):\n",
      "    return \"sin( {}-{}-phi_{},{} )\".format(x,y,i,j)\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sp_array_2,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                                  coupling_func_args={\"ph_matrix\":phi_matrix})\n",
      "11/44:\n",
      "new_data = dummy_tr_sin_diff.fit_transform(data_matrix_)\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "cop_ind\n",
      "11/45: dummy_tr_sin_diff.get_feature_names_out()\n",
      "11/46:\n",
      "phi_matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
      "\n",
      "def coup_fun(x,y,i,j,ph_matrix):\n",
      "    return x-y- ph_matrix[i,j]\n",
      "\n",
      "def coup_namer(x,y,i,j,ph_matrix):\n",
      "    return \"sin( {}-{} -phi_{},{} )\".format(x,y,i,j)\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sp_array_2,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                                  coupling_func_args={\"ph_matrix\":phi_matrix})\n",
      "11/47:\n",
      "phi_matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
      "\n",
      "def coup_fun(x,y,i,j,ph_matrix):\n",
      "    return x-y- ph_matrix[i,j]\n",
      "\n",
      "def coup_namer(x,y,i,j,ph_matrix):\n",
      "    return \"sin( {}-{} -phi_{},{} )\".format(x,y,i,j)\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sp_array_2,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"ph_matrix\":phi_matrix},\n",
      "                                              return_df=True)\n",
      "11/48:\n",
      "new_data = dummy_tr_sin_diff.fit_transform(data_matrix_)\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "cop_ind\n",
      "11/49: new_data\n",
      "11/50:\n",
      "phi_matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
      "\n",
      "def coup_fun(x,y,i,j,ph_matrix):\n",
      "    return np.sin(x-y- ph_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,ph_matrix):\n",
      "    return \"sin( {}-{} -phi_{},{} )\".format(x,y,i,j)\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sp_array_2,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"ph_matrix\":phi_matrix},\n",
      "                                              return_df=True)\n",
      "11/51:\n",
      "new_data = dummy_tr_sin_diff.fit_transform(data_matrix_)\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "cop_ind\n",
      "11/52: new_data\n",
      "11/53:\n",
      "i, j = 0,0\n",
      "np.sin(phi_matrix[i,j)]\n",
      "11/54:\n",
      "i, j = 0,0\n",
      "columns_list = list(data_matrix_.columns)\n",
      "np.sin(data_matrix_[columns_list[i]] - data_matrix_[columns_list[j]])\n",
      "11/55:\n",
      "i, j = 0,0\n",
      "columns_list = list(data_matrix_.columns)\n",
      "np.sin(data_matrix_[columns_list[i]] - data_matrix_[columns_list[j]] - phi_matrix[i,j])\n",
      "11/56:\n",
      "i, j = 0,0\n",
      "columns_list = list(data_matrix_.columns)\n",
      "np.sin(data_matrix_[columns_list[i]] - data_matrix_[columns_list[j]] - phi_matrix[i,j]) - new_data[\"sin( t-t -phi_0,0 )\"]\n",
      "11/57:\n",
      "i, j = 0,2\n",
      "columns_list = list(data_matrix_.columns)\n",
      "np.sin(data_matrix_[columns_list[i]] - data_matrix_[columns_list[j]] - phi_matrix[i,j]) - new_data[\"sin( t-y -phi_0,2 )\"]\n",
      "11/58:\n",
      "i, j = 0,2\n",
      "columns_list = list(data_matrix_.columns)\n",
      "np.sin(data_matrix_[columns_list[i]] - data_matrix_[columns_list[j]] - phi_matrix[i,j]) - new_data[\"sin( x-y -phi_1,2 )\"]\n",
      "11/59:\n",
      "i, j = 1,2\n",
      "columns_list = list(data_matrix_.columns)\n",
      "np.sin(data_matrix_[columns_list[i]] - data_matrix_[columns_list[j]] - phi_matrix[i,j]) - new_data[\"sin( x-y -phi_1,2 )\"]\n",
      "11/60:\n",
      "i, j = 1,2\n",
      "columns_list = list(data_matrix_.columns)\n",
      "np.sin(data_matrix_[columns_list[i]] - data_matrix_[columns_list[j]] - phi_matrix[i,j]) - new_data[\"sin( x-x -phi_1,1 )\"]\n",
      "11/61:\n",
      "i, j = 1,1\n",
      "columns_list = list(data_matrix_.columns)\n",
      "np.sin(data_matrix_[columns_list[i]] - data_matrix_[columns_list[j]] - phi_matrix[i,j]) - new_data[\"sin( x-x -phi_1,1 )\"]\n",
      "12/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "12/2:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "\n",
      "# Function to compute derivatives\n",
      "def pendulum_rhs(t, y, gamma, L=1):\n",
      "    \"\"\"\n",
      "    Function to compute derivatives for simple pendulum with damping\n",
      "    \n",
      "    Parameters:\n",
      "        t : float\n",
      "            Time\n",
      "        y : array_like\n",
      "            Vector containing [theta, omega], where\n",
      "            theta is the angle and omega is the angular velocity\n",
      "        gamma : float\n",
      "            Damping coefficient\n",
      "        L : float\n",
      "            Length of the pendulum\n",
      "        \n",
      "    Returns:\n",
      "        dydt : array_like\n",
      "            Vector containing [omega, alpha], where\n",
      "            omega is the angular velocity and alpha is the angular acceleration\n",
      "    \"\"\"\n",
      "    theta, omega = y\n",
      "    alpha = - (9.81 / L) * np.sin(theta) - gamma * omega\n",
      "    return [omega, alpha]\n",
      "\n",
      "# Parameters\n",
      "theta0 = np.pi / 4  # Initial angle (radians)\n",
      "omega0 = 0.0        # Initial angular velocity (radians per second)\n",
      "gamma = 0.0       # Damping coefficient\n",
      "L = 1.0             # Length of the pendulum (meters)\n",
      "t_span = (0, 10)    # Time span for the simulation\n",
      "\n",
      "# Function to integrate the system of ODEs\n",
      "def integrate_pendulum(t_span, y0, gamma, L):\n",
      "\n",
      "    sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, y0, method='RK45', t_eval=np.linspace(*t_span, 1000))\n",
      "    return sol\n",
      "\n",
      "# Integrate the pendulum system\n",
      "sol = integrate_pendulum(t_span, [theta0, omega0], gamma, L)\n",
      "\n",
      "# Plot the results\n",
      "plt.figure(figsize=(10, 5))\n",
      "plt.plot(sol.t, sol.y[0], label='Angle (radians)')\n",
      "plt.plot(sol.t, sol.y[1], label='Angular velocity (rad/s)')\n",
      "plt.title('Damped Simple Pendulum Simulation using scipy.solve_ivp')\n",
      "plt.xlabel('Time (s)')\n",
      "plt.ylabel('Value')\n",
      "plt.legend()\n",
      "plt.grid(True)\n",
      "plt.show()\n",
      "12/3: IC_df = pd.read_csv(\"parameters/init_cond_simp_pend.csv\")\n",
      "12/4:\n",
      "IC_df = IC_df[0:2]\n",
      "IC_df\n",
      "12/5:\n",
      "# Mechanical eEnergy level\n",
      "0.5*(IC_df[\"omega\"])**2 + 9.81*(1-np.cos(IC_df[\"theta\"]))\n",
      "12/6:\n",
      "params_df = pd.read_csv(\"parameters/pend_param.csv\")\n",
      "params_df\n",
      "12/7: g = 9.81   # Acceleration due to gravity (m/s^2)\n",
      "12/8:\n",
      "L = 5.0\n",
      "# y_shift = 0.9 * L\n",
      "# y_shift = 0\n",
      "\n",
      "num_time_points = 20\n",
      "# Time span\n",
      "t_span = (0.0, 10)  # from 0 to 10 seconds\n",
      "#Valuation points\n",
      "t_eval_ = np.linspace(t_span[0], t_span[1], num_time_points)\n",
      "data_matrix_df_list = []\n",
      "\n",
      "\n",
      "for param_index in params_df.index:\n",
      "    params = params_df.loc[param_index]\n",
      "    # Define parameters\n",
      "    m_c = params['m_c']  # Mass of the cart (kg)\n",
      "    m_p = params['m_p']  # Mass of the pendulum (kg)\n",
      "    l = params['l']    # Length of the pendulum (m)\n",
      "    for IC_index in IC_df.index:\n",
      "        IC = IC_df.loc[IC_index]\n",
      "        y0 = IC.values\n",
      "                # Parameters\n",
      "        theta0 = IC[\"theta\"]  # Initial angle (radians)\n",
      "        omega0 = IC[\"omega\"]        # Initial angular velocity (radians per second)\n",
      "        gamma = 0.0         # Damping coefficient\n",
      "        # Solve the ODEs\n",
      "        sol = solve_ivp(lambda t, y: pendulum_rhs(t, y, gamma, L), t_span, [theta0, omega0], method='RK45', t_eval=t_eval_)\n",
      "        sol_df = pd.DataFrame(sol.y.T, columns=[\"theta\", \"omega\"])\n",
      "        sol_df[\"x\"] = L*np.sin(sol_df[\"theta\"])\n",
      "        sol_df[\"y\"] = -L*np.cos(sol_df[\"theta\"])\n",
      "        sol_df[\"t\"] = t_eval_\n",
      "        data_matrix_df_list.append(sol_df[[\"t\", \"x\", \"y\"]])\n",
      "        # if IC_index == 0:\n",
      "        #     # Plot the results\n",
      "        #     plt.figure(figsize=(10, 6))\n",
      "        #     plt.plot(sol.t, sol.y[0], label='Cart Position (x)')\n",
      "        #     plt.plot(sol.t, sol.y[2], label='Pendulum Angle (theta)')\n",
      "        #     plt.xlabel('Time (s)')\n",
      "        #     plt.ylabel('Position (m) / Angle (rad)')\n",
      "        #     plt.title('Upright Pendulum on Moving Cart')\n",
      "        #     plt.legend()\n",
      "        #     plt.grid(True)\n",
      "        #     plt.show()\n",
      "\n",
      "data_matrix_df = pd.concat(data_matrix_df_list, ignore_index=True)\n",
      "data_matrix_df\n",
      "12/9:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 10\n",
      "\n",
      "\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"t\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"t\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "12/10: from scipy import interpolate\n",
      "12/11:\n",
      "#smoothing parameter: when equal weightage: num_data_points * std of data\n",
      "s_param = num_time_points * (0.01*noise_perc*data_matrix_df_list[1].std()[\"x\"])**2\n",
      "\n",
      "tck = interpolate.splrep(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], s=s_param)\n",
      "t_eval_new = np.linspace(t_span[0], t_span[1], 500)\n",
      "x_new = interpolate.splev(t_eval_new, tck, der=0)\n",
      "\n",
      "plt.figure()\n",
      "plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "        data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "plt.legend(['Linear', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation')\n",
      "plt.show()\n",
      "12/12: (3/4)*np.pi\n",
      "12/13: data_matrix_df[[\"x\",\"y\"]].plot()\n",
      "12/14:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "num_smoothed_points = num_time_points*10\n",
      "\n",
      "t_eval_new = np.linspace(data_matrix_df_list[0][\"t\"].iloc[0], data_matrix_df_list[0][\"t\"].iloc[-1], num_smoothed_points)\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,derr_order=1, noise_perc=noise_perc, eval_points=t_eval_new) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list[:-1], ignore_index=True)\n",
      "12/15:\n",
      "ind = 1\n",
      "feature_ = \"y\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"t\"], data_matrix_df_list[ind][feature_], \"x\", data_matrix_smooth_df_list[ind][\"t\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"t\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \"o\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "12/16: data_matrix_smooth_df_list[0][[\"d(x) /dt\"]].plot()\n",
      "12/17: data_matrix_smooth_df_list[0][[\"d(x) /dt\"]].plot()\n",
      "12/18:\n",
      "# data_matrix_df = data_matrix_df_appended[[\"x\",\"y\"]]\n",
      "# data_matrix_df = pd.concat([data_matrix_df, data_matrix_df_appended[[\"d(u) /dt\"]]], axis=1)\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[[\"x\",\"y\", \"d(x) /dt\", \"d(y) /dt\"]]\n",
      "data_matrix_df_smooth\n",
      "12/19:\n",
      "from copy import deepcopy\n",
      "new_df = deepcopy(data_matrix_df_smooth)\n",
      "\n",
      "new_df[\"energy\"] = 0.5*((new_df[\"d(x) /dt\"])**2 + (new_df[\"d(y) /dt\"])**2) +  9.81*new_df[\"y\"]\n",
      "\n",
      "new_df.plot()\n",
      "12/20:\n",
      "from sklearn.preprocessing import FunctionTransformer\n",
      "from copy import deepcopy\n",
      "\n",
      "def sin_transformer(period = 2*np.pi):\n",
      "    return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))\n",
      "\n",
      "\n",
      "def sin_diff_transformer(period = 2*np.pi):\n",
      "    return FunctionTransformer(lambda x,y: np.sin((x-y) / period * 2 * np.pi))\n",
      "\n",
      "def cos_transformer(period = 2*np.pi):\n",
      "    return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))\n",
      "\n",
      "\n",
      "def sin_shift_transformer(period = 2*np.pi):\n",
      "    return FunctionTransformer(lambda x,y: np.sin((x-y) / period * 2 * np.pi))\n",
      "\n",
      "data_matrix_df_with_trig = deepcopy(data_matrix_df)\n",
      "# data_matrix_df_with_trig[\"sin(theta)\"] = sin_transformer(1).fit_transform(data_matrix_df_with_trig)[\"theta\"]\n",
      "# data_matrix_df_with_trig[\"cos(theta)\"] = cos_transformer(1).fit_transform(data_matrix_df_with_trig)[\"theta\"]\n",
      "12/21:\n",
      "from scipy.sparse import coo_array\n",
      "\n",
      "row  = np.array([0, 3, 1, 0, 0, 0])\n",
      "col  = np.array([0, 3, 1, 1, 2, 4])\n",
      "data = np.array([4, 5, 7, 9, 100, 101])\n",
      "# sp_array = coo_array((data, (row, col)), shape=(4, 4))\n",
      "sp_array = coo_array((data, (row, col)))\n",
      "12/22: list(zip(sp_array.row, sp_array.col))\n",
      "12/23:\n",
      "# from sklearn.base import BaseEstimator, TransformerMixin\n",
      "# from scipy.sparse import coo_array\n",
      "# from sklearn.utils.validation import (\n",
      "#     FLOAT_DTYPES,\n",
      "#     _check_feature_names_in,\n",
      "#     _check_sample_weight,\n",
      "#     check_is_fitted,\n",
      "# )\n",
      "\n",
      "# class FeatureDiffTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "#     def __init__(self, sparsity_matrix = None, coupled_indices_list = None, power=1, comp_function=None):\n",
      "#         #If coupled indices list is not explicitly given to the constructor, a valid sparsity matrix\n",
      "#         # from which the coupled indices can be implied should be provided. \n",
      "#         if not coupled_indices_list:\n",
      "#             assert isinstance(sparsity_matrix, coo_array), \"FeatureDiffTransformer only support sparsity matrix\\\n",
      "#             in the scipy.sparse.coo_array format\"\n",
      "#             self.sparsity_matrix = sparsity_matrix\n",
      "#         self.coupled_indices_list = coupled_indices_list\n",
      "#         if not comp_function:\n",
      "#             self.comp_function = comp_function\n",
      "#         else:\n",
      "#             self.comp_function = lambda x: x\n",
      "#         assert power>=1, \"difference power needs to be atleast 1\"\n",
      "#         self.power = power\n",
      "\n",
      "#         self.n_features_in_ = 0\n",
      "#         self.feature_names_in_ = None\n",
      "\n",
      "\n",
      "#     def get_feature_names_out(self, input_features=None):\n",
      "#         \"\"\"Get output feature names for transformation.\n",
      "\n",
      "#         Parameters\n",
      "#         ----------\n",
      "#         input_features : array-like of str or None, default=None\n",
      "#             Input features.\n",
      "\n",
      "#             - If `input_features is None`, then `feature_names_in_` is\n",
      "#               used as feature names in. If `feature_names_in_` is not defined,\n",
      "#               then the following input feature names are generated:\n",
      "#               `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "#             - If `input_features` is an array-like, then `input_features` must\n",
      "#               match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "#             It is recommended that the coupling between features are given using a sparsity matrix\n",
      "#             instead of coupling indices. \n",
      "\n",
      "#         Returns\n",
      "#         -------\n",
      "#         feature_names_out : ndarray of str objects\n",
      "#             Transformed feature names.\n",
      "#         \"\"\"\n",
      "#         check_is_fitted(self)\n",
      "#         input_features = _check_feature_names_in(self, input_features)\n",
      "#         feature_names = [\"({}-{})\".format(input_features[i], input_features[j]) for i,j in self.coupled_indices_list]\n",
      "\n",
      "#         return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "#     def fit(self, X, y=None):\n",
      "#         \"\"\"\n",
      "        \n",
      "#         \"\"\"\n",
      "#         self.n_features_in_ = X.shape[1]\n",
      "#         if len(X.columns) >0:\n",
      "#             self.feature_names_in_ = X.columns\n",
      "#         if not self.coupled_indices_list: #sparsity matrix gives the coupling indices\n",
      "#             assert max(self.sparsity_matrix.col.max(), self.sparsity_matrix.row.max()) <=self.n_features_in_-1, \\\n",
      "#             \"sparsity matrix has indices out of bound of the number of features\"\n",
      "#             #Extracting the indices that has coupling with each other. \n",
      "#             self.coupled_indices_list = list(zip(self.sparsity_matrix.row, self.sparsity_matrix.col))\n",
      "        \n",
      "#         return self\n",
      "\n",
      "\n",
      "#     def transform(self, X):\n",
      "#         \"\"\"Transform data to specified difference feature\n",
      "\n",
      "#         Parameters\n",
      "#         ----------\n",
      "#         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "#             The data to transform, row by row.\n",
      "\n",
      "#         Returns\n",
      "#         -------\n",
      "#         XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "#             The matrix of features, where `NS` is the number of non-zero\n",
      "#             connections implied from the sparsity matrix.\n",
      "#         \"\"\"\n",
      "#         check_is_fitted(self)\n",
      "\n",
      "#         X = self._validate_data(\n",
      "#             X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "#         )\n",
      "#         X_transpose = X.T\n",
      "#         if self.comp_function == None:\n",
      "#             X_diff = np.vstack([X_transpose[i] - X_transpose[j] for i,j in self.coupled_indices_list]).T\n",
      "#         else:\n",
      "            \n",
      "\n",
      "#         return X_diff\n",
      "12/24:\n",
      "# from sklearn.base import BaseEstimator, TransformerMixin\n",
      "# from scipy.sparse import coo_array\n",
      "# from sklearn.utils.validation import (\n",
      "#     FLOAT_DTYPES,\n",
      "#     _check_feature_names_in,\n",
      "#     _check_sample_weight,\n",
      "#     check_is_fitted,\n",
      "# )\n",
      "\n",
      "# class FeatureCouplingTransformer(TransformerMixin, BaseEstimator):\n",
      "\n",
      "#     def __init__(self, sparsity_matrix = None, coupled_indices_list = None,\n",
      "#                  power=1, coupling_func=None, coupling_namer=None,\n",
      "#                 coupling_func_args = {}, return_df = False):\n",
      "#         \"\"\"\n",
      "#         Note that the coupling_func and coupling_namer function should have arguments\n",
      "#         (feature_1_value,feature_2_value, i, j) as the first four arguments. \n",
      "        \n",
      "#         coupling_func_args (dict): optional keyword arguments for the coupling_function and coupling_namer functions\n",
      "#         \"\"\"\n",
      "#         #If coupled indices list is not explicitly given to the constructor, a valid sparsity matrix\n",
      "#         # from which the coupled indices can be implied should be provided. \n",
      "#         if not coupled_indices_list:\n",
      "#             assert isinstance(sparsity_matrix, coo_array), \"FeatureDiffTransformer only support sparsity matrix\\\n",
      "#             in the scipy.sparse.coo_array format\"\n",
      "#             self.sparsity_matrix = sparsity_matrix\n",
      "#         self.coupled_indices_list = coupled_indices_list\n",
      "#         if not coupling_func:\n",
      "#             self.coupling_func = lambda x, y, i, j: x*y\n",
      "#         else:\n",
      "#             #If coupling function is not given, it is defined as the interaction term feature_1*feature_2\n",
      "#             self.coupling_func = coupling_func\n",
      "\n",
      "#         if not coupling_namer:\n",
      "#             self.coupling_namer = lambda feature_1, feature_2, i, j: \"{}*{}\".format(feature_1, feature_2)\n",
      "#         else:\n",
      "#             self.coupling_namer = coupling_namer\n",
      "\n",
      "#         self.coupling_func_args = coupling_func_args\n",
      "#         self.return_df = return_df\n",
      "            \n",
      "#         assert power>=1, \"difference power needs to be atleast 1\"\n",
      "#         self.power = power\n",
      "\n",
      "#         self.n_features_in_ = 0\n",
      "#         self.feature_names_in_ = None\n",
      "\n",
      "\n",
      "#     def get_feature_names_out(self, input_features=None):\n",
      "#         \"\"\"Get output feature names for transformation.\n",
      "\n",
      "#         Parameters\n",
      "#         ----------\n",
      "#         input_features : array-like of str or None, default=None\n",
      "#             Input features.\n",
      "\n",
      "#             - If `input_features is None`, then `feature_names_in_` is\n",
      "#               used as feature names in. If `feature_names_in_` is not defined,\n",
      "#               then the following input feature names are generated:\n",
      "#               `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\n",
      "#             - If `input_features` is an array-like, then `input_features` must\n",
      "#               match `feature_names_in_` if `feature_names_in_` is defined.\n",
      "#             It is recommended that the coupling between features are given using a sparsity matrix\n",
      "#             instead of coupling indices. \n",
      "\n",
      "#         Returns\n",
      "#         -------\n",
      "#         feature_names_out : ndarray of str objects\n",
      "#             Transformed feature names.\n",
      "#         \"\"\"\n",
      "#         check_is_fitted(self)\n",
      "#         input_features = _check_feature_names_in(self, input_features)\n",
      "        \n",
      "#         feature_names = [self.coupling_namer(input_features[i], input_features[j], i, j, **self.coupling_func_args) for i,j in self.coupled_indices_list]\n",
      "\n",
      "#         return np.asarray(feature_names, dtype=object)\n",
      "\n",
      "#     def fit(self, X, y=None):\n",
      "#         \"\"\"\n",
      "        \n",
      "#         \"\"\"\n",
      "#         self.n_features_in_ = X.shape[1]\n",
      "#         if len(X.columns) >0:\n",
      "#             self.feature_names_in_ = X.columns\n",
      "#         if not self.coupled_indices_list: #sparsity matrix gives the coupling indices\n",
      "#             assert max(self.sparsity_matrix.col.max(), self.sparsity_matrix.row.max()) <=self.n_features_in_-1, \\\n",
      "#             \"sparsity matrix has indices out of bound of the number of features\"\n",
      "#             #Extracting the indices that has coupling with each other. \n",
      "#             self.coupled_indices_list = list(zip(self.sparsity_matrix.row, self.sparsity_matrix.col))\n",
      "        \n",
      "#         return self\n",
      "\n",
      "\n",
      "#     def transform(self, X):\n",
      "#         \"\"\"Transform data to specified difference feature\n",
      "\n",
      "#         Parameters\n",
      "#         ----------\n",
      "#         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "#             The data to transform, row by row.\n",
      "\n",
      "#         Returns\n",
      "#         -------\n",
      "#         XP : {ndarray, sparse matrix} of shape (n_samples, NS)\n",
      "#             The matrix of features, where `NS` is the number of non-zero\n",
      "#             connections implied from the sparsity matrix.\n",
      "#         \"\"\"\n",
      "#         check_is_fitted(self)\n",
      "\n",
      "#         X = self._validate_data(\n",
      "#             X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
      "#         )\n",
      "#         X_transpose = X.T\n",
      "#         X_coupled = np.vstack([self.coupling_func(X_transpose[i], X_transpose[j], i, j, **self.coupling_func_args) \n",
      "#                                for i,j in self.coupled_indices_list]).T\n",
      "#         if self.return_df:\n",
      "#             return pd.DataFrame(X_coupled, columns= self.get_feature_names_out())\n",
      "            \n",
      "\n",
      "#         return X_coupled\n",
      "12/25:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "from scipy.sparse import coo_array\n",
      "\n",
      "row  = np.array([0, 0, 1, 1])\n",
      "col  = np.array([0, 2, 2, 1])\n",
      "data = np.array([4, 5, 7, 5])\n",
      "sp_array_2 = coo_array((data, (row, col)))\n",
      "\n",
      "\n",
      "# dummy_tr_interact = FeatureDiffTransformer(sp_array_2, power=1)\n",
      "dummy_tr = FeatureCouplingTransformer(sp_array_2, return_df=True)\n",
      "dummy_tr_diff = FeatureCouplingTransformer(sp_array_2,\n",
      "                                           coupling_func= lambda x,y, i, j: x-y,\n",
      "                                           coupling_namer= lambda x,y,i,j : \"{}-{}\".format(x,y))\n",
      "\n",
      "def coup_fun(x,y,i,j,k=0):\n",
      "    return x-y-k\n",
      "dummy_tr_diff_minus_k = FeatureCouplingTransformer(sp_array_2,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= lambda x,y,i,j,k : \"{}-{}-{}\".format(x,y,k),\n",
      "                                                  coupling_func_args={\"k\":2})\n",
      "\n",
      "dummy_tr_diff \n",
      "# dummy_tr = FeatureDiffTransformer(coupled_indices_list=[(1, 2), (1, 1), (100, 101)], power=1)\n",
      "12/26:\n",
      "phi_matrix = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
      "\n",
      "def coup_fun(x,y,i,j,ph_matrix):\n",
      "    return np.sin(x-y- ph_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,ph_matrix):\n",
      "    return \"sin( {}-{} -phi_{},{} )\".format(x,y,i,j)\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sp_array_2,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"ph_matrix\":phi_matrix},\n",
      "                                              return_df=True)\n",
      "12/27:\n",
      "new_data = dummy_tr_sin_diff.fit_transform(data_matrix_)\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "cop_ind\n",
      "12/28:\n",
      "# dummy_tr._check_n_features(data_matrix_, 0)\n",
      "# dummy_tr._check_feature_names(data_matrix_, reset = 0)\n",
      "12/29: new_data\n",
      "12/30:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "from scipy.sparse import coo_array\n",
      "\n",
      "row  = np.array([0, 0, 1, 1])\n",
      "col  = np.array([0, 2, 2, 1])\n",
      "data = np.array([4, 5, 7, 5])\n",
      "sp_array_2 = coo_array((data, (row, col)))\n",
      "12/31: np.triu(3)\n",
      "12/32: np.triu(,13)\n",
      "12/33: np.triu(2,3)\n",
      "12/34: np.triu((2,3))\n",
      "12/35: np.ones(3,3)\n",
      "12/36: np.ones((3,3))\n",
      "12/37: np.triu(np.ones((3,3)))\n",
      "12/38: np.triu(np.ones((3,3)), 2)\n",
      "12/39: np.triu(np.ones((3,3)), 1)\n",
      "12/40:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "13/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "13/2: gamma_df = pd.read_csv(\"powergrid/Datasets/case3_gamma.csv\")\n",
      "13/3: !pwd\n",
      "13/4: gamma_df = pd.read_csv(\"powergrid/Datasets/case3_gamma.csv\")\n",
      "13/5: gamma_df\n",
      "13/6: data_matrix_df = pd.read_csv(\"powergrid/Datasets/case3_timeseries.csv\")\n",
      "13/7: data_matrix_df\n",
      "13/8:\n",
      "skip_n_rows_btw = 10\n",
      "rows_to_keep = np.linspace(0, len(data_matrix_df), 10)\n",
      "13/9: data_matrix_df = pd.read_csv(\"powergrid/Datasets/case3_timeseries.csv\")\n",
      "13/10:\n",
      "skip_n_rows_btw = 10\n",
      "rows_to_keep = np.linspace(0, len(data_matrix_df), 10)\n",
      "13/11: len(data_matr\n",
      "13/12: len(data_matrix_df)\n",
      "13/13: rows_to_keep\n",
      "13/14: len(rows_to_keep)\n",
      "13/15: rows_to_keep\n",
      "13/16: np.arange??\n",
      "13/17: np.arange(1,10,5)\n",
      "13/18: np.arange(0,10,5)\n",
      "13/19:\n",
      "skip_n_rows_btw = 10\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df), 10)\n",
      "13/20: rows_to_keep\n",
      "13/21: len(rows_to_keep)\n",
      "13/22:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df), 10)\n",
      "13/23: len(rows_to_keep)\n",
      "13/24:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df), skip_n_rows_btw)\n",
      "13/25: len(rows_to_keep)\n",
      "13/26: rows_to_keep\n",
      "13/27: data_matrix_df\n",
      "13/28: data_matrix_df = pd.read_csv(\"powergrid/Datasets/case3_timeseries.csv\")\n",
      "13/29: data_matrix_df\n",
      "13/30:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df), skip_n_rows_btw)\n",
      "13/31: data_matrix_df\n",
      "13/32: data_matrix_df.iloc[rows_to_keep]\n",
      "13/33: data_matrix_df_2 = data_matrix_df.iloc[rows_to_keep]\n",
      "13/34: data_matrix_df_2\n",
      "13/35: data_matrix_df_2.reset_index()\n",
      "13/36: data_matrix_df_2.reset_index??\n",
      "13/37: data_matrix_df_2.reset_index(drop=True)\n",
      "13/38: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "13/39: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case3_timeseries.csv\")\n",
      "13/40:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df), skip_n_rows_btw)\n",
      "13/41: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "13/42: data_matrix_df\n",
      "13/43: Y_df = pd.read_csv(\"powergrid/Datasets/case3_Y.csv\")\n",
      "13/44: Y_df\n",
      "13/45:\n",
      "#Dictionary keeping track of the index in the connection matrix and the node name\"\n",
      "index_to_name_dict = {ind:name for ind,name in enumerate(Y_df.columns)}\n",
      "13/46: index_to_name_dict\n",
      "13/47: static_param_df = pd.read_csv(\"powergrid/Datasets/case3_staticparams.csv\")\n",
      "13/48: static_param_df\n",
      "13/49: admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case3_Y.csv\")\n",
      "13/50: admittance_Y_df\n",
      "13/51:\n",
      "#Dictionary keeping track of the index in the admittance matrix, Y and the node name\"\n",
      "index_to_name_dict = {ind:name for ind,name in enumerate(Y_df.columns)}\n",
      "13/52: coupling_Y_df = pd.read_csv(\"powergrid/Datasets/case3_K.csv\")\n",
      "13/53: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case3_K.csv\")\n",
      "13/54: admittance_K_df\n",
      "13/55: coupling_K_df\n",
      "13/56: admittance_Y_df[\"gen_1\"]\n",
      "13/57: admittance_Y_df[\"gen_1\"][0]\n",
      "13/58: type(admittance_Y_df[\"gen_1\"][0])\n",
      "13/59: admittance_Y_df[\"gen_1\"][0]\n",
      "13/60: np.complex(admittance_Y_df[\"gen_1\"][0])\n",
      "13/61: np.complex_\n",
      "13/62: np.complex_??\n",
      "13/63: np.complex_(admittance_Y_df[\"gen_1\"][0])\n",
      "13/64: np.complex_('0-0.8j')\n",
      "13/65: admittance_Y_df.replace('i', 'j')\n",
      "13/66: admittance_Y_df.apply(lambda x: x+ \"hehehe\")\n",
      "13/67: admittance_Y_df.apply(lambda x: x.replace(\"i\", \"j\")\n",
      "13/68: admittance_Y_df.apply(lambda x: x.replace(\"i\", \"j\"))\n",
      "13/69: admittance_Y_df[\"gen_1\"][0]\n",
      "13/70: admittance_Y_df[\"gen_1\"][0].replace('i', 'j')\n",
      "13/71: admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case3_Y.csv\")\n",
      "13/72: admittance_Y_df\n",
      "13/73: admittance_Y_df.apply(lambda x: x.replace('i', 'j'))\n",
      "13/74: admittance_Y_df.apply(lambda x: x+\"j\")\n",
      "13/75: admittance_Y_df.apply(lambda x: str(x))\n",
      "13/76: np.complex_??\n",
      "13/77: admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case3_Y.csv\")\n",
      "13/78: admittance_Y_df\n",
      "13/79: admittance_Y_df\n",
      "13/80: np.complex_(admittance_Y_df[\"gen_1\"][0].replace('i', 'j'))\n",
      "13/81: admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case3_Y.csv\")\n",
      "13/82: admittance_Y_df\n",
      "13/83: admittance_Y_df.apply(lambda x: x + len(x))\n",
      "13/84: admittance_Y_df.apply(lambda x: x + str(len(x)))\n",
      "13/85: admittance_Y_df.apply(lambda x: x)\n",
      "13/86: admittance_Y_df.apply(lambda x: x.replace('i', '')\n",
      "13/87: admittance_Y_df.apply(lambda x: x.replace('i', ''))\n",
      "13/88: admittance_Y_df.apply(lambda x: x + str(len(x)))\n",
      "13/89: admittance_Y_df.apply(lambda x: x + str(len(x)), 1)\n",
      "13/90: admittance_Y_df.apply(lambda x: x + str(len(x)), 0)\n",
      "13/91: admittance_Y_df.apply(lambda x: x.replace('i', 'j'), 0)\n",
      "13/92: admittance_Y_df.apply(lambda x: x.replace('i', 'j'), 0)\n",
      "13/93: admittance_Y_df.apply(lambda x: x.replace('i', 'j'), axis=1)\n",
      "13/94: admittance_Y_df.apply(lambda x: x.replace('i', 'j'), axis=0)\n",
      "13/95: admittance_Y_df.apply(lambda x: np.complex_(x), axis=0)\n",
      "13/96: admittance_Y_df.apply(lambda x: np.complex_(x.replace('i', 'j'), axis=0)\n",
      "13/97: admittance_Y_df.apply(lambda x: np.complex_(x.replace('i', 'j')), axis=0)\n",
      "13/98: admittance_Y_df[\"gen_1\"].apply(lambda x: x.replace('i', 'j'), axis=0)\n",
      "13/99: admittance_Y_df[\"gen_1\"].apply(lambda x: x.replace('i', 'j'))\n",
      "13/100: admittance_Y_df.apply(lambda x: x.replace('i', 'j'))\n",
      "13/101:\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "13/102: admittance_Y_df\n",
      "13/103:\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "13/104: admittance_Y_df\n",
      "13/105:\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: np.complex_(x.replace('i', 'j')))\n",
      "13/106: admittance_Y_df\n",
      "13/107: np.complex(0)\n",
      "13/108: np.complex_(0)\n",
      "13/109: admittance_Y_df[\"gen_1]\n",
      "13/110: admittance_Y_df[\"gen_1\"]\n",
      "13/111: admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case3_Y.csv\")\n",
      "13/112: admittance_Y_df\n",
      "13/113:\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "13/114: admittance_Y_df[\"gen_1\"]\n",
      "13/115: admittance_Y_df[\"gen_1\"].apply(lambda x: np.complex_(x))\n",
      "13/116: np.complex_('0')\n",
      "13/117: np.complex_(0)\n",
      "13/118:\n",
      "for el in admittance_Y_df[\"gen_1\"]:\n",
      "    print(el)\n",
      "13/119:\n",
      "for el in admittance_Y_df[\"gen_1\"]:\n",
      "    print(np.complex_(el))\n",
      "13/120: np.complex(admittance_Y_df[\"gen_1\"])\n",
      "13/121: np.complex_(admittance_Y_df[\"gen_1\"])\n",
      "13/122: admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case3_Y.csv\")\n",
      "13/123: admittance_Y_df\n",
      "13/124:\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "13/125: admittance_Y_df\n",
      "13/126:\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = np.complex_(admittance_Y_df[column].apply(lambda x: x.replace('i', 'j')))\n",
      "13/127: admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case3_Y.csv\")\n",
      "13/128: admittance_Y_df\n",
      "13/129:\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = np.complex_(admittance_Y_df[column].apply(lambda x: x.replace('i', 'j')))\n",
      "13/130: admittance_Y_df\n",
      "13/131: admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case3_Y.csv\")\n",
      "13/132: admittance_Y_df\n",
      "13/133:\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "13/134:\n",
      "for column in admittance_Y_df:\n",
      "    print(np.complex_(admittance_Y_df[column]))\n",
      "13/135:\n",
      "for column in admittance_Y_df:\n",
      "    admittance_Y_df[column] = np.complex_(admittance_Y_df[column])\n",
      "    print(np.complex_(admittance_Y_df[column]))\n",
      "13/136: admittance_Y_df\n",
      "13/137:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case3_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = np.complex_(admittance_Y_df[column].apply(lambda x: x.replace('i', 'j')))\n",
      "13/138: admittance_Y_df\n",
      "13/139:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case3_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "13/140: admittance_Y_df\n",
      "13/141: static_param_df = pd.read_csv(\"powergrid/Datasets/case3_staticparams.csv\")\n",
      "13/142: static_param_df\n",
      "13/143: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case3_K.csv\")\n",
      "13/144: coupling_K_df\n",
      "13/145:\n",
      "# This part comes after finding conservation laws and refining the library\n",
      "\n",
      "num_alg_relationships = 3\n",
      "\n",
      "for i in range(num_alg_relationships):\n",
      "    algebraic_model_seletor.fit(refined_candit_lib)\n",
      "\n",
      "    #Find the best fit relationship form algebraic_model_seletor.best_models(1):\n",
      "#    This includes something like [E2] = [E2]*[B] + intercept\n",
      "# Find the intercept using algebraic_model_selector.get_fitted_intercepts()[\"E2\"]\n",
      "# Form the equation (possibly using sympy): [E2] = [E2]*[B] + 3.0 (if abs(intercept)>0.2, other wise intercept =0)\n",
      "13/146:\n",
      "# This part comes after finding conservation laws and refining the library\n",
      "\n",
      "num_alg_relationships = 3\n",
      "\n",
      "for i in range(num_alg_relationships):\n",
      "    algebraic_model_seletor.fit(refined_candit_lib)\n",
      "\n",
      "    #Find the best fit relationship form algebraic_model_seletor.best_models(1):\n",
      "#    This includes something like [E2] = [E2]*[B] + intercept\n",
      "# Find the intercept using algebraic_model_selector.get_fitted_intercepts()[\"E2\"]\n",
      "# Form the equation (possibly using sympy): [E2] = [E2]*[B] + 3.0 (if abs(intercept)>0.2, other wise intercept =0)\\\n",
      "#Factor out possible common terms if hte intercept == 0. \n",
      "# Find the term with the highest power (sum of power of each term) and feed that term to get_refined_library() function\n",
      "13/147: coupling_K_df\n",
      "13/148:\n",
      "from scipy.sparse import coo_array\n",
      "\n",
      "row  = np.array([0, 3, 1, 0, 0, 0])\n",
      "col  = np.array([0, 3, 1, 1, 2, 4])\n",
      "data = np.array([4, 5, 7, 9, 100, 101])\n",
      "# sp_array = coo_array((data, (row, col)), shape=(4, 4))\n",
      "sp_array = coo_array((data, (row, col)))\n",
      "13/149: sp_array\n",
      "13/150: sp_array.toarray()\n",
      "13/151: gamma_df.to_numpy()\n",
      "13/152: gamma_matrix = gamma_df.to_numpy()\n",
      "13/153:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "13/154: admittance_Y_matrix\n",
      "13/155: admittance_Y_matrix.shape\n",
      "13/156: coupling_matrix_init = np.(np.ones(admittance_Y_matrix.shape), 1)\n",
      "13/157: coupling_matrix_init = np.triu(np.ones(admittance_Y_matrix.shape), 1)\n",
      "13/158:\n",
      "coupling_matrix_init = np.triu(np.ones(admittance_Y_matrix.shape), 1)\n",
      "coupling_matrix_init\n",
      "13/159:\n",
      "coupling_matrix_init = np.triu(np.ones(admittance_Y_matrix.shape), 0)\n",
      "coupling_matrix_init\n",
      "13/160:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "13/161:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "coupling_matrix_init = np.triu(np.ones(admittance_Y_matrix.shape), 0)\n",
      "13/162:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "13/163:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "13/164:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "13/165:\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "13/166:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "13/167:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "from scipy.sparse import coo_array\n",
      "13/168:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_ini\n",
      "13/169:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init\n",
      "13/170:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "13/171:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "13/172: data_matrix_df\n",
      "13/173: data_matrix_df.drop([\"time\", axis=1])\n",
      "13/174: data_matrix_df.drop([\"time\"], axis=1)\n",
      "13/175:\n",
      "new_data = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "cop_ind\n",
      "13/176: new_data_\n",
      "13/177: new_data\n",
      "13/178:\n",
      "new_data = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "13/179:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "13/180: sin_diff_library\n",
      "13/181:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "13/182: coupling_K_df.set_index(coupling_K_df.columns)\n",
      "13/183:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "13/184: pd.concat(data_matrix_df, sin_diff_library)\n",
      "13/185: pd.concat([data_matrix_df, sin_diff_library])\n",
      "13/186: pd.concat([data_matrix_df, sin_diff_library], axis=1)\n",
      "13/187: candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1), sin_diff_library], axis=1)\n",
      "13/188: candidate_lib\n",
      "13/189: candidate_lib\n",
      "13/190:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "13/191: phase_gen_1\n",
      "13/192: phase_gen_2\n",
      "13/193: phase_term_2\n",
      "13/194:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       fit_intercept=True)\n",
      "13/195: algebraic_model_lasso.fit(candidate_lib, scale_columns= True)\n",
      "13/196:\n",
      "#Best 10 models using R2 metrix\n",
      "algebraic_model_lasso.best_models(5)\n",
      "13/197: list(data_matrix_df.columns)\n",
      "13/198:\n",
      "for name in list(data_matrix_df.columns):\n",
      "    print(name)\n",
      "13/199:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\"]\n",
      "new_column_names\n",
      "13/200: data_matrix_df.columns\n",
      "13/201:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "new_column_names\n",
      "13/202:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "data_matrix_df.rename_axis??\n",
      "13/203:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "data_matrix_df.rename_axis(new_column_names, axis=1)\n",
      "13/204:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "data_matrix_df.rename(new_column_names, axis=1)\n",
      "13/205:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "data_matrix_df.rename{dict((zip(data_matrix_df.columns, new_column_names)))}\n",
      "13/206:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "data_matrix_df.rename(dict((zip(data_matrix_df.columns, new_column_names))))\n",
      "13/207: dict((zip(data_matrix_df.columns, new_column_names)))\n",
      "13/208:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "data_matrix_df.rename(dict((zip(data_matrix_df.columns, new_column_names))), inplace=True)\n",
      "13/209: data_matrix_df\n",
      "13/210:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "data_matrix_df.rename(dict((zip(data_matrix_df.columns, new_column_names))))\n",
      "13/211: data_matrix_df\n",
      "13/212:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))))\n",
      "13/213:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "13/214: data_matrix_df\n",
      "13/215: dict((zip(data_matrix_df.columns, new_column_names)))\n",
      "13/216: data_matrix_df.columns\n",
      "13/217:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case3_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "13/218: admittance_Y_df\n",
      "13/219: static_param_df = pd.read_csv(\"powergrid/Datasets/case3_staticparams.csv\")\n",
      "13/220: static_param_df\n",
      "13/221: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case3_K.csv\")\n",
      "13/222:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "13/223:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "13/224:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "13/225:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "13/226:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "13/227: sin_diff_library\n",
      "13/228: candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1), sin_diff_library], axis=1)\n",
      "13/229: candidate_lib\n",
      "13/230:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "13/231:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       fit_intercept=True)\n",
      "13/232: algebraic_model_lasso.fit(candidate_lib, scale_columns= True)\n",
      "13/233:\n",
      "#Best 10 models using R2 metrix\n",
      "algebraic_model_lasso.best_models(5)\n",
      "13/234:\n",
      "#Best 10 models using R2 metrix\n",
      "algebraic_model_lasso.best_models(1)\n",
      "13/235:\n",
      "#Best 10 models using R2 metrix\n",
      "algebraic_model_lasso.best_models()\n",
      "13/236:\n",
      "#Best 10 models using R2 metrix\n",
      "algebraic_model_lasso.best_models()[\"P_0\"]\n",
      "13/237:\n",
      "#Best 10 models using R2 metrix\n",
      "algebraic_model_lasso.best_models()[\"P_1\"]\n",
      "13/238: algebraic_model_lasso.get_fitted_intercepts()\n",
      "13/239: algebraic_model_lasso.get_fitted_intercepts()[\"P_1\"]\n",
      "13/240: algebraic_model_lasso.best_models()[\"P_1\"]\n",
      "13/241: algebraic_model_lasso.best_models()[\"P_1\"]>1\n",
      "13/242: abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.1\n",
      "13/243: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.1]\n",
      "13/244: algebraic_model_lasso.best_models()[\"Ph_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.1]\n",
      "13/245: algebraic_model_lasso.best_models()[\"ph_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.1]\n",
      "13/246: algebraic_model_lasso.best_models()[\"Phi_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.1]\n",
      "13/247: algebraic_model_lasso.best_models()[\"Phi_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.1]\n",
      "13/248: algebraic_model_lasso.best_models()[\"Phi_2\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.1]\n",
      "13/249: algebraic_model_lasso.best_models()[\"Phi_3\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.1]\n",
      "13/250: algebraic_model_lasso.best_models()[\"Phi_4\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.1]\n",
      "13/251: algebraic_model_lasso.best_models()[\"Phi_5\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.1]\n",
      "13/252: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.1]\n",
      "13/253:\n",
      "feat = \"Phi_0\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/254:\n",
      "feat = \"Phi_1\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/255:\n",
      "feat = \"Phi_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/256:\n",
      "feat = \"Phi_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/257:\n",
      "feat = \"Phi_4\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/258:\n",
      "feat = \"Phi_5\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/259:\n",
      "feat = \"Phi_6\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/260:\n",
      "feat = \"om_0\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/261:\n",
      "feat = \"om_1\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/262:\n",
      "feat = \"om1\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/263:\n",
      "feat = \"P_0\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/264:\n",
      "feat = \"P_1\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/265:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/266: gamma_matrix\n",
      "13/267:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/268:\n",
      "feat = \"P_4\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/269:\n",
      "feat = \"P_5\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/270:\n",
      "feat = \"P_6\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/271:\n",
      "feat = \"P_5\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/272:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "13/273: algebraic_model_lasso.fit(candidate_lib, scale_columns= True)\n",
      "13/274:\n",
      "#Best 10 models using R2 metrix\n",
      "algebraic_model_lasso.best_models()[\"P_1\"]\n",
      "13/275: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.1]\n",
      "13/276:\n",
      "feat = \"P\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/277:\n",
      "feat = \"P_0\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/278:\n",
      "feat = \"P_1\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/279:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/280:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/281:\n",
      "feat = \"P_4\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/282:\n",
      "feat = \"P_5\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/283:\n",
      "feat = \"phi_1\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/284:\n",
      "feat = \"Phi_0\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/285:\n",
      "feat = \"Phi_1\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/286:\n",
      "feat = \"Phi_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/287:\n",
      "feat = \"Phi_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/288:\n",
      "feat = \"Phi_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/289:\n",
      "feat = \"Phi_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/290:\n",
      "feat = \"Phi_0\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "13/291:\n",
      "feat = \"P_i\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "15/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "from scipy.sparse import coo_array\n",
      "15/2: gamma_df = pd.read_csv(\"powergrid/Datasets/case3_gamma.csv\")\n",
      "15/3: gamma_df\n",
      "15/4: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case3_timeseries.csv\")\n",
      "15/5:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df), skip_n_rows_btw)\n",
      "15/6: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case3_K.csv\")\n",
      "15/7:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "15/8:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "15/9:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "15/10: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "15/11:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "15/12: data_matrix_df\n",
      "15/13: data_matrix_df.columns\n",
      "15/14:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case3_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "15/15: admittance_Y_df\n",
      "15/16: static_param_df = pd.read_csv(\"powergrid/Datasets/case3_staticparams.csv\")\n",
      "15/17: static_param_df\n",
      "15/18: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case3_K.csv\")\n",
      "15/19:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "15/20:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "15/21:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "15/22:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "15/23:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "15/24: sin_diff_library\n",
      "15/25: candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1), sin_diff_library], axis=1)\n",
      "15/26: candidate_lib\n",
      "16/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "from scipy.sparse import coo_array\n",
      "16/2: gamma_df = pd.read_csv(\"powergrid/Datasets/case3_gamma.csv\")\n",
      "16/3: gamma_df\n",
      "16/4: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case3_timeseries.csv\")\n",
      "16/5:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "16/6: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "16/7:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "16/8: data_matrix_df\n",
      "16/9: data_matrix_df.columns\n",
      "16/10:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case3_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "16/11: admittance_Y_df\n",
      "16/12: static_param_df = pd.read_csv(\"powergrid/Datasets/case3_staticparams.csv\")\n",
      "16/13: static_param_df\n",
      "16/14: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case3_K.csv\")\n",
      "16/15:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "16/16:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "16/17:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "16/18:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "16/19:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "16/20: sin_diff_library\n",
      "16/21: candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1), sin_diff_library], axis=1)\n",
      "16/22: candidate_lib\n",
      "16/23:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "16/24:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "16/25:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "16/26: algebraic_model_lasso.best_models()\n",
      "16/27:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "16/28:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "\n",
      "# coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "16/29:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "coupling_matrix_init[:,3] = 1\n",
      "\n",
      "# coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "16/30:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "16/31:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "16/32: sin_diff_library\n",
      "16/33: candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1), sin_diff_library], axis=1)\n",
      "16/34: candidate_lib\n",
      "16/35:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "16/36:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "16/37:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "16/38: algebraic_model_lasso.best_models()\n",
      "16/39:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "coupling_matrix_init[3,:] = 1\n",
      "\n",
      "# coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "16/40:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "16/41:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "16/42: sin_diff_library\n",
      "16/43: candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1), sin_diff_library], axis=1)\n",
      "16/44: candidate_lib\n",
      "16/45:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "16/46:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "16/47:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "16/48: algebraic_model_lasso.best_models()\n",
      "16/49:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "16/50:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.1,\n",
      "                                       fit_intercept=True)\n",
      "16/51:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "16/52: algebraic_model_lasso.best_models()\n",
      "16/53: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.1]\n",
      "16/54:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "16/55:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "16/56:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "16/57: algebraic_model_lasso.best_models()\n",
      "16/58: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.1]\n",
      "16/59:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "16/60:\n",
      "features_to_remove = {E}\n",
      "\n",
      "features_to_remove, refined_candid_lib = get_refined_lib(features_to_remove, data_matrix_df,\n",
      "                                                  candidate_lib_full, get_dropped_feat=True)\n",
      "\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       fit_intercept=False, alpha=0.2)\n",
      "algebraic_model_lasso.fit(refined_candid_lib, scale_columns= False)\n",
      "16/61:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=False)\n",
      "16/62:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=False)\n",
      "16/63:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "16/64: algebraic_model_lasso.best_models()\n",
      "16/65:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "16/66:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "16/67:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= False,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "16/68: algebraic_model_lasso.best_models()\n",
      "16/69: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.1]\n",
      "16/70:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "16/71:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "16/72:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= False,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "16/73: algebraic_model_lasso.best_models()\n",
      "16/74:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "16/75: algebraic_model_lasso.best_models()\n",
      "16/76: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "16/77:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "16/78: from sklearn.linear_model import LinearRegression\n",
      "16/79: lin_model = LinearRegression()\n",
      "16/80: candidate_lib.columns\n",
      "16/81:\n",
      "candidate_lib['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
      "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
      "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']\n",
      "16/82:\n",
      "candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
      "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
      "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']]\n",
      "16/83:\n",
      "lin_model.fit(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
      "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
      "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])\n",
      "16/84: lin_model.coef_\n",
      "16/85:\n",
      "lin_model.score(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
      "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
      "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])\n",
      "16/86: from sklearn.linear_model import LinearRegression, Lasso\n",
      "16/87:\n",
      "lin_model = LinearRegression()\n",
      "lass_model = Lasso(alpha=0.3)\n",
      "16/88:\n",
      "lass_model.fit(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
      "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
      "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])\n",
      "16/89: lass_model.coef_\n",
      "16/90:\n",
      "lass_model.score(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
      "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
      "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])\n",
      "16/91:\n",
      "lin_model = LinearRegression()\n",
      "lass_model = Lasso(alpha=0.01)\n",
      "16/92:\n",
      "lass_model.fit(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
      "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
      "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])\n",
      "16/93: lass_model.coef_\n",
      "16/94:\n",
      "lass_model.score(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
      "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
      "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])\n",
      "16/95:\n",
      "lin_model = LinearRegression()\n",
      "lass_model = Lasso(alpha=0.01)\n",
      "ridge_model = Ridge()\n",
      "16/96: from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
      "16/97:\n",
      "lin_model = LinearRegression()\n",
      "lass_model = Lasso(alpha=0.01)\n",
      "ridge_model = Ridge()\n",
      "16/98:\n",
      "lass_model.fit(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
      "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
      "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])\n",
      "\n",
      "ridge_model.fit(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
      "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
      "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])\n",
      "16/99: ridge_model.coef_\n",
      "16/100:\n",
      "ridge_model.score(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
      "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
      "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])\n",
      "16/101:\n",
      "candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
      "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
      "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']]\n",
      "16/102:\n",
      "candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
      "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
      "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']].plot()\n",
      "16/103: data_matrix_df\n",
      "16/104: data_matrix_df.plot()\n",
      "16/105: data_matrix_df.drop(\"time\", axis=1).plot()\n",
      "16/106: data_matrix_df.drop([\"time\", \"P_0\",\"P_1\",\"P_2\",\"P_3\",\"P_4\",\"P_5\" axis=1).plot()\n",
      "16/107: data_matrix_df.drop([\"time\", \"P_0\",\"P_1\",\"P_2\",\"P_3\",\"P_4\",\"P_5\"), axis=1).plot()\n",
      "16/108: data_matrix_df.drop([\"time\", \"P_0\",\"P_1\",\"P_2\",\"P_3\",\"P_4\",\"P_5\"], axis=1).plot()\n",
      "16/109: data_matrix_df.drop([\"time\", \"P_0\",\"P_1\",\"P_2\",\"P_3\",\"P_4\",\"P_5\", \"om_0\", \"om_1\"], axis=1).plot()\n",
      "16/110: data_matrix_df([\"P_0\",\"P_1\",\"P_2\",\"P_3\",\"P_4\",\"P_5\"])\n",
      "16/111: data_matrix_df[[\"P_0\",\"P_1\",\"P_2\",\"P_3\",\"P_4\",\"P_5\"]]\n",
      "16/112: data_matrix_df[[\"P_0\",\"P_1\",\"P_2\",\"P_3\",\"P_4\",\"P_5\"]].plot()\n",
      "17/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "from scipy.sparse import coo_array\n",
      "17/2: gamma_df = pd.read_csv(\"powergrid/Datasets/case3_gamma.csv\")\n",
      "17/3: gamma_df\n",
      "17/4:\n",
      "skip_n_rows_btw = 10\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "17/5: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case3_timeseries.csv\")\n",
      "17/6:\n",
      "skip_n_rows_btw = 10\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "17/7: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "17/8:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "17/9: data_matrix_df\n",
      "17/10: data_matrix_df.columns\n",
      "17/11:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case3_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "17/12: admittance_Y_df\n",
      "17/13: static_param_df = pd.read_csv(\"powergrid/Datasets/case3_staticparams.csv\")\n",
      "17/14: static_param_df\n",
      "17/15: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case3_K.csv\")\n",
      "17/16:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "17/17:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "17/18:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "coupling_matrix_init[3,:] = 1\n",
      "\n",
      "# coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "17/19:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "17/20:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "17/21: sin_diff_library\n",
      "17/22: candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1), sin_diff_library], axis=1)\n",
      "17/23: candidate_lib\n",
      "17/24:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "17/25:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "17/26:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "17/27: algebraic_model_lasso.best_models()\n",
      "17/28: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "17/29:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "17/30:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "17/31: data_matrix_df.columns\n",
      "17/32:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case3_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "17/33: admittance_Y_df\n",
      "17/34: static_param_df = pd.read_csv(\"powergrid/Datasets/case3_staticparams.csv\")\n",
      "17/35: static_param_df\n",
      "17/36: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case3_K.csv\")\n",
      "17/37:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "17/38:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "17/39:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "# coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "19/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "from scipy.sparse import coo_array\n",
      "19/2: gamma_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_gamma.csv\")\n",
      "19/3: gamma_df\n",
      "19/4: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_timeseries.csv\")\n",
      "19/5:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "19/6: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "19/7:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "19/8:\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "19/9: data_matrix_df.columns\n",
      "19/10:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "19/11: admittance_Y_df\n",
      "19/12: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "19/13: static_param_df\n",
      "19/14: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "19/15:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "19/16: gamma_df\n",
      "19/17:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "19/18:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "19/19:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "19/20:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "19/21: sin_diff_library\n",
      "19/22: candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1), sin_diff_library], axis=1)\n",
      "19/23: candidate_lib\n",
      "19/24:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "19/25:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "19/26:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "19/27: algebraic_model_lasso.best_models()\n",
      "19/28: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "19/29:\n",
      "feat = \"P_5\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "19/30:\n",
      "feat = \"P_0\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "19/31:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "19/32:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    return np.sin(x-y)\n",
      "    # return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "20/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "from scipy.sparse import coo_array\n",
      "20/2: gamma_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_gamma.csv\")\n",
      "20/3: gamma_df\n",
      "20/4: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_timeseries.csv\")\n",
      "20/5:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "20/6: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "20/7:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "20/8:\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "20/9: data_matrix_df.columns\n",
      "20/10:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "20/11: admittance_Y_df\n",
      "20/12: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "20/13: static_param_df\n",
      "20/14: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "20/15:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "20/16: gamma_df\n",
      "20/17:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "20/18:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "20/19:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    return np.sin(x-y)\n",
      "    # return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "20/20:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "20/21: sin_diff_library\n",
      "20/22: candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1), sin_diff_library], axis=1)\n",
      "20/23: candidate_lib\n",
      "20/24:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "20/25:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "20/26:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "20/27: algebraic_model_lasso.best_models()\n",
      "20/28: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "20/29:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "20/30:\n",
      "feat = \"P_0\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "20/31:\n",
      "feat = \"P_1\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "20/32:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "20/33:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "20/34: candidate_lib.describe()\n",
      "20/35: candidate_lib.std()\n",
      "20/36: candidate_lib.sum()\n",
      "20/37: candidate_lib**2.sum()\n",
      "20/38: (candidate_lib**2).sum()\n",
      "20/39: candidate_lib**2 > 0.00001\n",
      "20/40: (candidate_lib**2).sum() > 0.00001\n",
      "20/41: candidate_lib = candidate_lib[(candidate_lib**2).sum() > 0.00001]\n",
      "20/42:\n",
      "non_zero_columns = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_columns\n",
      "20/43:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "20/44:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "20/45: non_columns\n",
      "20/46:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "20/47: candidate_lib\n",
      "20/48: (candidate_lib**2).sum()\n",
      "21/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "from scipy.sparse import coo_array\n",
      "21/2: gamma_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_gamma.csv\")\n",
      "21/3: gamma_df\n",
      "21/4: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_timeseries.csv\")\n",
      "21/5:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "21/6: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "21/7:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "21/8:\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "21/9: data_matrix_df.columns\n",
      "21/10:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "21/11: admittance_Y_df\n",
      "21/12: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "21/13: static_param_df\n",
      "21/14: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "21/15:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "21/16: gamma_df\n",
      "21/17:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "21/18:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "21/19:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    return np.sin(x-y)\n",
      "    # return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "21/20:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "21/21: sin_diff_library\n",
      "21/22: candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1), sin_diff_library], axis=1)\n",
      "21/23:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "21/24: candidate_lib\n",
      "21/25:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "21/26:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "21/27:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "21/28: algebraic_model_lasso.best_models()\n",
      "21/29: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "21/30:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/31:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/32:\n",
      "feat = \"P_1\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/33:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/34:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/35:\n",
      "feat = \"P_4\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/36:\n",
      "feat = \"P_5\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/37:\n",
      "feat = \"P_6\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/38:\n",
      "feat = \"P_5\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/39:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.5,\n",
      "                                       fit_intercept=True)\n",
      "21/40:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "21/41: algebraic_model_lasso.best_models()\n",
      "21/42: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "21/43:\n",
      "feat = \"P_5\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/44:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/45:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.6,\n",
      "                                       fit_intercept=True)\n",
      "21/46:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "21/47: algebraic_model_lasso.best_models()\n",
      "21/48: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "21/49:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/50:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/51:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "21/52:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "21/53: algebraic_model_lasso.best_models()\n",
      "21/54: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "21/55:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/56:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "21/57:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= False,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "21/58: algebraic_model_lasso.best_models()\n",
      "21/59: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "21/60:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/61:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/62:\n",
      "feat = \"P_0\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/63:\n",
      "feat = \"P_1\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/64:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "21/65: algebraic_model_lasso.best_models()\n",
      "21/66: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "21/67:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "21/68:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "21/69: algebraic_model_lasso.best_models()\n",
      "21/70: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "21/71: algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "21/72:\n",
      "feat = \"P_0\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/73:\n",
      "feat = \"P_1\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/74:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/75:\n",
      "feat = \"P_5\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/76:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/77:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "21/78:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    # return np.sin(x-y)\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "21/79:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "21/80: sin_diff_library\n",
      "21/81: candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1), sin_diff_library], axis=1)\n",
      "21/82:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "21/83: candidate_lib\n",
      "21/84:\n",
      "for i in gamma_matrix:\n",
      "    print(i)\n",
      "21/85: dummy_tr_sin_diff\n",
      "21/86: dummy_tr_sin_diff.coupled_indices_list\n",
      "21/87: cop_ind\n",
      "21/88: cop_ind\n",
      "21/89:\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_)\n",
      "21/90:\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1)[[\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]],\n",
      "                          sin_diff_library], axis=1)\n",
      "21/91:\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1)[[\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]],\n",
      "                          sin_diff_library], axis=1)\n",
      "21/92:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "21/93: candidate_lib\n",
      "21/94:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "21/95:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "21/96:\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_)\n",
      "21/97:\n",
      "# features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "#                                           \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_)\n",
      "21/98: algebraic_model_lasso.best_models()\n",
      "21/99:\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
      "                          sin_diff_library], axis=1)\n",
      "21/100:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "21/101: candidate_lib\n",
      "21/102:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "21/103:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "21/104:\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_)\n",
      "21/105: algebraic_model_lasso.best_models()\n",
      "21/106:\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "feature_to_libr_map = {}\n",
      "power_features\n",
      "21/107:\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.drop(power_features, axis=1) for power_feat in power_features}\n",
      "21/108: feature_to_libr_map\n",
      "21/109: candidate_lib.columns.drop(power_features, axis=1)\n",
      "21/110: candidate_lib.columns.drop(power_features)\n",
      "21/111:\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "21/112: feature_to_libr_map\n",
      "21/113: feature_to_libr_map\n",
      "21/114: data_matrix_df.drop??\n",
      "21/115: data_matrix_df.drop(\"sjsds\", axis=1)\n",
      "21/116: data_matrix_df.drop(\"sjsds\", axis=1, errors='ignore')\n",
      "21/117: features_to_fit_ in data_matrix_df.columns\n",
      "21/118: set(features_to_fit_) in data_matrix_df.columns\n",
      "21/119: set(features_to_fit_).subset( data_matrix_df.columns)\n",
      "21/120: set(features_to_fit_) <= data_matrix_df.columns\n",
      "21/121: set(features_to_fit_) <= set(data_matrix_df.columns)\n",
      "22/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "from scipy.sparse import coo_array\n",
      "22/2: gamma_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_gamma.csv\")\n",
      "22/3: gamma_df\n",
      "22/4: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_timeseries.csv\")\n",
      "22/5:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "22/6: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "22/7:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "22/8:\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "22/9: data_matrix_df.columns\n",
      "22/10:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "22/11: admittance_Y_df\n",
      "22/12: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "22/13: static_param_df\n",
      "22/14: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "22/15:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "22/16: gamma_df\n",
      "22/17:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "22/18:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "22/19:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    # return np.sin(x-y)\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "22/20:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "22/21: cop_ind\n",
      "22/22: sin_diff_library\n",
      "22/23:\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
      "                          sin_diff_library], axis=1)\n",
      "22/24:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "22/25: candidate_lib\n",
      "22/26:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "22/27:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "22/28:\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "22/29: set(features_to_fit_) <= set(data_matrix_df.columns)\n",
      "22/30:\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "feature_to_libr_map = {}\n",
      "for \n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_)\n",
      "22/31:\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "feature_to_libr_map = {}\n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_)\n",
      "22/32: algebraic_model_lasso.best_models()\n",
      "22/33: algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "22/34:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "22/35:\n",
      "feat = \"P_0\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "22/36:\n",
      "feat = \"P_1\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "22/37:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "22/38:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "22/39:\n",
      "feat = \"P_4\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "22/40:\n",
      "feat = \"P_5\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "22/41:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "22/42:\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "\n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "23/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "from scipy.sparse import coo_array\n",
      "23/2: gamma_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_gamma.csv\")\n",
      "23/3: gamma_df\n",
      "23/4: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_timeseries.csv\")\n",
      "23/5:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "23/6: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "23/7:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "23/8:\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "23/9: data_matrix_df.columns\n",
      "23/10:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "23/11: admittance_Y_df\n",
      "23/12: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "23/13: static_param_df\n",
      "23/14: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "23/15:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "23/16: gamma_df\n",
      "23/17:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "23/18:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "23/19:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    # return np.sin(x-y)\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "23/20:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "23/21: cop_ind\n",
      "23/22: sin_diff_library\n",
      "23/23:\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
      "                          sin_diff_library], axis=1)\n",
      "23/24:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "23/25: candidate_lib\n",
      "23/26:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "23/27:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "23/28:\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "\n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "23/29: algebraic_model_lasso.best_models()\n",
      "23/30: algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "23/31:\n",
      "feat = \"P_0\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "23/32:\n",
      "feat = \"P_1\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "23/33:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "23/34:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "23/35:\n",
      "feat = \"P_4\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "23/36:\n",
      "feat = \"P_5\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "23/37:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "23/38:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "23/39:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "23/40:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lin_reg_model = LinearRegression\n",
      "lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "seq_th_model = sequentialThLin(custom_model=True,\n",
      "                               custom_model_ob = lin_reg_model,\n",
      "                               custom_model_arg= lin_reg_model_arg,\n",
      "                              coef_threshold=0.1)\n",
      "# seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "algebraic_model_th = AlgModelFinder(custom_model=True, custom_model_ob= seq_th_model)\n",
      "23/41:\n",
      "algebraic_model_th.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "23/42:\n",
      "#Best 10 models using R2 metrix\n",
      "algebraic_model_th.best_models()\n",
      "23/43: algebraic_model_th.get_fitted_intercepts()\n",
      "23/44:\n",
      "feat = \"P_0\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "23/45:\n",
      "feat = \"P_1\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "23/46:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "23/47:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "23/48:\n",
      "feat = \"P_4\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "23/49:\n",
      "feat = \"P_5\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "23/50:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "algebraic_model_th = AlgModelFinder(custom_model=True, custom_model_ob= seq_th_model)\n",
      "23/51:\n",
      "algebraic_model_th.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "23/52:\n",
      "#Best 10 models using R2 metrix\n",
      "algebraic_model_th.best_models()\n",
      "23/53: algebraic_model_th.get_fitted_intercepts()\n",
      "23/54:\n",
      "feat = \"P_5\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "23/55:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "23/56:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "23/57:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "23/58:\n",
      "feat = \"P_1\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "23/59:\n",
      "feat = \"P_0\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "23/60:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.4,\n",
      "                                       fit_intercept=True)\n",
      "23/61:\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "\n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "23/62: algebraic_model_lasso.best_models()\n",
      "23/63: algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "23/64:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "23/65:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "23/66:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "23/67:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "23/68:\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "\n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "23/69: algebraic_model_lasso.best_models()\n",
      "23/70: algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "23/71:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "23/72: data_matrix_df\n",
      "23/73: data_matrix_df\n",
      "23/74:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"[t]\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "23/75: data_matrix_df_list[0]\n",
      "23/76: data_matrix_df_list[0] - data_matrix_df\n",
      "23/77: (data_matrix_df_list[0] - data_matrix_df).sum()\n",
      "23/78:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 2\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"[t]\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "23/79: (data_matrix_df_list[0] - data_matrix_df).sum()\n",
      "23/80:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 2\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"[t]\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "23/81: (data_matrix_df_list[0] - data_matrix_df).sum()\n",
      "23/82:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 10\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"[t]\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "23/83: (data_matrix_df_list[0] - data_matrix_df).sum()\n",
      "23/84:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"[t]\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "23/85: (data_matrix_df_list[0] - data_matrix_df).sum()\n",
      "23/86:\n",
      "# num_smoothed_points = num_time_points\n",
      "# t_eval_new = np.linspace(data_matrix_df_list[0][\"[t]\"].iloc[0], data_matrix_df_list[0][\"[t]\"].iloc[-1], num_smoothed_points)\n",
      "\n",
      "\n",
      "# amplify_left = 0.5\n",
      "# amplify_right = 3.0\n",
      "# amplify_factor = 1\n",
      "# t_length = t_span[1] - t_span[0]\n",
      "\n",
      "# num_left_points = int(num_smoothed_points * (amplify_left-t_span[0])/t_length)\n",
      "# num_right_points = int(num_smoothed_points * (t_span[1] - amplify_right)/t_length)\n",
      "\n",
      "# t_left = t_eval_new[:num_left_points]\n",
      "# t_right = t_eval_new[-num_right_points:]\n",
      "# t_middle = np.linspace(t_eval_new[num_left_points], t_eval_new[-(num_right_points+1)],\n",
      "#                        amplify_factor*(len(t_eval_new) - (num_left_points + num_right_points) ))\n",
      "\n",
      "# t_eval_new = np.concatenate((t_left, t_middle, t_right))\n",
      "23/87:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"[t]\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "23/88:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"[t]\",derr_order=1, noise_perc=noise_perc,\n",
      "                                          eval_points=t_eval_new) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "if \"time\" in data_matrix_df_smooth:\n",
      "    data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "23/89:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"[t]\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "if \"time\" in data_matrix_df_smooth:\n",
      "    data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "23/90: data_matrix_df_smooth\n",
      "23/91: data_matrix_df_smooth- data_matrix_df\n",
      "23/92:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"[t]\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "23/93:\n",
      "# num_smoothed_points = num_time_points\n",
      "# t_eval_new = np.linspace(data_matrix_df_list[0][\"[t]\"].iloc[0], data_matrix_df_list[0][\"[t]\"].iloc[-1], num_smoothed_points)\n",
      "\n",
      "\n",
      "# amplify_left = 0.5\n",
      "# amplify_right = 3.0\n",
      "# amplify_factor = 1\n",
      "# t_length = t_span[1] - t_span[0]\n",
      "\n",
      "# num_left_points = int(num_smoothed_points * (amplify_left-t_span[0])/t_length)\n",
      "# num_right_points = int(num_smoothed_points * (t_span[1] - amplify_right)/t_length)\n",
      "\n",
      "# t_left = t_eval_new[:num_left_points]\n",
      "# t_right = t_eval_new[-num_right_points:]\n",
      "# t_middle = np.linspace(t_eval_new[num_left_points], t_eval_new[-(num_right_points+1)],\n",
      "#                        amplify_factor*(len(t_eval_new) - (num_left_points + num_right_points) ))\n",
      "\n",
      "# t_eval_new = np.concatenate((t_left, t_middle, t_right))\n",
      "23/94:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"[t]\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "# if \"time\" in data_matrix_df_smooth:\n",
      "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "23/95:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"[t]\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "23/96:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"[t]\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "# if \"time\" in data_matrix_df_smooth:\n",
      "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "23/97: data_matrix_df_smooth- data_matrix_df\n",
      "23/98:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "num_time_points = len(data_matrix_df\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"[t]\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "23/99:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "num_time_points = len(data_matrix_df)\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"[t]\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "23/100: len(data_matrix_df)\n",
      "23/101:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "num_time_points = len(data_matrix_df)\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"[t]\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "23/102:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"[t]\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "# if \"time\" in data_matrix_df_smooth:\n",
      "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "23/103: data_matrix_df_smooth- data_matrix_df\n",
      "23/104:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"om_1\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \"x\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \"o\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/105:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"om_1\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \"o\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/106:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"om_1\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \"-\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/107:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"om_1\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \"-\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/108:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"om_1\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\"--\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \"-\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/109:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"om_1\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\"-\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \"-\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/110:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"om_1\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\"..\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \"-\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/111:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"om_1\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \"-\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/112:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"om_1\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\"-\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/113:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"om_2\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\"-\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/114:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"om_0\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\"-\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/115:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_0\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\"-\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/116:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_1\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\"-\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/117:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_2\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\"-\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/118:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_2\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/119:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_0\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/120:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_`\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/121:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_`\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/122:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_1\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/123:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_2\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/124:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_3\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/125:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_4\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/126:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_5\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/127:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "from scipy.sparse import coo_array\n",
      "23/128: gamma_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_gamma.csv\")\n",
      "23/129: gamma_df\n",
      "23/130: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_timeseries.csv\")\n",
      "23/131:\n",
      "skip_n_rows_btw = 10\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "23/132: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "23/133:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "23/134:\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "23/135: data_matrix_df.columns\n",
      "23/136:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "23/137: admittance_Y_df\n",
      "23/138: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "23/139: static_param_df\n",
      "23/140: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "23/141:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "23/142: gamma_df\n",
      "23/143:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "23/144:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "23/145:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    # return np.sin(x-y)\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "23/146:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "23/147: cop_ind\n",
      "23/148: sin_diff_library\n",
      "23/149:\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
      "                          sin_diff_library], axis=1)\n",
      "23/150:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "23/151: candidate_lib\n",
      "23/152:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "num_time_points = len(data_matrix_df)\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"[t]\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "23/153:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"[t]\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "# if \"time\" in data_matrix_df_smooth:\n",
      "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "23/154: data_matrix_df_smooth- data_matrix_df\n",
      "23/155:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_5\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/156: data_matrix_smooth_df_list\n",
      "23/157: data_matrix_smooth_df_list[0][[\"d(om_0) /dt\"]].plot()\n",
      "23/158: data_matrix_df_smooth_appended\n",
      "23/159: data_matrix_smooth_df_list[0][[\"d(om_0) /dt\"]].plot()\n",
      "23/160: data_matrix_df_smooth_appended[[\"d(om_0) /dt\"]].plot()\n",
      "23/161: data_matrix_smooth_df_list[0][[\"d(Phi_0) /dt\"]].plot()\n",
      "23/162: data_matrix_smooth_df_list[0][[\"d(Phi_0) /dt\", \"Phi_0\"]].plot()\n",
      "23/163: data_matrix_smooth_df_list[0][[\"d(Phi_0) /dt\", \"om_0\"]].plot()\n",
      "23/164: data_matrix_df_smooth_appended[[\"om_0\"]].plot()\n",
      "23/165: gamma_df\n",
      "23/166:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "26/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "from scipy.sparse import coo_array\n",
      "26/2: gamma_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_smallperturb/case_4bus2gen_gamma.csv\")\n",
      "26/3: gamma_df\n",
      "26/4: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_smallperturb/case_4bus2gen_timeseries.csv\")\n",
      "26/5:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "26/6: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "26/7:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "26/8:\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "26/9: data_matrix_df.columns\n",
      "26/10:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_smallperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "26/11: admittance_Y_df\n",
      "26/12: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_smallperturb/case_4bus2gen_staticparams.csv\")\n",
      "26/13: static_param_df\n",
      "26/14: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_smallperturb/case_4bus2gen_K.csv\")\n",
      "26/15:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "26/16: gamma_df\n",
      "26/17:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "26/18:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "26/19:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    # return np.sin(x-y)\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "26/20:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "26/21: cop_ind\n",
      "26/22: sin_diff_library\n",
      "26/23:\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
      "                          sin_diff_library], axis=1)\n",
      "26/24:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "26/25: candidate_lib\n",
      "26/26:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "26/27:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "26/28:\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "\n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "26/29: algebraic_model_lasso.best_models()\n",
      "26/30: algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "26/31:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "26/32:\n",
      "feat = \"P_0\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "26/33:\n",
      "feat = \"P_1\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "26/34:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "26/35:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "26/36:\n",
      "feat = \"P_4\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "26/37:\n",
      "feat = \"P_5\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "26/38:\n",
      "feat = \"P_0\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "26/39:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "algebraic_model_th = AlgModelFinder(custom_model=True, custom_model_ob= seq_th_model)\n",
      "26/40:\n",
      "algebraic_model_th.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "26/41:\n",
      "#Best 10 models using R2 metrix\n",
      "algebraic_model_th.best_models()\n",
      "26/42: algebraic_model_th.get_fitted_intercepts()\n",
      "26/43:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "26/44:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lin_reg_model = LinearRegression\n",
      "lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "seq_th_model = sequentialThLin(custom_model=True,\n",
      "                               custom_model_ob = lin_reg_model,\n",
      "                               custom_model_arg= lin_reg_model_arg,\n",
      "                              coef_threshold=0.1)\n",
      "# seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "algebraic_model_th = AlgModelFinder(custom_model=True, custom_model_ob= seq_th_model)\n",
      "26/45:\n",
      "algebraic_model_th.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "26/46:\n",
      "#Best 10 models using R2 metrix\n",
      "algebraic_model_th.best_models()\n",
      "26/47: algebraic_model_th.get_fitted_intercepts()\n",
      "26/48:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "26/49:\n",
      "feat = \"P_0\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "26/50:\n",
      "feat = \"P_1\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "26/51:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "26/52:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "26/53:\n",
      "feat = \"P_4\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "26/54:\n",
      "feat = \"P_5\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "26/55:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "num_time_points = len(data_matrix_df)\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"[t]\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "26/56:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"[t]\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "# if \"time\" in data_matrix_df_smooth:\n",
      "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "26/57: data_matrix_df_smooth- data_matrix_df\n",
      "26/58:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_5\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "26/59: data_matrix_smooth_df_list[0][[\"d(Phi_0) /dt\", \"om_0\"]].plot()\n",
      "26/60: data_matrix_df_smooth_appended[[\"om_1\"]].plot()\n",
      "26/61: data_matrix_df_smooth_appended[[\"om_0\"]].plot()\n",
      "26/62: data_matrix_df_smooth_appended[[\"om_1\"]].plot()\n",
      "26/63: data_matrix_df_smooth_appended[[\"om_2\"]].plot()\n",
      "26/64: data_matrix_df_smooth_appended[[\"om_1\"]].plot()\n",
      "26/65: data_matrix_df_smooth_appended[[\"d(om_0) /dt\"]].plot()\n",
      "26/66: data_matrix_df_smooth_appended[[\"d(Phi_0) /dt\"]].plot()\n",
      "23/167: data_matrix_smooth_df_list[0][[\"d(Phi_0) /dt\", \"om_0\"]].plot()\n",
      "23/168: data_matrix_df_smooth_appended[[\"d(Phi_0) /dt\"]].plot()\n",
      "23/169: data_matrix_df_smooth_appended[[\"d(om_0) /dt\"]].plot()\n",
      "23/170: data_matrix_df_smooth_appended.plot()\n",
      "23/171: data_matrix_dfplot()\n",
      "23/172: data_matrix_df.plot()\n",
      "23/173: data_matrix_smooth_df_list[0][[\"Phi_0\"]].plot()\n",
      "23/174: data_matrix_smooth_df_list[0][[\"Phi_0\"]].plot()\n",
      "23/175: data_matrix[[\"Phi_0\"]].plot()\n",
      "23/176: data_matrix_df[[\"Phi_0\"]].plot()\n",
      "23/177: data_matrix_df_smooth_appended[[\"d(om_1) /dt\"]].plot()\n",
      "23/178: data_matrix_df_smooth_appended.query(\"d(om_1) /dt < 5\")\n",
      "23/179: data_matrix_df_smooth_appended[data_matrix_df_smooth_appended[\"d(om_1) /dt\" < 5]]\n",
      "23/180: data_matrix_df_smooth_appended[data_matrix_df_smooth_appended[\"d(om_1) /dt\"] < 5]\n",
      "23/181:\n",
      "new_df = data_matrix_df_smooth_appended[data_matrix_df_smooth_appended[\"d(om_1) /dt\"] < 5]\n",
      "new_df\n",
      "23/182:\n",
      "new_df = data_matrix_df_smooth_appended[data_matrix_df_smooth_appended[\"d(om_0) /dt\"] < 5]\n",
      "\n",
      "new_df[[\"d(Phi_0) /dt\", \"om_0\"]].plot()\n",
      "23/183:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended[\"d(om_0) /dt\"]) < 5]\n",
      "\n",
      "new_df[[\"d(Phi_0) /dt\", \"om_0\"]].plot()\n",
      "23/184:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended[\"d(Phi_0) /dt\"]) < 5]\n",
      "\n",
      "new_df[[\"d(Phi_0) /dt\", \"om_0\"]].plot()\n",
      "23/185:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended[\"d(Phi_0) /dt\"]) < 10]\n",
      "\n",
      "new_df[[\"d(Phi_0) /dt\", \"om_0\"]].plot()\n",
      "23/186:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended[\"d(Phi_0) /dt\"]) < 15]\n",
      "\n",
      "new_df[[\"d(Phi_0) /dt\", \"om_0\"]].plot()\n",
      "23/187:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended[\"d(Phi_0) /dt\"]) < 10]\n",
      "\n",
      "new_df[[\"d(Phi_0) /dt\", \"om_0\"]].plot()\n",
      "23/188:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended[\"d(Phi_0) /dt\"]) < 5]\n",
      "\n",
      "new_df[[\"d(Phi_0) /dt\", \"om_0\"]].plot()\n",
      "23/189:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended[\"d(Phi_0) /dt\"]) < 5]\n",
      "\n",
      "new_df[[\"d(Phi_0) /dt\", \"om_0\",\"d(om_0) /dt\"]].plot()\n",
      "23/190:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended[\"d(Phi_0) /dt\"]) < 2]\n",
      "\n",
      "new_df[[\"d(Phi_0) /dt\", \"om_0\",\"d(om_0) /dt\"]].plot()\n",
      "23/191: plt.plot(data_matrix_df_smooth_appended[[\"time\"]], data_matrix_df_smooth_appended[[\"d(Phi_0) /dt\"]])\n",
      "23/192:\n",
      "plt.plot(data_matrix_df_smooth_appended[[\"time\"]], data_matrix_df_smooth_appended[[\"d(Phi_0) /dt\"]],\n",
      "         data_matrix_df_smooth_appended[[\"time\"]], data_matrix_df_smooth_appended[[\"om_0\"]])\n",
      "23/193:\n",
      "plt.plot(data_matrix_df_smooth_appended[[\"time\"]], data_matrix_df_smooth_appended[[\"d(Phi_0) /dt\"]], \".\"\n",
      "         data_matrix_df_smooth_appended[[\"time\"]], data_matrix_df_smooth_appended[[\"om_0\"]], \".\")\n",
      "23/194:\n",
      "plt.plot(data_matrix_df_smooth_appended[[\"time\"]], data_matrix_df_smooth_appended[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         data_matrix_df_smooth_appended[[\"time\"]], data_matrix_df_smooth_appended[[\"om_0\"]], \".\")\n",
      "23/195:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) < 10]\n",
      "\n",
      "\n",
      "plt.plot(data_matrix_df_smooth_appended[[\"time\"]], data_matrix_df_smooth_appended[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         data_matrix_df_smooth_appended[[\"time\"]], data_matrix_df_smooth_appended[[\"om_0\"]], \".\")\n",
      "23/196:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) < 10]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\")\n",
      "23/197:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\")\n",
      "23/198:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "23/199: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "23/200:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "23/201:\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "23/202: data_matrix_df.columns\n",
      "23/203:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "23/204: admittance_Y_df\n",
      "23/205: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "23/206: static_param_df\n",
      "23/207: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "23/208:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "23/209: gamma_df\n",
      "23/210:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "23/211:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "23/212:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    # return np.sin(x-y)\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "23/213:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "23/214: cop_ind\n",
      "23/215: sin_diff_library\n",
      "23/216:\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
      "                          sin_diff_library], axis=1)\n",
      "23/217:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "23/218: candidate_lib\n",
      "23/219:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "num_time_points = len(data_matrix_df)\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"[t]\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "23/220: gamma_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_gamma.csv\")\n",
      "23/221: gamma_df\n",
      "23/222: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_timeseries.csv\")\n",
      "23/223:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "23/224: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "23/225:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "23/226:\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "23/227: data_matrix_df.columns\n",
      "23/228:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "23/229: admittance_Y_df\n",
      "23/230: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "23/231: static_param_df\n",
      "23/232: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "23/233:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "23/234: gamma_df\n",
      "23/235:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "23/236:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "23/237:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    # return np.sin(x-y)\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "23/238:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "23/239: cop_ind\n",
      "23/240: sin_diff_library\n",
      "23/241:\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
      "                          sin_diff_library], axis=1)\n",
      "23/242:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "23/243: candidate_lib\n",
      "23/244:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "num_time_points = len(data_matrix_df)\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"time\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "23/245:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"[t]\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "# if \"time\" in data_matrix_df_smooth:\n",
      "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "23/246: data_matrix_df_smooth- data_matrix_df\n",
      "23/247:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"time\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "# if \"time\" in data_matrix_df_smooth:\n",
      "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "23/248: data_matrix_df_smooth- data_matrix_df\n",
      "23/249:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_5\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/250: data_matrix_smooth_df_list[0][[\"Phi_0\"]].plot()\n",
      "23/251: data_matrix_df[[\"Phi_0\"]].plot()\n",
      "23/252:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\")\n",
      "23/253: new_df\n",
      "23/254:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 100]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\")\n",
      "23/255: new_df\n",
      "23/256:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\")\n",
      "23/257: new_df\n",
      "23/258: new_df.max()\n",
      "23/259: len(new_df)\n",
      "23/260: len(data_matrix_df_smooth_appended)\n",
      "23/261: new_df[\"time\"]\n",
      "23/262: data_matrix_df_smooth_appended\n",
      "23/263: data_matrix_df_smooth_appended[\"time\"]\n",
      "23/264: data_matrix_df_smooth_appended\n",
      "23/265: new_df\n",
      "23/266: data_matrix_df_smooth_appended\n",
      "23/267: data_matrix_df_smooth_appended[\"time\"]\n",
      "23/268: data_matrix_df_smooth_appended[\"time\"].plot()\n",
      "23/269:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 100]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\")\n",
      "23/270:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\")\n",
      "23/271:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], data_matrix_df_smooth_appended[[\"om_0\"]], \".\")\n",
      "23/272:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], data_matrix_df_smooth_appended[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\")\n",
      "23/273:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\")\n",
      "23/274:\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\")\n",
      "\n",
      "new_df.plot()\n",
      "23/275:\n",
      "# Removing some of the outliers coming from sudden jump during perturbations\n",
      "\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\")\n",
      "\n",
      "new_df.plot()\n",
      "23/276:\n",
      "skip_n_rows_btw = 1000\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "23/277: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "23/278:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "23/279:\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "23/280: data_matrix_df.columns\n",
      "23/281:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "23/282: admittance_Y_df\n",
      "23/283: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "23/284: static_param_df\n",
      "23/285: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "23/286:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "23/287: gamma_df\n",
      "23/288:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "23/289:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "23/290:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    # return np.sin(x-y)\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "23/291:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "23/292: cop_ind\n",
      "23/293: sin_diff_library\n",
      "23/294:\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
      "                          sin_diff_library], axis=1)\n",
      "23/295:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "23/296: candidate_lib\n",
      "23/297:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "num_time_points = len(data_matrix_df)\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"time\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "23/298:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"time\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "# if \"time\" in data_matrix_df_smooth:\n",
      "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "23/299: data_matrix_df_smooth- data_matrix_df\n",
      "23/300:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_5\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "23/301: data_matrix_smooth_df_list[0][[\"Phi_0\"]].plot()\n",
      "23/302: data_matrix_df[[\"Phi_0\"]].plot()\n",
      "23/303:\n",
      "# Removing some of the outliers coming from sudden jump during perturbations\n",
      "\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\")\n",
      "\n",
      "new_df.plot()\n",
      "23/304:\n",
      "# Removing some of the outliers coming from sudden jump during perturbations\n",
      "\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\",\n",
      "        new_df[[\"time\"]], new_df[[\"d(om_0) /dt\"]], \".\")\n",
      "\n",
      "new_df.plot()\n",
      "27/1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "from scipy.sparse import coo_array\n",
      "27/2: gamma_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_gamma.csv\")\n",
      "27/3: gamma_df\n",
      "27/4: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_timeseries.csv\")\n",
      "27/5:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "27/6: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "27/7:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "27/8:\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "27/9: data_matrix_df.columns\n",
      "27/10:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "27/11: admittance_Y_df\n",
      "27/12: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "27/13: static_param_df\n",
      "27/14: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "27/15:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "27/16: gamma_df\n",
      "27/17:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "27/18:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "27/19:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    # return np.sin(x-y)\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "27/20:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "27/21: cop_ind\n",
      "27/22: sin_diff_library\n",
      "27/23:\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
      "                          sin_diff_library], axis=1)\n",
      "27/24:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "27/25: candidate_lib\n",
      "27/26:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "num_time_points = len(data_matrix_df)\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"time\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "27/27:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"time\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "# if \"time\" in data_matrix_df_smooth:\n",
      "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "27/28: data_matrix_df_smooth- data_matrix_df\n",
      "27/29:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_5\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "27/30: data_matrix_smooth_df_list[0][[\"Phi_0\"]].plot()\n",
      "27/31: data_matrix_df[[\"Phi_0\"]].plot()\n",
      "27/32:\n",
      "# Removing some of the outliers coming from sudden jump during perturbations\n",
      "\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\",\n",
      "        new_df[[\"time\"]], new_df[[\"d(om_0) /dt\"]], \".\")\n",
      "\n",
      "new_df.plot()\n",
      "27/33:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "27/34:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "27/35:\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "\n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "27/36: algebraic_model_lasso.best_models()\n",
      "27/37: algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "27/38: candidate_lib\n",
      "27/39: data_matrix_features\n",
      "27/40: data_matrix_features\n",
      "27/41: features_to_fit_\n",
      "27/42:\n",
      "refined_candid_lib = candidate_lib[['time', 'Phi_0', 'Phi_1', 'Phi_2', 'Phi_3', 'Phi_4', 'Phi_5', 'om_0',\n",
      "       'om_1', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']]\n",
      "\n",
      "refined_candid_lib\n",
      "27/43:\n",
      "refined_candid_lib = candidate_lib[['Phi_0', 'Phi_1', 'Phi_2', 'Phi_3', 'Phi_4', 'Phi_5', 'om_0',\n",
      "       'om_1', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']]\n",
      "\n",
      "refined_candid_lib\n",
      "27/44:\n",
      "# from dae_finder import sequentialThLin\n",
      "\n",
      "# seq_th_model = sequentialThLin(fit_intercept=False)\n",
      "\n",
      "# seq_th_model.fit(X=candidate_lib_full,  y=data_matrix_df_smooth_appended['d([P]) /dt'])\n",
      "# features_to_remove = {E, S*ES}\n",
      "\n",
      "# features_to_remove, refined_candid_lib = get_refined_lib(features_to_remove, data_matrix_df,\n",
      "#                                                   candidate_lib_full, get_dropped_feat=True)\n",
      "\n",
      "refined_candid_lib = candidate_lib[['Phi_0', 'Phi_1', 'Phi_2', 'Phi_3', 'Phi_4', 'Phi_5', 'om_0',\n",
      "       'om_1', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']]\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "s_scaler = StandardScaler(with_std=True, with_mean=False)\n",
      "scaled_refined_lib = pd.DataFrame(s_scaler.fit_transform(refined_candid_lib), columns=s_scaler.feature_names_in_)\n",
      "27/45: scaled_refined_lib\n",
      "27/46: scaled_refined_lib.describe()\n",
      "27/47:\n",
      "from sklearn.linear_model import Lasso\n",
      "alg_lasso = Lasso(fit_intercept=True, alpha=0.2)\n",
      "alg_lasso.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "alg_lasso.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/48: dict(zip(alg_lasso.feature_names_in_, alg_lasso.coef_))\n",
      "27/49:\n",
      "from sklearn.linear_model import Lasso\n",
      "alg_lasso = Lasso(fit_intercept=True, alpha=0.3)\n",
      "alg_lasso.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "alg_lasso.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/50: dict(zip(alg_lasso.feature_names_in_, alg_lasso.coef_))\n",
      "27/51:\n",
      "skip_n_rows_btw = 10\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "27/52: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "27/53:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "27/54:\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "27/55: data_matrix_df.columns\n",
      "27/56:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "27/57: admittance_Y_df\n",
      "27/58: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "27/59: static_param_df\n",
      "27/60: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "27/61:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "27/62: gamma_df\n",
      "27/63:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "27/64:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "27/65:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    # return np.sin(x-y)\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "27/66:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "27/67: cop_ind\n",
      "27/68: sin_diff_library\n",
      "27/69:\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
      "                          sin_diff_library], axis=1)\n",
      "27/70:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "27/71: candidate_lib\n",
      "27/72:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "num_time_points = len(data_matrix_df)\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"time\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "27/73:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"time\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "# if \"time\" in data_matrix_df_smooth:\n",
      "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "27/74: data_matrix_df_smooth- data_matrix_df\n",
      "27/75:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_5\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "27/76:\n",
      "# Removing some of the outliers coming from sudden jump during perturbations\n",
      "\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\",\n",
      "        new_df[[\"time\"]], new_df[[\"d(om_0) /dt\"]], \".\")\n",
      "\n",
      "new_df.plot()\n",
      "27/77:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "27/78:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "27/79:\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "\n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "27/80: algebraic_model_lasso.best_models()\n",
      "27/81: algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "27/82:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "27/83: candidate_lib\n",
      "27/84:\n",
      "# from dae_finder import sequentialThLin\n",
      "\n",
      "# seq_th_model = sequentialThLin(fit_intercept=False)\n",
      "\n",
      "# seq_th_model.fit(X=candidate_lib_full,  y=data_matrix_df_smooth_appended['d([P]) /dt'])\n",
      "# features_to_remove = {E, S*ES}\n",
      "\n",
      "# features_to_remove, refined_candid_lib = get_refined_lib(features_to_remove, data_matrix_df,\n",
      "#                                                   candidate_lib_full, get_dropped_feat=True)\n",
      "\n",
      "refined_candid_lib = candidate_lib[['Phi_0', 'Phi_1', 'Phi_2', 'Phi_3', 'Phi_4', 'Phi_5', 'om_0',\n",
      "       'om_1', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']]\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "s_scaler = StandardScaler(with_std=True, with_mean=False)\n",
      "scaled_refined_lib = pd.DataFrame(s_scaler.fit_transform(refined_candid_lib), columns=s_scaler.feature_names_in_)\n",
      "27/85:\n",
      "from sklearn.linear_model import Lasso\n",
      "alg_lasso = Lasso(fit_intercept=True, alpha=0.3)\n",
      "alg_lasso.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "alg_lasso.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/86: dict(zip(alg_lasso.feature_names_in_, alg_lasso.coef_))\n",
      "27/87:\n",
      "# from dae_finder import sequentialThLin\n",
      "\n",
      "# seq_th_model = sequentialThLin(fit_intercept=False)\n",
      "\n",
      "# seq_th_model.fit(X=candidate_lib_full,  y=data_matrix_df_smooth_appended['d([P]) /dt'])\n",
      "# features_to_remove = {E, S*ES}\n",
      "\n",
      "# features_to_remove, refined_candid_lib = get_refined_lib(features_to_remove, data_matrix_df,\n",
      "#                                                   candidate_lib_full, get_dropped_feat=True)\n",
      "\n",
      "refined_candid_lib = candidate_lib[['Phi_0', 'Phi_1', 'Phi_2', 'Phi_3', 'Phi_4', 'Phi_5', 'om_0',\n",
      "       'om_1', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']]\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "s_scaler = StandardScaler(with_std=True, with_mean=False)\n",
      "scaled_refined_lib = pd.DataFrame(s_scaler.fit_transform(refined_candid_lib), columns=s_scaler.feature_names_in_)\n",
      "scaled_candid_lib = pd.DataFrame(s_scaler.fit_transform(candidate_lib), columns=s_scaler.feature_names_in_)\n",
      "27/88:\n",
      "from sklearn.linear_model import Lasso\n",
      "alg_lasso = Lasso(fit_intercept=True, alpha=0.3)\n",
      "alg_lasso.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "alg_lasso.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/89: dict(zip(alg_lasso.feature_names_in_, alg_lasso.coef_))\n",
      "27/90: alg_lasso.fit(X=scaled_cand_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/91:\n",
      "# from dae_finder import sequentialThLin\n",
      "\n",
      "# seq_th_model = sequentialThLin(fit_intercept=False)\n",
      "\n",
      "# seq_th_model.fit(X=candidate_lib_full,  y=data_matrix_df_smooth_appended['d([P]) /dt'])\n",
      "# features_to_remove = {E, S*ES}\n",
      "\n",
      "# features_to_remove, refined_candid_lib = get_refined_lib(features_to_remove, data_matrix_df,\n",
      "#                                                   candidate_lib_full, get_dropped_feat=True)\n",
      "\n",
      "refined_candid_lib = candidate_lib[['Phi_0', 'Phi_1', 'Phi_2', 'Phi_3', 'Phi_4', 'Phi_5', 'om_0',\n",
      "       'om_1', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']]\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "s_scaler = StandardScaler(with_std=True, with_mean=False)\n",
      "scaled_refined_lib = pd.DataFrame(s_scaler.fit_transform(refined_candid_lib), columns=s_scaler.feature_names_in_)\n",
      "scaled_cand_lib = pd.DataFrame(s_scaler.fit_transform(candidate_lib), columns=s_scaler.feature_names_in_)\n",
      "27/92:\n",
      "from sklearn.linear_model import Lasso\n",
      "alg_lasso = Lasso(fit_intercept=True, alpha=0.3)\n",
      "alg_lasso.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "alg_lasso.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/93: dict(zip(alg_lasso.feature_names_in_, alg_lasso.coef_))\n",
      "27/94:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "lin_model = LinearRegression()\n",
      "lin_model.fit(X=scaled_refined_lib[[\"[ES]\"]],  y=data_matrix_df_smooth_appended['d([P]) /dt'])\n",
      "27/95: dict(zip(lin_model.feature_names_in_, lin_model.coef_))\n",
      "27/96: alg_lasso.fit(X=scaled_cand_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/97: dict(zip(alg_lasso.feature_names_in_, alg_lasso.coef_))\n",
      "27/98:\n",
      "from sklearn.linear_model import Lasso\n",
      "alg_lasso = Lasso(fit_intercept=True, alpha=0.3)\n",
      "alg_lasso.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "alg_lasso.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "\n",
      "from dae_finder import sequentialThLin\n",
      "\n",
      "seq_th_model = sequentialThLin(fit_intercept=False, coef_threshold=0.1)\n",
      "\n",
      "seq_th_model.fit(X=refined_candid_lib,  y=data_matrix_df_smooth_appended['d([P]) /dt'])\n",
      "27/99:\n",
      "# from sklearn.linear_model import Lasso\n",
      "# alg_lasso = Lasso(fit_intercept=True, alpha=0.3)\n",
      "# alg_lasso.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "# alg_lasso.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "\n",
      "# from dae_finder import sequentialThLin\n",
      "\n",
      "# seq_th_model = sequentialThLin(fit_intercept=True, coef_threshold=0.1)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/100:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/101: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "27/102:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(coef_threshold=0.5, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/103: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "27/104:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/105: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "27/106:\n",
      "# from dae_finder import sequentialThLin\n",
      "\n",
      "# seq_th_model = sequentialThLin(fit_intercept=False)\n",
      "\n",
      "# seq_th_model.fit(X=candidate_lib_full,  y=data_matrix_df_smooth_appended['d([P]) /dt'])\n",
      "# features_to_remove = {E, S*ES}\n",
      "\n",
      "# features_to_remove, refined_candid_lib = get_refined_lib(features_to_remove, data_matrix_df,\n",
      "#                                                   candidate_lib_full, get_dropped_feat=True)\n",
      "\n",
      "# refined_candid_lib = candidate_lib[['Phi_0', 'Phi_1', 'Phi_2', 'Phi_3', 'Phi_4', 'Phi_5', 'om_0',\n",
      "#        'om_1', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']]\n",
      "refined_candid_lib = data_matrix_df_smooth_appended[['Phi_0', 'Phi_1', 'Phi_2', 'Phi_3', 'Phi_4', 'Phi_5', 'om_0',\n",
      "       'om_1', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']]\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "s_scaler = StandardScaler(with_std=True, with_mean=False)\n",
      "scaled_refined_lib = pd.DataFrame(s_scaler.fit_transform(refined_candid_lib), columns=s_scaler.feature_names_in_)\n",
      "scaled_cand_lib = pd.DataFrame(s_scaler.fit_transform(candidate_lib), columns=s_scaler.feature_names_in_)\n",
      "27/107:\n",
      "from sklearn.linear_model import Lasso\n",
      "alg_lasso = Lasso(fit_intercept=True, alpha=0.3)\n",
      "alg_lasso.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "alg_lasso.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/108: dict(zip(alg_lasso.feature_names_in_, alg_lasso.coef_))\n",
      "27/109:\n",
      "# from dae_finder import sequentialThLin\n",
      "\n",
      "# seq_th_model = sequentialThLin(fit_intercept=False)\n",
      "\n",
      "# seq_th_model.fit(X=candidate_lib_full,  y=data_matrix_df_smooth_appended['d([P]) /dt'])\n",
      "# features_to_remove = {E, S*ES}\n",
      "\n",
      "# features_to_remove, refined_candid_lib = get_refined_lib(features_to_remove, data_matrix_df,\n",
      "#                                                   candidate_lib_full, get_dropped_feat=True)\n",
      "\n",
      "# refined_candid_lib = candidate_lib[['Phi_0', 'Phi_1', 'Phi_2', 'Phi_3', 'Phi_4', 'Phi_5', 'om_0',\n",
      "#        'om_1', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']]\n",
      "refined_candid_lib = data_matrix_df_smooth_appended[['Phi_0', 'Phi_1', 'Phi_2', 'Phi_3', 'Phi_4', 'Phi_5', 'om_0',\n",
      "       'om_1', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']]\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "s_scaler = StandardScaler(with_std=True, with_mean=False)\n",
      "scaled_refined_lib = pd.DataFrame(s_scaler.fit_transform(refined_candid_lib), columns=s_scaler.feature_names_in_)\n",
      "scaled_cand_lib = pd.DataFrame(s_scaler.fit_transform(candidate_lib), columns=s_scaler.feature_names_in_)\n",
      "27/110:\n",
      "from sklearn.linear_model import Lasso\n",
      "alg_lasso = Lasso(fit_intercept=True, alpha=0.3)\n",
      "alg_lasso.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "alg_lasso.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/111: dict(zip(alg_lasso.feature_names_in_, alg_lasso.coef_))\n",
      "27/112:\n",
      "from sklearn.linear_model import Lasso\n",
      "alg_lasso = Lasso(fit_intercept=True, alpha=0.4)\n",
      "alg_lasso.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "alg_lasso.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/113:\n",
      "from sklearn.linear_model import Lasso\n",
      "alg_lasso = Lasso(fit_intercept=True, alpha=0.2)\n",
      "alg_lasso.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "alg_lasso.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/114:\n",
      "from sklearn.linear_model import Lasso\n",
      "alg_lasso = Lasso(fit_intercept=True, alpha=0.3)\n",
      "alg_lasso.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "alg_lasso.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/115: dict(zip(alg_lasso.feature_names_in_, alg_lasso.coef_))\n",
      "27/116: alg_lasso.fit(X=scaled_cand_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/117: dict(zip(alg_lasso.feature_names_in_, alg_lasso.coef_))\n",
      "27/118:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/119: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "27/120:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=refined_candid_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/121: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "27/122:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/123: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "27/124:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lin_reg_model = LinearRegression\n",
      "lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "seq_th_model = sequentialThLin(custom_model=True,\n",
      "                               custom_model_ob = lin_reg_model,\n",
      "                               custom_model_arg= lin_reg_model_arg,\n",
      "                              coef_threshold=0.1)\n",
      "# seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/125: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "27/126:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lin_reg_model = LinearRegression\n",
      "lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "seq_th_model = sequentialThLin(custom_model=True,\n",
      "                               custom_model_ob = lin_reg_model,\n",
      "                               custom_model_arg= lin_reg_model_arg,\n",
      "                              coef_threshold=0.1)\n",
      "# seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/127: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "27/128:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/129: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "27/130:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(coef_threshold=0.1, alpha=0.3, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/131: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "27/132:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(coef_threshold=0.1, alpha=0.6, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/133: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "27/134:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(model_id=\"lasso\",coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "27/135: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "27/136:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(model_id=\"lasso\",coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_1) /dt'])\n",
      "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_1) /dt'])\n",
      "27/137: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "27/138: seq_th_model.intercept_\n",
      "27/139:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "algebraic_model_th = AlgModelFinder(custom_model=True, custom_model_ob= seq_th_model)\n",
      "27/140:\n",
      "algebraic_model_th.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "27/141:\n",
      "#Best 10 models using R2 metrix\n",
      "algebraic_model_th.best_models()\n",
      "27/142: algebraic_model_th.get_fitted_intercepts()\n",
      "27/143:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "27/144:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "   1:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "from scipy.sparse import coo_array\n",
      "   2: gamma_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_gamma.csv\")\n",
      "   3: gamma_df\n",
      "   4: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_timeseries.csv\")\n",
      "   5:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "   6: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "   7:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "   8:\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "   9: data_matrix_df.columns\n",
      "  10:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "  11: admittance_Y_df\n",
      "  12: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "  13: static_param_df\n",
      "  14: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "  15:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "  16: gamma_df\n",
      "  17:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "  18:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "  19:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    # return np.sin(x-y)\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "  20:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "  21: cop_ind\n",
      "  22: sin_diff_library\n",
      "  23:\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
      "                          sin_diff_library], axis=1)\n",
      "  24:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "  25: candidate_lib\n",
      "  26:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "num_time_points = len(data_matrix_df)\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"time\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "  27:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"time\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "# if \"time\" in data_matrix_df_smooth:\n",
      "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "  28: data_matrix_df_smooth- data_matrix_df\n",
      "  29:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_5\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "  30:\n",
      "# Removing some of the outliers coming from sudden jump during perturbations\n",
      "\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\",\n",
      "        new_df[[\"time\"]], new_df[[\"d(om_0) /dt\"]], \".\")\n",
      "\n",
      "new_df.plot()\n",
      "  31:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "  32:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "  33:\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "\n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "  34: algebraic_model_lasso.best_models()\n",
      "  35: algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "  36:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "  37:\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "\n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "  38: algebraic_model_lasso.best_models()\n",
      "  39: algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "  40:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "  41:\n",
      "# from dae_finder import sequentialThLin\n",
      "\n",
      "# seq_th_model = sequentialThLin(fit_intercept=False)\n",
      "\n",
      "# seq_th_model.fit(X=candidate_lib_full,  y=data_matrix_df_smooth_appended['d([P]) /dt'])\n",
      "# features_to_remove = {E, S*ES}\n",
      "\n",
      "# features_to_remove, refined_candid_lib = get_refined_lib(features_to_remove, data_matrix_df,\n",
      "#                                                   candidate_lib_full, get_dropped_feat=True)\n",
      "\n",
      "# refined_candid_lib = candidate_lib[['Phi_0', 'Phi_1', 'Phi_2', 'Phi_3', 'Phi_4', 'Phi_5', 'om_0',\n",
      "#        'om_1', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']]\n",
      "refined_candid_lib = data_matrix_df_smooth_appended[['Phi_0', 'Phi_1', 'Phi_2', 'Phi_3', 'Phi_4', 'Phi_5', 'om_0',\n",
      "       'om_1', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']]\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "s_scaler = StandardScaler(with_std=True, with_mean=False)\n",
      "scaled_refined_lib = pd.DataFrame(s_scaler.fit_transform(refined_candid_lib), columns=s_scaler.feature_names_in_)\n",
      "scaled_cand_lib = pd.DataFrame(s_scaler.fit_transform(candidate_lib), columns=s_scaler.feature_names_in_)\n",
      "  42:\n",
      "from sklearn.linear_model import Lasso\n",
      "alg_lasso = Lasso(fit_intercept=True, alpha=0.3)\n",
      "alg_lasso.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "alg_lasso.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "  43: dict(zip(alg_lasso.feature_names_in_, alg_lasso.coef_))\n",
      "  44:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "lin_model = LinearRegression()\n",
      "lin_model.fit(X=scaled_refined_lib[[\"[ES]\"]],  y=data_matrix_df_smooth_appended['d([P]) /dt'])\n",
      "  45: lin_model.intercept_\n",
      "  46: alg_lasso.fit(X=scaled_cand_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "  47: dict(zip(alg_lasso.feature_names_in_, alg_lasso.coef_))\n",
      "  48:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "algebraic_model_th = AlgModelFinder(custom_model=True, custom_model_ob= seq_th_model)\n",
      "  49:\n",
      "algebraic_model_th.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "  50:\n",
      "#Best 10 models using R2 metrix\n",
      "algebraic_model_th.best_models()\n",
      "  51: algebraic_model_th.get_fitted_intercepts()\n",
      "  52:\n",
      "feat = \"P_3\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "  53:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "  54:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(model_id=\"lasso\",coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_1) /dt'])\n",
      "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_1) /dt'])\n",
      "  55: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "  56:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(model_id=\"lasso\",coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "  57: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "  58:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(model_id=\"lasso\",coef_threshold=0.5, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "  59: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "  60:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(model_id=\"lasso\",coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "  61: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "  62: seq_th_model.intercept_\n",
      "  63:\n",
      "coef_dict = dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "coef_dict\n",
      "  64: non_zero_feat = [feat for feat,coef in coef_dict.items() if abs(coef)>0.01]\n",
      "  65: non_zero_feat\n",
      "  66:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "lin_model = LinearRegression(fit_intercept=True)\n",
      "lin_model.fit(X=X=scaled_refined_lib[non_zero_feat],  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "  67:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "lin_model = LinearRegression(fit_intercept=True)\n",
      "lin_model.fit(X=scaled_refined_lib[non_zero_feat],  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "  68:\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "lin_model = LinearRegression(fit_intercept=True)\n",
      "lin_model.fit(X=scaled_refined_lib[non_zero_feat],  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "lin_model.score(X=scaled_refined_lib[non_zero_feat],  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "  69: dict(zip(lin_model.feature_names_in_, lin_model.coef_))\n",
      "  70:\n",
      "seq_th_model.fit(X=scaled_cand_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "seq_th_model.score(X=scaled_cand_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "  71: dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "  72:\n",
      "skip_n_rows_btw = 10\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "  73: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "  74:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "  75:\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "  76: data_matrix_df.columns\n",
      "  77:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "  78: admittance_Y_df\n",
      "  79: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "  80: static_param_df\n",
      "  81: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "  82:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "  83: gamma_df\n",
      "  84:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "  85:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "  86:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    # return np.sin(x-y)\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "  87:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "  88: cop_ind\n",
      "  89: sin_diff_library\n",
      "  90:\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
      "                          sin_diff_library], axis=1)\n",
      "  91:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "  92: candidate_lib\n",
      "  93:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "num_time_points = len(data_matrix_df)\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"time\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "  94:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"time\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "# if \"time\" in data_matrix_df_smooth:\n",
      "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "  95: data_matrix_df_smooth- data_matrix_df\n",
      "  96:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_5\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "  97:\n",
      "# Removing some of the outliers coming from sudden jump during perturbations\n",
      "\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\",\n",
      "        new_df[[\"time\"]], new_df[[\"d(om_0) /dt\"]], \".\")\n",
      "\n",
      "new_df.plot()\n",
      "  98:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "  99:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      " 100:\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "\n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      " 101: algebraic_model_lasso.best_models()\n",
      " 102: algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      " 103:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      " 104:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      " 105:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      " 106:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      " 107:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      " 108: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      " 109:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      " 110:\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      " 111: data_matrix_df.columns\n",
      " 112:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      " 113: admittance_Y_df\n",
      " 114: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      " 115: static_param_df\n",
      " 116: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      " 117:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      " 118: gamma_df\n",
      " 119:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      " 120:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      " 121:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    # return np.sin(x-y)\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      " 122:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      " 123: cop_ind\n",
      " 124: sin_diff_library\n",
      " 125:\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
      "                          sin_diff_library], axis=1)\n",
      " 126:\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      " 127: candidate_lib\n",
      " 128:\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "num_time_points = len(data_matrix_df)\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"time\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      " 129:\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"time\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "# if \"time\" in data_matrix_df_smooth:\n",
      "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      " 130: data_matrix_df_smooth- data_matrix_df\n",
      " 131:\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_5\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      " 132:\n",
      "# Removing some of the outliers coming from sudden jump during perturbations\n",
      "\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\",\n",
      "        new_df[[\"time\"]], new_df[[\"d(om_0) /dt\"]], \".\")\n",
      "\n",
      "new_df.plot()\n",
      " 133:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      " 134:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      " 135:\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "\n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      " 136: algebraic_model_lasso.best_models()\n",
      " 137:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      " 138:\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "from scipy.sparse import coo_array\n",
      " 139: gamma_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_gamma.csv\")\n",
      " 140: gamma_df\n",
      " 141: data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_timeseries.csv\")\n",
      " 142:\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      " 143: data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      " 144:\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      " 145:\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      " 146: data_matrix_df.columns\n",
      " 147:\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      " 148: admittance_Y_df\n",
      " 149: static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      " 150: static_param_df\n",
      " 151: coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      " 152:\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      " 153: gamma_df\n",
      " 154:\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      " 155:\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      " 156:\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      " 157:\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      " 158: sin_diff_library\n",
      " 159: candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1), sin_diff_library], axis=1)\n",
      " 160: candidate_lib\n",
      " 161:\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      " 162:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      " 163:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      " 164: algebraic_model_lasso.best_models()\n",
      " 165: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      " 166:\n",
      "feat = \"P_5\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      " 167:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lin_reg_model = LinearRegression\n",
      "lin_reg_model_arg = {\"fit_intercept\": False}\n",
      "seq_th_model = sequentialThLin(custom_model=True,\n",
      "                               custom_model_ob = lin_reg_model,\n",
      "                               custom_model_arg= lin_reg_model_arg,\n",
      "                              coef_threshold=0.5)\n",
      "# seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "algebraic_model_th = AlgModelFinder(custom_model=True, custom_model_ob= seq_th_model)\n",
      " 168:\n",
      "algebraic_model_th.fit(candidate_lib, scale_columns= False,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      " 169:\n",
      "#Best 10 models using R2 metrix\n",
      "algebraic_model_th.best_models()\n",
      " 170: algebraic_model_th.get_fitted_intercepts()\n",
      " 171:\n",
      "feat = \"P_2\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      " 172:\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      " 173:\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      " 174: algebraic_model_lasso.best_models()\n",
      " 175: algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      " 176:\n",
      "feat = \"P_5\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      " 177: from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
      " 178:\n",
      "lin_model = LinearRegression()\n",
      "lass_model = Lasso(alpha=0.01)\n",
      "ridge_model = Ridge()\n",
      " 179:\n",
      "lin_model.fit(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
      "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
      "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])\n",
      " 180: gamma_df\n",
      " 181: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d39abeaa-2e31-4386-b91b-4f3aa55a0c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "from scipy.sparse import coo_array\n",
      "gamma_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_gamma.csv\")\n",
      "gamma_df\n",
      "data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_timeseries.csv\")\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "data_matrix_df.columns\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "admittance_Y_df\n",
      "static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "static_param_df\n",
      "coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "gamma_df\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    # return np.sin(x-y)\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "cop_ind\n",
      "sin_diff_library\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
      "                          sin_diff_library], axis=1)\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "candidate_lib\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "num_time_points = len(data_matrix_df)\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"time\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"time\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "# if \"time\" in data_matrix_df_smooth:\n",
      "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "data_matrix_df_smooth- data_matrix_df\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_5\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "# Removing some of the outliers coming from sudden jump during perturbations\n",
      "\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\",\n",
      "        new_df[[\"time\"]], new_df[[\"d(om_0) /dt\"]], \".\")\n",
      "\n",
      "new_df.plot()\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "\n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "algebraic_model_lasso.best_models()\n",
      "algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "\n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "algebraic_model_lasso.best_models()\n",
      "algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "# from dae_finder import sequentialThLin\n",
      "\n",
      "# seq_th_model = sequentialThLin(fit_intercept=False)\n",
      "\n",
      "# seq_th_model.fit(X=candidate_lib_full,  y=data_matrix_df_smooth_appended['d([P]) /dt'])\n",
      "# features_to_remove = {E, S*ES}\n",
      "\n",
      "# features_to_remove, refined_candid_lib = get_refined_lib(features_to_remove, data_matrix_df,\n",
      "#                                                   candidate_lib_full, get_dropped_feat=True)\n",
      "\n",
      "# refined_candid_lib = candidate_lib[['Phi_0', 'Phi_1', 'Phi_2', 'Phi_3', 'Phi_4', 'Phi_5', 'om_0',\n",
      "#        'om_1', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']]\n",
      "refined_candid_lib = data_matrix_df_smooth_appended[['Phi_0', 'Phi_1', 'Phi_2', 'Phi_3', 'Phi_4', 'Phi_5', 'om_0',\n",
      "       'om_1', 'P_0', 'P_1', 'P_2', 'P_3', 'P_4', 'P_5']]\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "s_scaler = StandardScaler(with_std=True, with_mean=False)\n",
      "scaled_refined_lib = pd.DataFrame(s_scaler.fit_transform(refined_candid_lib), columns=s_scaler.feature_names_in_)\n",
      "scaled_cand_lib = pd.DataFrame(s_scaler.fit_transform(candidate_lib), columns=s_scaler.feature_names_in_)\n",
      "from sklearn.linear_model import Lasso\n",
      "alg_lasso = Lasso(fit_intercept=True, alpha=0.3)\n",
      "alg_lasso.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "alg_lasso.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "dict(zip(alg_lasso.feature_names_in_, alg_lasso.coef_))\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "lin_model = LinearRegression()\n",
      "lin_model.fit(X=scaled_refined_lib[[\"[ES]\"]],  y=data_matrix_df_smooth_appended['d([P]) /dt'])\n",
      "lin_model.intercept_\n",
      "alg_lasso.fit(X=scaled_cand_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "dict(zip(alg_lasso.feature_names_in_, alg_lasso.coef_))\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "algebraic_model_th = AlgModelFinder(custom_model=True, custom_model_ob= seq_th_model)\n",
      "algebraic_model_th.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "#Best 10 models using R2 metrix\n",
      "algebraic_model_th.best_models()\n",
      "algebraic_model_th.get_fitted_intercepts()\n",
      "feat = \"P_3\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "feat = \"P_2\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(model_id=\"lasso\",coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_1) /dt'])\n",
      "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_1) /dt'])\n",
      "dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(model_id=\"lasso\",coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(model_id=\"lasso\",coef_threshold=0.5, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "# lin_reg_model = LinearRegression\n",
      "# lin_reg_model_arg = {\"fit_intercept\": True}\n",
      "# seq_th_model = sequentialThLin(custom_model=True,\n",
      "#                                custom_model_ob = lin_reg_model,\n",
      "#                                custom_model_arg= lin_reg_model_arg,\n",
      "#                               coef_threshold=0.1)\n",
      "seq_th_model = sequentialThLin(model_id=\"lasso\",coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "seq_th_model.fit(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "seq_th_model.score(X=scaled_refined_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "seq_th_model.intercept_\n",
      "coef_dict = dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "coef_dict\n",
      "non_zero_feat = [feat for feat,coef in coef_dict.items() if abs(coef)>0.01]\n",
      "non_zero_feat\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "lin_model = LinearRegression(fit_intercept=True)\n",
      "lin_model.fit(X=X=scaled_refined_lib[non_zero_feat],  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "lin_model = LinearRegression(fit_intercept=True)\n",
      "lin_model.fit(X=scaled_refined_lib[non_zero_feat],  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "from sklearn.linear_model import LinearRegression\n",
      "\n",
      "lin_model = LinearRegression(fit_intercept=True)\n",
      "lin_model.fit(X=scaled_refined_lib[non_zero_feat],  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "lin_model.score(X=scaled_refined_lib[non_zero_feat],  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "dict(zip(lin_model.feature_names_in_, lin_model.coef_))\n",
      "seq_th_model.fit(X=scaled_cand_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "seq_th_model.score(X=scaled_cand_lib,  y=data_matrix_df_smooth_appended['d(om_0) /dt'])\n",
      "dict(zip(seq_th_model.feature_names_in_, seq_th_model.coef_))\n",
      "skip_n_rows_btw = 10\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "data_matrix_df.columns\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "admittance_Y_df\n",
      "static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "static_param_df\n",
      "coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "gamma_df\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    # return np.sin(x-y)\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "cop_ind\n",
      "sin_diff_library\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
      "                          sin_diff_library], axis=1)\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "candidate_lib\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "num_time_points = len(data_matrix_df)\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"time\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"time\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "# if \"time\" in data_matrix_df_smooth:\n",
      "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "data_matrix_df_smooth- data_matrix_df\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_5\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "# Removing some of the outliers coming from sudden jump during perturbations\n",
      "\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\",\n",
      "        new_df[[\"time\"]], new_df[[\"d(om_0) /dt\"]], \".\")\n",
      "\n",
      "new_df.plot()\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "\n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "algebraic_model_lasso.best_models()\n",
      "algebraic_model_lasso.best_models()[\"P_0\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "data_matrix_df.columns\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "admittance_Y_df\n",
      "static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "static_param_df\n",
      "coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "gamma_df\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    # return np.sin(x-y)\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "cop_ind\n",
      "sin_diff_library\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1),\n",
      "                          sin_diff_library], axis=1)\n",
      "non_zero_column_series = (candidate_lib**2).sum() > 0.00001\n",
      "non_zero_column_series\n",
      "non_columns = [column for column in candidate_lib if non_zero_column_series[column]]\n",
      "\n",
      "candidate_lib = candidate_lib[non_columns]\n",
      "candidate_lib\n",
      "from dae_finder import add_noise_to_df\n",
      "noise_perc = 0\n",
      "data_matrix_df_list = [data_matrix_df]\n",
      "num_time_points = len(data_matrix_df)\n",
      "data_matrix_features = data_matrix_df_list[0].columns\n",
      "for ind, data_matrix_ in enumerate(data_matrix_df_list):\n",
      "    t_exact = data_matrix_[\"time\"]\n",
      "    noisy_data_df = add_noise_to_df(data_matrix_, noise_perc=noise_perc, random_seed=111)\n",
      "    noisy_data_df[\"time\"] = t_exact\n",
      "    data_matrix_df_list[ind] = noisy_data_df\n",
      "from dae_finder import smooth_data\n",
      "\n",
      "#Calling the smoothening function\n",
      "data_matrix_smooth_df_list = [smooth_data(data_matrix,domain_var=\"time\",derr_order=1, noise_perc=noise_perc) for data_matrix in data_matrix_df_list]\n",
      "\n",
      "if len(data_matrix_df_list) >1:\n",
      "    data_matrix_df_smooth_appended = pd.concat(data_matrix_smooth_df_list, ignore_index=True)\n",
      "else:\n",
      "    data_matrix_df_smooth_appended = data_matrix_smooth_df_list[0]\n",
      "\n",
      "data_matrix_df_smooth = data_matrix_df_smooth_appended[data_matrix_features]\n",
      "# if \"time\" in data_matrix_df_smooth:\n",
      "#     data_matrix_df_smooth = data_matrix_df_smooth.drop(\"time\", axis=1)\n",
      "data_matrix_df_smooth- data_matrix_df\n",
      "\n",
      "ind = 0\n",
      "feature_ = \"Phi_5\"\n",
      "\n",
      "plt.figure()\n",
      "# plt.plot(data_matrix_df_list[1][\"t\"], data_matrix_df_list[1][\"x\"], \"x\", t_eval_new, x_new,\n",
      "#         data_matrix_df[50:100][\"t\"], data_matrix_df[50:100][\"x\"], \"o\")\n",
      "\n",
      "plt.plot(data_matrix_df_list[ind][\"time\"], data_matrix_df_list[ind][feature_], \".\", data_matrix_smooth_df_list[ind][\"time\"],\n",
      "         data_matrix_smooth_df_list[ind][feature_],\".\",data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][\"time\"], data_matrix_df[ind*num_time_points:(ind+1)*num_time_points][feature_], \".\")\n",
      "plt.legend(['Noisy', 'Cubic Spline', 'True'])\n",
      "# plt.axis([-0.05, 6.33, -1.05, 1.05])\n",
      "plt.title('Cubic-spline interpolation of {} - Noise: {}%'.format(feature_, noise_perc))\n",
      "plt.show()\n",
      "# Removing some of the outliers coming from sudden jump during perturbations\n",
      "\n",
      "new_df = data_matrix_df_smooth_appended[abs(data_matrix_df_smooth_appended) <= 20]\n",
      "\n",
      "\n",
      "plt.plot(new_df[[\"time\"]], new_df[[\"d(Phi_0) /dt\"]], \".\",\n",
      "         new_df[[\"time\"]], new_df[[\"om_0\"]], \".\",\n",
      "        new_df[[\"time\"]], new_df[[\"d(om_0) /dt\"]], \".\")\n",
      "\n",
      "new_df.plot()\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "features_to_fit_ = [\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "# features_to_fit_ = [\"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]\n",
      "num_nodes = 6\n",
      "power_features = [\"P_{}\".format(ind) for ind in range(num_nodes)]\n",
      "#Mapping each power feature to possible expressions in the algebraic relationship\n",
      "feature_to_libr_map = {power_feat: candidate_lib.columns.drop(power_features) for power_feat in power_features}\n",
      "\n",
      "\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=features_to_fit_,\n",
      "                         feature_to_library_map=feature_to_libr_map)\n",
      "algebraic_model_lasso.best_models()\n",
      "feat = \"P_2\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "import numpy as np\n",
      "from scipy.integrate import odeint\n",
      "import pandas as pd\n",
      "import warnings\n",
      "pd.set_option('display.float_format', '{:0.8f}'.format)\n",
      "import operator\n",
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.integrate import solve_ivp\n",
      "from scipy.sparse import coo_array\n",
      "gamma_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_gamma.csv\")\n",
      "gamma_df\n",
      "data_matrix_df_orig = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_timeseries.csv\")\n",
      "skip_n_rows_btw = 100\n",
      "rows_to_keep = np.arange(0, len(data_matrix_df_orig), skip_n_rows_btw)\n",
      "data_matrix_df = data_matrix_df_orig.iloc[rows_to_keep].reset_index(drop=True)\n",
      "new_column_names = [\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\",\n",
      "                   \"Q_0\", \"Q_1\", \"Q_2\", \"Q_3\", \"Q_4\", \"Q_5\"]\n",
      "data_matrix_df.rename(columns=dict((zip(data_matrix_df.columns, new_column_names))),\n",
      "                     inplace=True)\n",
      "data_matrix_df = data_matrix_df[[\"time\", \"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                   \"om_0\", \"om_1\", \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"]]\n",
      "\n",
      "data_matrix_df\n",
      "data_matrix_df.columns\n",
      "admittance_Y_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_Y.csv\")\n",
      "for column in admittance_Y_df.columns:\n",
      "    admittance_Y_df[column] = admittance_Y_df[column].apply(lambda x: x.replace('i', 'j'))\n",
      "admittance_Y_df\n",
      "static_param_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_staticparams.csv\")\n",
      "static_param_df\n",
      "coupling_K_df = pd.read_csv(\"powergrid/Datasets/case_4bus2gen_largeperturb/case_4bus2gen_K.csv\")\n",
      "coupling_K_df_labeled = coupling_K_df.set_index(coupling_K_df.columns)\n",
      "coupling_K_df_labeled\n",
      "gamma_df\n",
      "gamma_matrix = gamma_df.to_numpy()\n",
      "admittance_Y_matrix = admittance_Y_df.to_numpy()\n",
      "\n",
      "gamma_matrix\n",
      "coupling_matrix_init = np.ones(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init = np.zeros(admittance_Y_matrix.shape)\n",
      "# coupling_matrix_init[3,:] = 1\n",
      "\n",
      "coupling_matrix_init = np.triu(coupling_matrix_init, 0)\n",
      "coupling_matrix_init\n",
      "sparse_coupling_matrix_init = coo_array(coupling_matrix_init)\n",
      "sparse_coupling_matrix_init.toarray()\n",
      "from dae_finder import FeatureCouplingTransformer\n",
      "\n",
      "def coup_fun(x,y,i,j,gam_matrix):\n",
      "    return np.sin(x-y- gam_matrix[i,j])\n",
      "\n",
      "def coup_namer(x,y,i,j,gam_matrix):\n",
      "    return \"sin( {}-{} -gamma_{},{} )\".format(x,y,i,j)\n",
      "    \n",
      "\n",
      "dummy_tr_sin_diff = FeatureCouplingTransformer(sparse_coupling_matrix_init,\n",
      "                                           coupling_func= coup_fun,\n",
      "                                           coupling_namer= coup_namer,\n",
      "                                           coupling_func_args={\"gam_matrix\":gamma_matrix},\n",
      "                                              return_df=True)\n",
      "sin_diff_library = dummy_tr_sin_diff.fit_transform(data_matrix_df.drop([\"time\"], axis=1))\n",
      "cop_ind = dummy_tr_sin_diff.coupled_indices_list\n",
      "\n",
      "# cop_ind\n",
      "sin_diff_library\n",
      "candidate_lib = pd.concat([data_matrix_df.drop(\"time\", axis=1), sin_diff_library], axis=1)\n",
      "candidate_lib\n",
      "import sympy\n",
      "\n",
      "from dae_finder import get_refined_lib, remove_paranth_from_feat\n",
      "\n",
      "# Adding the state variables as scipy symbols\n",
      "feat_list = list(data_matrix_df.columns)\n",
      "feat_list_str = \", \".join(remove_paranth_from_feat(data_matrix_df.columns))\n",
      "exec(feat_list_str+ \"= sympy.symbols(\"+str(feat_list)+\")\")\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "algebraic_model_lasso.best_models()\n",
      "algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "feat = \"P_5\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "from sklearn.linear_model import LinearRegression\n",
      "lin_reg_model = LinearRegression\n",
      "lin_reg_model_arg = {\"fit_intercept\": False}\n",
      "seq_th_model = sequentialThLin(custom_model=True,\n",
      "                               custom_model_ob = lin_reg_model,\n",
      "                               custom_model_arg= lin_reg_model_arg,\n",
      "                              coef_threshold=0.5)\n",
      "# seq_th_model = sequentialThLin(coef_threshold=0.1, fit_intercept=True)\n",
      "\n",
      "algebraic_model_th = AlgModelFinder(custom_model=True, custom_model_ob= seq_th_model)\n",
      "algebraic_model_th.fit(candidate_lib, scale_columns= False,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "#Best 10 models using R2 metrix\n",
      "algebraic_model_th.best_models()\n",
      "algebraic_model_th.get_fitted_intercepts()\n",
      "feat = \"P_2\"\n",
      "algebraic_model_th.best_models()[feat][abs(algebraic_model_th.best_models()[feat])>0.1]\n",
      "from dae_finder import sequentialThLin, AlgModelFinder\n",
      "algebraic_model_lasso = AlgModelFinder(model_id='lasso',\n",
      "                                       alpha=0.3,\n",
      "                                       fit_intercept=True)\n",
      "algebraic_model_lasso.fit(candidate_lib, scale_columns= True,\n",
      "                          features_to_fit=[\"Phi_0\", \"Phi_1\", \"Phi_2\", \"Phi_3\", \"Phi_4\", \"Phi_5\",\n",
      "                                          \"P_0\", \"P_1\", \"P_2\", \"P_3\", \"P_4\", \"P_5\"])\n",
      "algebraic_model_lasso.best_models()\n",
      "algebraic_model_lasso.best_models()[\"P_1\"][abs(algebraic_model_lasso.best_models()[\"P_1\"])>0.01]\n",
      "feat = \"P_5\"\n",
      "algebraic_model_lasso.best_models()[feat][abs(algebraic_model_lasso.best_models()[feat])>0.1]\n",
      "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
      "lin_model = LinearRegression()\n",
      "lass_model = Lasso(alpha=0.01)\n",
      "ridge_model = Ridge()\n",
      "lin_model.fit(candidate_lib[['sin( Phi_3-Phi_0 -gamma_3,0 )', 'sin( Phi_3-Phi_1 -gamma_3,1 )',\n",
      "       'sin( Phi_3-Phi_2 -gamma_3,2 )', 'sin( Phi_3-Phi_3 -gamma_3,3 )',\n",
      "       'sin( Phi_3-Phi_4 -gamma_3,4 )', 'sin( Phi_3-Phi_5 -gamma_3,5 )']], candidate_lib[\"P_3\"])\n",
      "gamma_df\n",
      "%history -g\n",
      "%history\n"
     ]
    }
   ],
   "source": [
    "%history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c26848-3c01-44fb-af7b-81f937384af7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
